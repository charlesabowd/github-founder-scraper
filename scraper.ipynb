{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5b930f8e",
   "metadata": {},
   "source": [
    "#  SETUP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "execution_count": 1,
   "id": "b9e6fd8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# if the script finishes very quickly (and generates an empty excel file), click run again\n",
    "# if the script errors on the \"Login Cell\" (added a comment to indicate which cell that is below), set IS_HEADLESS to \"False\" and run again. The scraper will automatically launch a page and attempt to login to LinkedIn. It's likely erroring because LinkedIn is asking for a captcha to verify the user is not a bot. Solve the captch/challenge and login. Once successfully logged in, set IS_HEADLESS back to \"True\" and run again.\n",
    "\n",
    "import pickle\n",
    "import csv\n",
    "import os\n",
    "import random\n",
    "import time\n",
    "from linkedin_scraper import actions\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.firefox.options import Options as FirefoxOptions\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.common.exceptions import TimeoutException, StaleElementReferenceException, NoSuchElementException, ElementClickInterceptedException\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "\n",
    "# for google docs upload\n",
    "import pickle\n",
    "import pickle\n",
    "import gspread\n",
    "from google.oauth2.service_account import Credentials\n",
    "from googleapiclient.discovery import build\n",
    "from datetime import datetime\n",
    "from google.oauth2.service_account import Credentials\n",
    "from googleapiclient.discovery import build\n",
    "from datetime import datetime\n",
    "\n",
    "IS_HEADLESS = False\n",
    "\n",
    "# Set environment variables to email and password\n",
    "load_dotenv()\n",
    "LINKEDIN_EMAIL = os.environ.get('LINKEDIN_EMAIL')\n",
    "LINKEDIN_PASSWORD = os.environ.get('LINKEDIN_PASSWORD')\n",
    "# Check if the environment variables are set\n",
    "if not LINKEDIN_EMAIL or not LINKEDIN_PASSWORD:\n",
    "    raise ValueError(\"LinkedIn credentials not set in environment variables\")\n",
    "\n",
    "\n",
    "COMPANY_CATEGORIES = {\n",
    "    \"SECURITY\": [\n",
    "        \"Okta\",\n",
    "        \"Snyk\",\n",
    "        \"R2C/Semgrep\", \n",
    "        \"Wiz\",\n",
    "        \"Lacework\",\n",
    "        \"Crowdstrike\",\n",
    "        \"Palo Alto Networks\",\n",
    "        \"Island\",\n",
    "        \"Vanta\",\n",
    "        \"Material Security\",\n",
    "        \"Abnormal Security\",\n",
    "        \"Samsara\",\n",
    "    ],\n",
    "    \"OTHER\": [\n",
    "        \"Figma\",\n",
    "        \"Airtable\",\n",
    "        \"Notion\",\n",
    "        \"Canva\",\n",
    "        \"Webflow\",\n",
    "        \"Faire\",\n",
    "        \"Deel\",\n",
    "        \"Rippling\",\n",
    "        \"Flexport\",\n",
    "        \"Benchling\",\n",
    "        \"Solugen\"\n",
    "    ],\n",
    "    \"PUBLIC\": [\n",
    "        \"Doordash\",\n",
    "        \"Uber\",\n",
    "        \"Palantir\",\n",
    "        \"Airbnb\",\n",
    "        \"Instacart\"\n",
    "    ],\n",
    "    \"INFRA\": [\n",
    "        \"Fivetran\",\n",
    "        \"DBT\",\n",
    "        \"Temporal\",\n",
    "        \"Cockroach Labs\",\n",
    "        \"Grafana\",\n",
    "        \"Zapier\",\n",
    "        \"Starburst\",\n",
    "        \"Retool\",\n",
    "        \"Sentry\",\n",
    "        \"Sourcegraph\",\n",
    "        \"Cribl\",\n",
    "        \"Vercel\",\n",
    "        \"Clickhouse\",\n",
    "        \"Github,\"\n",
    "        \"Cisco Meraki\",\n",
    "    ],\n",
    "    \"FINTECH\": [\n",
    "        \"Robinhood\",\n",
    "        \"Square\",\n",
    "        \"Stripe\",\n",
    "        \"Ramp\",\n",
    "        \"Brex\",\n",
    "        \"Plaid\",\n",
    "        \"Modern Treasury\",\n",
    "        \"Mercury\",\n",
    "        \"Persona\",\n",
    "        \"Klarna\",\n",
    "        \"Nubank\"\n",
    "    ],\n",
    "    \"CRYPTO\": [\n",
    "        \"Coinbase\",\n",
    "        \"Uniswap\",\n",
    "        \"Chainalysis\",\n",
    "        \"Arbitrum\",\n",
    "        \"TRM\",\n",
    "        \"Fireblocks\",\n",
    "        \"Eigenlayer\"\n",
    "    ],\n",
    "    \"FRONTIER\": [\n",
    "        \"Anduril\",\n",
    "        \"SpaceX\",\n",
    "        \"Zipline\",\n",
    "        \"Varda\",\n",
    "        \"Hadrian\"\n",
    "    ],\n",
    "    \"AI\": [\n",
    "        \"Bytedance\",\n",
    "        \"Scale AI\",\n",
    "        \"Anthropic\",\n",
    "        \"Robust intelligence\",\n",
    "        \"OpenAI\",\n",
    "        \"Predibase\",\n",
    "        \"Cohere\",\n",
    "        \"Databricks\",\n",
    "        \"Hugging Face\",\n",
    "        \"RunwayML\",\n",
    "        \"Tecton\",\n",
    "        \"Weights & Biases\",\n",
    "        \"Kumo AI\",\n",
    "        \"NVIDIA\",\n",
    "        \"Adept\",\n",
    "        \"Glean\",\n",
    "        \"Character.ai\",\n",
    "        \"Midjourney\",\n",
    "        \"Facebook AI\",\n",
    "        \"FAIR\",\n",
    "        \"Google brain\"\n",
    "    ]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "execution_count": null,
   "id": "adbde830",
   "metadata": {},
   "outputs": [],
   "outputs": [],
   "source": [
    "import sys\n",
    "print(sys.executable)"
    "print(sys.executable)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7957b23f",
   "metadata": {},
   "source": [
    "# LOGIN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "execution_count": 2,
   "id": "3596a5f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# IF THIS CELL ERRORS DUE TO CAPTCHA, do this:\n",
    "# manually complete the captcha, click on the next cell, and select Run menu, select \"run selected cell and all below\" \n",
    "\n",
    "def instantiate_driver():\n",
    "    options = FirefoxOptions()\n",
    "    if IS_HEADLESS:\n",
    "        options.add_argument(\"--headless\")\n",
    "    driver = webdriver.Firefox(options=options)\n",
    "    \n",
    "    try:\n",
    "        actions.login(driver, LINKEDIN_EMAIL, LINKEDIN_PASSWORD)\n",
    "    except Exception as e:\n",
    "        print(\"Error logging in. Please complete the captcha challenge and login manually.\")\n",
    "        print(e)\n",
    "\n",
    "    time.sleep(15)\n",
    "    return driver\n",
    "    \n",
    "driver = instantiate_driver()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d58f36b7",
   "id": "d58f36b7",
   "metadata": {},
   "source": [
    "# General Helper Functions"
    "# General Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb0c197a",
   "id": "cb0c197a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def wait_for_element_to_load(driver, by=By.CLASS_NAME, name=\"pv-top-card\", base=None, timeout=100):\n",
    "    \"\"\"\n",
    "    Wait for an element to be present on the page and return it.\n",
    "\n",
    "    Parameters:\n",
    "    - driver: The WebDriver instance\n",
    "    - by: The method to locate the element (default: By.CLASS_NAME)\n",
    "    - name: The name or identifier of the element to wait for\n",
    "    - base: The base element to search from (default: None, which uses the driver)\n",
    "    - timeout: Maximum time to wait for the element (default: 180 seconds)\n",
    "\n",
    "    Returns:\n",
    "    - The WebElement if found\n",
    "    - None if the element is not found within the timeout period\n",
    "    \"\"\"\n",
    "    base = base or driver\n",
    "    try:\n",
    "        element = WebDriverWait(base, timeout).until(\n",
    "            EC.presence_of_element_located((by, name))\n",
    "        )\n",
    "        return element\n",
    "    except TimeoutException:\n",
    "        print(f\"Timed out waiting for element: {by}={name}\")\n",
    "        return None\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred while waiting for element {by}={name}: {str(e)}\")\n",
    "        return None\n",
    "        \n",
    "\n",
    "def readPickle(input):\n",
    "    \"\"\"\n",
    "    Read data from a pickle file.\n",
    "\n",
    "    Parameters:\n",
    "    - input: The path to the pickle file to be read\n",
    "\n",
    "    Returns:\n",
    "    - A list containing the data from the pickle file\n",
    "    - An empty list if there's an error reading the file\n",
    "\n",
    "    Prints:\n",
    "    - A success message with the file name and number of results if successful\n",
    "    - An error message if there's an exception while reading the file\n",
    "    \"\"\"\n",
    "    results = []\n",
    "    with open(input, 'rb') as f:\n",
    "        try:\n",
    "            results = pickle.load(f)\n",
    "            print(\"Loaded pickle file: \" + input + \" with \" + str(len(results)) + \" results\")\n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "            print(\"Error loading the pickle file: \" + input)\n",
    "    return results\n",
    "\n",
    "def writePickle(output, data):\n",
    "    \"\"\"\n",
    "    Write data to a pickle file.\n",
    "\n",
    "    Parameters:\n",
    "    - output: The path where the pickle file will be saved\n",
    "    - data: The data to be written to the pickle file\n",
    "\n",
    "    Prints:\n",
    "    - A success message with the file name and number of results if successful\n",
    "    - An error message if there's an exception while writing the file\n",
    "    \"\"\"\n",
    "    with open(output, 'wb') as f:\n",
    "        try:\n",
    "            pickle.dump(data, f)\n",
    "            print(\"Saved pickle file: \" + output + \" with \" + str(len(data)) + \" results\")\n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "            print(\"Error saving the pickle file: \" + output)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "515b071f",
   "metadata": {},
   "source": [
    "# SALES NAV HELPER FUNCTIONS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "027fd1fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_and_load_pickles(historyPickle, toScrapePickle):\n",
    "    \"\"\"\n",
    "    Load data from two pickle files.\n",
    "\n",
    "    Parameters:\n",
    "    - historyPickle: Path to the pickle file containing scraped URLs\n",
    "    - toScrapePickle: Path to the pickle file containing URLs to scrape\n",
    "\n",
    "    Returns:\n",
    "    - A tuple containing two lists: (scraped_urls, to_scrape_urls)\n",
    "    \"\"\"\n",
    "    scraped_urls = []\n",
    "    to_scrape_urls = []\n",
    "    scraped_urls = readPickle(historyPickle)\n",
    "    to_scrape_urls = readPickle(toScrapePickle)\n",
    "    return scraped_urls, to_scrape_urls\n",
    "    \n",
    "\n",
    "def deduplicate(toScrapePickle, to_scrape_urls, already_scraped_urls):\n",
    "    \"\"\"\n",
    "    Remove duplicates from the to_scrape_urls list and update the pickle file.\n",
    "\n",
    "    Parameters:\n",
    "    - toScrapePickle: Path to the pickle file containing URLs to scrape\n",
    "    - to_scrape_urls: List of URLs to scrape\n",
    "    - already_scraped_urls: List of URLs that have already been scraped\n",
    "\n",
    "    Prints:\n",
    "    - The number of duplicates removed\n",
    "    \"\"\"\n",
    "    to_scrape_urls = list(set(to_scrape_urls))\n",
    "    originalCount = len(to_scrape_urls)\n",
    "    to_scrape_urls = [url for url in to_scrape_urls if url not in already_scraped_urls]\n",
    "    print(f\"Removed {originalCount - len(to_scrape_urls)} duplicates\")\n",
    "    writePickle(toScrapePickle, to_scrape_urls)\n",
    "    \n",
    "\n",
    "def scroll_to_profile(driver, profile):\n",
    "    \"\"\"\n",
    "    Scroll the page to bring the specified profile element into view.\n",
    "\n",
    "    Parameters:\n",
    "    - driver: The WebDriver instance\n",
    "    - profile: The profile element to scroll to\n",
    "\n",
    "    Returns:\n",
    "    - True if successful, False if an error occurs\n",
    "\n",
    "    Prints:\n",
    "    - Error message if scrolling fails\n",
    "    \"\"\"\n",
    "    try:\n",
    "        driver.execute_script(\"arguments[0].scrollIntoView();\", profile)\n",
    "        return True\n",
    "    except Exception as e:\n",
    "        print(f\"Error scrolling to profile: {e}\")\n",
    "        return False\n",
    "\n",
    "\n",
    "def click_profile(profile):\n",
    "    \"\"\"\n",
    "    Click on the profile element to open its details.\n",
    "\n",
    "    Parameters:\n",
    "    - profile: The profile element to click\n",
    "\n",
    "    Returns:\n",
    "    - True if successful, False if an error occurs\n",
    "\n",
    "    Prints:\n",
    "    - Error message if clicking fails\n",
    "    \"\"\"\n",
    "    try:\n",
    "        salesNavOpenProfileButton = profile.find_element(By.CLASS_NAME, \"artdeco-entity-lockup__title\")\n",
    "        salesNavOpenProfileButton.click()\n",
    "        return True\n",
    "    except (NoSuchElementException, ElementClickInterceptedException) as e:\n",
    "        print(f\"Error clicking profile: {e}\")\n",
    "        return False\n",
    "\n",
    "\n",
    "def click_three_dots_button(driver):\n",
    "    \"\"\"\n",
    "    Click the three dots button to open the dropdown menu.\n",
    "\n",
    "    Parameters:\n",
    "    - driver: The WebDriver instance\n",
    "\n",
    "    Returns:\n",
    "    - True if successful, False if an error occurs\n",
    "\n",
    "    Prints:\n",
    "    - Error message if clicking fails\n",
    "    \"\"\"\n",
    "    try:\n",
    "        actionContainer = driver.find_element(By.CLASS_NAME, \"_actions-container_1dg5u8\")\n",
    "        threeDotsButton = actionContainer.find_element(By.CLASS_NAME, \"_icon_ps32ck\")\n",
    "        threeDotsButton.click()\n",
    "        return True\n",
    "    except (NoSuchElementException, ElementClickInterceptedException) as e:\n",
    "        print(f\"Error clicking three dots button\")\n",
    "        return False\n",
    "\n",
    "\n",
    "def get_linkedin_url(driver):\n",
    "    \"\"\"\n",
    "    Retrieve the LinkedIn URL from the dropdown menu.\n",
    "\n",
    "    Parameters:\n",
    "    - driver: The WebDriver instance\n",
    "\n",
    "    Returns:\n",
    "    - The LinkedIn URL if successful, None if an error occurs\n",
    "\n",
    "    Prints:\n",
    "    - Error message if retrieval fails\n",
    "    \"\"\"\n",
    "    try:\n",
    "        dropdownContainer = driver.find_element(By.CLASS_NAME, \"_visible_x5gf48\")\n",
    "        normalLinkedInUrl = dropdownContainer.find_elements(By.TAG_NAME, \"a\")[1].get_attribute(\"href\")\n",
    "        return normalLinkedInUrl\n",
    "    except (NoSuchElementException, IndexError) as e:\n",
    "        print(f\"Error getting LinkedIn URL: {e}\")\n",
    "        return None\n",
    "\n",
    "\n",
    "def close_popout(driver):\n",
    "    \"\"\"\n",
    "    Close the profile popout.\n",
    "\n",
    "    Parameters:\n",
    "    - driver: The WebDriver instance\n",
    "\n",
    "    Returns:\n",
    "    - True if successful, False if an error occurs\n",
    "\n",
    "    Prints:\n",
    "    - Error message if closing fails\n",
    "    \"\"\"\n",
    "    try:\n",
    "        header = driver.find_element(By.CLASS_NAME, \"_inline-sidesheet-header-actions_1cn7lg\")\n",
    "        button = header.find_elements(By.CLASS_NAME, \"_button_ps32ck\")[1]\n",
    "        button.click()\n",
    "        return True\n",
    "    except (NoSuchElementException, ElementClickInterceptedException, IndexError) as e:\n",
    "        print(f\"Error closing popout: {e}\")\n",
    "        return False\n",
    "\n",
    "def navigate_to_next_page(driver):\n",
    "    \"\"\"\n",
    "    Navigate to the next page of search results.\n",
    "\n",
    "    Parameters:\n",
    "    - driver: The WebDriver instance\n",
    "\n",
    "    Returns:\n",
    "    - True if successful, False if there are no more pages or an error occurs\n",
    "\n",
    "    Prints:\n",
    "    - Error message if navigation fails\n",
    "    \"\"\"\n",
    "    try:\n",
    "        nextPageButton = driver.find_element(By.CLASS_NAME, \"artdeco-pagination__button--next\")\n",
    "        nextPageButton.click()\n",
    "        return True\n",
    "    except (NoSuchElementException, ElementClickInterceptedException) as e:\n",
    "        print(f\"No more pages or error navigating: {e}\")\n",
    "        return False\n",
    "\n",
    "def scrape_profiles(driver, SALES_NAV_SEARCH_URL, already_scraped_urls, to_scrape_urls, toScrapePickle):\n",
    "    \"\"\"\n",
    "    Main scraping function to extract LinkedIn profile URLs from Sales Navigator search results.\n",
    "\n",
    "    Parameters:\n",
    "    - driver: The WebDriver instance\n",
    "    - SALES_NAV_SEARCH_URL: The URL of the Sales Navigator search results page\n",
    "    - already_scraped_urls: List of URLs that have already been scraped\n",
    "    - to_scrape_urls: List of URLs to be scraped\n",
    "    - toScrapePickle: Path to the pickle file for storing URLs to be scraped\n",
    "\n",
    "    Returns:\n",
    "    - Updated list of URLs to be scraped\n",
    "\n",
    "    This function navigates through search result pages, extracts LinkedIn profile URLs,\n",
    "    and updates the to_scrape_urls list and pickle file.\n",
    "    \"\"\"\n",
    "    driver.get(SALES_NAV_SEARCH_URL)\n",
    "    \n",
    "    while True:\n",
    "        wait_for_element_to_load(driver, By.ID, \"search-results-container\")\n",
    "        profiles = driver.find_elements(By.CLASS_NAME, \"artdeco-list__item\")\n",
    "        \n",
    "        for profile in profiles:\n",
    "            if not scroll_to_profile(driver, profile):\n",
    "                continue\n",
    "\n",
    "            time.sleep(2 + random.random() * 6)\n",
    "\n",
    "\n",
    "            time.sleep(2 + random.random() * 6)\n",
    "\n",
    "\n",
    "            if not wait_for_element_to_load(driver, By.CLASS_NAME, \"artdeco-entity-lockup__title\"):\n",
    "                continue\n",
    "\n",
    "            if not click_profile(profile):\n",
    "                continue\n",
    "            \n",
    "            time.sleep(2 + random.random() * 6)\n",
    "            \n",
    "            time.sleep(2 + random.random() * 6)\n",
    "\n",
    "            if not wait_for_element_to_load(driver, By.CLASS_NAME, \"_actions-container_1dg5u8\"):\n",
    "                continue\n",
    "\n",
    "            if not click_three_dots_button(driver):\n",
    "                continue\n",
    "\n",
    "            time.sleep(2 + random.random() * 6)\n",
    "\n",
    "\n",
    "            time.sleep(2 + random.random() * 6)\n",
    "\n",
    "\n",
    "            if not wait_for_element_to_load(driver, By.CLASS_NAME, \"_visible_x5gf48\"):\n",
    "                continue\n",
    "\n",
    "            normalLinkedInUrl = get_linkedin_url(driver)\n",
    "            if normalLinkedInUrl:\n",
    "                if normalLinkedInUrl in already_scraped_urls:\n",
    "                    print(\"Skipping (already scraped): \" + normalLinkedInUrl)\n",
    "                else:\n",
    "                    to_scrape_urls.append(normalLinkedInUrl)\n",
    "                    writePickle(toScrapePickle, to_scrape_urls)\n",
    "                    writePickle(toScrapePickle, to_scrape_urls)\n",
    "                    print(\"Successfully scraped: \" + normalLinkedInUrl)\n",
    "\n",
    "            if not close_popout(driver):\n",
    "                continue\n",
    "\n",
    "            time.sleep(2 + random.random() * 6)\n",
    "\n",
    "\n",
    "            time.sleep(2 + random.random() * 6)\n",
    "\n",
    "\n",
    "        next_button = wait_for_element_to_load(driver, By.CLASS_NAME, \"artdeco-pagination__button--next\")\n",
    "        if not next_button or not next_button.is_enabled():\n",
    "            break\n",
    "\n",
    "        next_button.click()\n",
    "\n",
    "        if not wait_for_element_to_load(driver, By.CLASS_NAME, \"artdeco-list__item\"):\n",
    "            break\n",
    "\n",
    "    return to_scrape_urls"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d9c83c0",
   "metadata": {},
   "source": [
    "# PROFILE SCRAPING HELPER FUNCTIONS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce900eb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter():\n",
    "    \"\"\"\n",
    "    Placeholder function for filtering profiles.\n",
    "    Currently does nothing (pass).\n",
    "    \"\"\"\n",
    "    pass     \n",
    "    # try:\n",
    "        #     WebDriverWait(driver, 240).until(lambda d: d.execute_script('return document.readyState') == 'complete')\n",
    "        # except:\n",
    "        #     driver = reinstantiate_driver()\n",
    "\n",
    "        # # FILTERING\n",
    "        \n",
    "        # likely_founder = True\n",
    "        # relevant_exp = True\n",
    "\n",
    "        # cur_exp = experiences[0]\n",
    "        # relevant_companies = [\"stealth\", \"new\"]\n",
    "        # if any(company in cur_exp.institution_name.split(\" Â·\")[0].lower() for company in relevant_companies) or \"present\" not in cur_exp.to_date.lower():\n",
    "        #     likely_founder = True\n",
    "\n",
    "        # relevant_titles = [\"product\", \"engineer\", \"sales\", \"business development\", \"founder\", \"head\", \"lead\", \"senior\", \"staff\", \"chief\", \"growth\"]\n",
    "        # for experience in experiences[1:5]:\n",
    "        #     if any(title in experience.position_title.lower() for title in relevant_titles):\n",
    "        #         relevant_exp = True\n",
    "        #         break\n",
    "        # relevant_exp = True\n",
    "\n",
    "        # if not (likely_founder and relevant_exp):\n",
    "        #     print(likely_founder, relevant_exp)\n",
    "        #     return None\n",
    "\n",
    "        # person_obj = Person(profile_link, driver = driver, scrape=False, experiences = [None])\n",
    "        # try:\n",
    "        #     WebDriverWait(driver, 240).until(lambda d: d.execute_script('return document.readyState') == 'complete')\n",
    "        # except:\n",
    "        #     driver = reinstantiate_driver(driver)\n",
    "        # time.sleep(2 + random.random() * 7)\n",
    "        \n",
    "\n",
    "def get_experiences(driver):\n",
    "    \"\"\"\n",
    "    Scrape work experiences from a LinkedIn profile.\n",
    "\n",
    "    Parameters:\n",
    "    - driver: The WebDriver instance\n",
    "\n",
    "    Returns:\n",
    "    - A list of dictionaries containing scraped experience information\n",
    "\n",
    "    Prints:\n",
    "    - Success or error messages during the scraping process\n",
    "    \"\"\"\n",
    "    scraped_experiences = []\n",
    "\n",
    "    try:\n",
    "        wait_for_element_to_load(driver, By.CSS_SELECTOR, \"li.pvs-list__paged-list-item.artdeco-list__item.pvs-list__item--line-separated.pvs-list__item--one-column\")\n",
    "        experience_items = driver.find_elements(By.CSS_SELECTOR, \"li.pvs-list__paged-list-item.artdeco-list__item.pvs-list__item--line-separated.pvs-list__item--one-column\")\n",
    "\n",
    "        if len(experience_items) > 0:\n",
    "            for item in experience_items:\n",
    "                hidden_spans = item.find_elements(By.CSS_SELECTOR, \"span.visually-hidden\")\n",
    "                experience_texts = [span.text for span in hidden_spans]\n",
    "                experience = {\n",
    "                    \"title: \": experience_texts[0],\n",
    "                    \"company: \": experience_texts[1],\n",
    "                    \"dates: \": experience_texts[2],\n",
    "                }\n",
    "                \n",
    "                if len(experience_texts) > 3:\n",
    "                    experience[\"location: \"] = experience_texts[3]\n",
    "                if len(experience_texts) > 4:\n",
    "                    experience[\"summary: \"] = experience_texts[4]\n",
    "                if len(experience_texts) > 5:\n",
    "                    experience[\"remaining: \"] = (\", \").join(experience_texts[5:])\n",
    "                scraped_experiences.append(experience)\n",
    "\n",
    "            print(\"Successfully scraped experiences\")\n",
    "        else:\n",
    "            print(\"No experiences found\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        print(\"ERROR: No experiences found\")\n",
    "\n",
    "    return scraped_experiences\n",
    "\n",
    "def get_education(driver):\n",
    "    \"\"\"\n",
    "    Scrape education information from a LinkedIn profile.\n",
    "\n",
    "    Parameters:\n",
    "    - driver: The WebDriver instance\n",
    "\n",
    "    Returns:\n",
    "    - A list of dictionaries containing scraped education information\n",
    "\n",
    "    Prints:\n",
    "    - Success or error messages during the scraping process\n",
    "    \"\"\"\n",
    "    scraped_education = []\n",
    "\n",
    "    try:\n",
    "        wait_for_element_to_load(driver, By.CSS_SELECTOR, \"li.pvs-list__paged-list-item.artdeco-list__item.pvs-list__item--line-separated.pvs-list__item--one-column\")\n",
    "        education_items = driver.find_elements(By.CSS_SELECTOR, \"li.pvs-list__paged-list-item.artdeco-list__item.pvs-list__item--line-separated.pvs-list__item--one-column\")\n",
    "\n",
    "        if len(education_items) > 0:\n",
    "            for item in education_items:\n",
    "                hidden_spans = item.find_elements(By.CSS_SELECTOR, \"span.visually-hidden\")\n",
    "                education_texts = [span.text for span in hidden_spans]\n",
    "                education = {\n",
    "                    \"school: \": education_texts[0],\n",
    "                }\n",
    "                \n",
    "                if len(education_texts) > 1:\n",
    "                    education[\"degree: \"] = education_texts[1]\n",
    "                if len(education_texts) > 2:\n",
    "                    education[\"dates: \"] = education_texts[2]\n",
    "                if len(education_texts) > 3:\n",
    "                    education[\"remaining: \"] = (\", \").join(education_texts[5:])\n",
    "                scraped_education.append(education)\n",
    "\n",
    "            print(\"Successfully scraped education\")\n",
    "        else:\n",
    "            print(\"No education found\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        print(\"ERROR: No education found\")\n",
    "\n",
    "    return scraped_education\n",
    "\n",
    "def get_degree_of_connection_and_mutuals(driver):\n",
    "    \"\"\"\n",
    "    Scrape the degree of connection and mutual connections from a LinkedIn profile.\n",
    "\n",
    "    Parameters:\n",
    "    - driver: The WebDriver instance\n",
    "\n",
    "    Returns:\n",
    "    - A tuple containing the degree of connection and number of mutual connections\n",
    "\n",
    "    Prints:\n",
    "    - Success or error messages during the scraping process\n",
    "    \"\"\"\n",
    "    scraped_profile_dist = \"4+\"\n",
    "    scraped_mutuals = \"N/A\"\n",
    "\n",
    "    try:\n",
    "        scraped_profile_dist = wait_for_element_to_load(driver, By.CSS_SELECTOR, \"span.dist-value\").text\n",
    "        if scraped_profile_dist == \"1st\" or scraped_profile_dist == \"2nd\":\n",
    "            try:\n",
    "                span_element = wait_for_element_to_load(driver, By.CSS_SELECTOR, \"span.t-normal.t-black--light.t-14.hoverable-link-text\")\n",
    "                scraped_mutuals = span_element.text.split('\\n')[0]\n",
    "                print(\"Successfully found mutual connections: \" + scraped_mutuals)\n",
    "            except:\n",
    "                print(\"ERROR: mutuals not found\")\n",
    "        print(\"Successfully scraped degree of connection: \" + scraped_profile_dist)\n",
    "\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        print(\"No degree of connection found\")\n",
    "\n",
    "    return scraped_profile_dist, scraped_mutuals\n",
    "\n",
    "\n",
    "def get_description(driver):\n",
    "    \"\"\"\n",
    "    Scrape the profile description from a LinkedIn profile.\n",
    "\n",
    "    Parameters:\n",
    "    - driver: The WebDriver instance\n",
    "\n",
    "    Returns:\n",
    "    - A string containing the scraped description or \"N/A\" if not found\n",
    "\n",
    "    Prints:\n",
    "    - Success or error messages during the scraping process\n",
    "    \"\"\"\n",
    "    scraped_description = \"N/A\"\n",
    "    \n",
    "    try:\n",
    "        scraped_description = driver.find_element(By.CLASS_NAME, \"text-body-medium.break-words\").text\n",
    "        print(\"Successfully scraped description: \" + scraped_description)\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        print(\"ERROR: description not found\")\n",
    "        \n",
    "    return scraped_description\n",
    "\n",
    "def scrape_profile(driver, scraped_link):\n",
    "    \"\"\"\n",
    "    Main function to scrape a LinkedIn profile.\n",
    "\n",
    "    Parameters:\n",
    "    - driver: The WebDriver instance\n",
    "    - scraped_link: The URL of the LinkedIn profile to scrape\n",
    "\n",
    "    Returns:\n",
    "    - A dictionary containing all scraped profile information\n",
    "\n",
    "    Prints:\n",
    "    - Detailed information about the scraped profile\n",
    "    \"\"\"\n",
    "    # Scrape Name\n",
    "    driver.get(scraped_link)\n",
    "    scraped_profile_name = wait_for_element_to_load(driver, By.CSS_SELECTOR, \"h1.text-heading-xlarge.inline.t-24.v-align-middle.break-words\").text\n",
    "    time.sleep(2 + random.random() * 6)\n",
    "    time.sleep(2 + random.random() * 6)\n",
    "\n",
    "    # Scrape Experiences\n",
    "    experiences_url = os.path.join(scraped_link, \"details/experience\")\n",
    "    driver.get(experiences_url)\n",
    "    scraped_experiences = get_experiences(driver)\n",
    "    time.sleep(2 + random.random() * 6)\n",
    "\n",
    "    time.sleep(2 + random.random() * 6)\n",
    "\n",
    "\n",
    "    # Scrape Education\n",
    "    education_url = os.path.join(scraped_link, \"details/education\")\n",
    "    driver.get(education_url)\n",
    "    scraped_education = get_education(driver)\n",
    "    time.sleep(2 + random.random() * 6)\n",
    "\n",
    "    time.sleep(2 + random.random() * 6)\n",
    "\n",
    "\n",
    "    # Scrape degree of connection and mutuals if available\n",
    "    driver.get(scraped_link)\n",
    "    scraped_profile_dist, scraped_mutuals = get_degree_of_connection_and_mutuals(driver)\n",
    "    time.sleep(2 + random.random() * 6)\n",
    "\n",
    "    time.sleep(2 + random.random() * 6)\n",
    "\n",
    "    # Scrape description\n",
    "    scraped_description = get_description(driver)\n",
    "    time.sleep(2 + random.random() * 6)\n",
    "\n",
    "    time.sleep(2 + random.random() * 6)\n",
    "\n",
    "\n",
    "    # Scape profile link\n",
    "    scraped_link = driver.current_url\n",
    "    print(\"Successfully scraped profile link: \" + scraped_link)\n",
    "    time.sleep(2 + random.random() * 6)\n",
    "\n",
    "    profile = {}\n",
    "    profile[\"name\"] = scraped_profile_name\n",
    "    profile[\"experiences\"] = scraped_experiences\n",
    "    profile[\"education\"] = scraped_education\n",
    "    profile[\"profile_dist\"] = scraped_profile_dist\n",
    "    profile[\"mutuals\"] = scraped_mutuals\n",
    "    profile[\"profile_description\"] = scraped_description\n",
    "    profile[\"profile_link\"] = scraped_link\n",
    "\n",
    "    time.sleep(2 + random.random() * 6)\n",
    "\n",
    "    profile = {}\n",
    "    profile[\"name\"] = scraped_profile_name\n",
    "    profile[\"experiences\"] = scraped_experiences\n",
    "    profile[\"education\"] = scraped_education\n",
    "    profile[\"profile_dist\"] = scraped_profile_dist\n",
    "    profile[\"mutuals\"] = scraped_mutuals\n",
    "    profile[\"profile_description\"] = scraped_description\n",
    "    profile[\"profile_link\"] = scraped_link\n",
    "\n",
    "    print(\"\\nSuccess!\")\n",
    "    print(f\"Name: {profile['name']}\")\n",
    "    print(\"Experiences:\")\n",
    "    for exp in profile[\"experiences\"]:\n",
    "        print(f\"  Title: {exp.get('title: ', '')}\")\n",
    "        print(f\"  Company: {exp.get('company: ', '')}\")\n",
    "        print(f\"  Dates: {exp.get('dates: ', '')}\")\n",
    "        print(f\"  Location: {exp.get('location: ', '')}\")\n",
    "        print(f\"  Summary: {exp.get('summary: ', '')}\")\n",
    "        print(f\"  Remaining: {exp.get('remaining: ', '')}\")\n",
    "    print(\"Education:\")\n",
    "    for edu in profile[\"education\"]:\n",
    "        print(f\"  School: {edu.get('school: ', '')}\")\n",
    "        print(f\"  Degree: {edu.get('degree: ', '')}\")\n",
    "        print(f\"  Dates: {edu.get('dates: ', '')}\")\n",
    "        print(f\"  Remaining: {edu.get('remaining: ', '')}\")\n",
    "    print(f\"Profile Distance: {profile['profile_dist']}\")\n",
    "    print(f\"Mutuals: {profile['mutuals']}\")\n",
    "    print(f\"Description: {profile['profile_description']}\")\n",
    "    print(f\"Link: {profile['profile_link']}\\n\")\n",
    "\n",
    "    print(f\"Name: {profile['name']}\")\n",
    "    print(\"Experiences:\")\n",
    "    for exp in profile[\"experiences\"]:\n",
    "        print(f\"  Title: {exp.get('title: ', '')}\")\n",
    "        print(f\"  Company: {exp.get('company: ', '')}\")\n",
    "        print(f\"  Dates: {exp.get('dates: ', '')}\")\n",
    "        print(f\"  Location: {exp.get('location: ', '')}\")\n",
    "        print(f\"  Summary: {exp.get('summary: ', '')}\")\n",
    "        print(f\"  Remaining: {exp.get('remaining: ', '')}\")\n",
    "    print(\"Education:\")\n",
    "    for edu in profile[\"education\"]:\n",
    "        print(f\"  School: {edu.get('school: ', '')}\")\n",
    "        print(f\"  Degree: {edu.get('degree: ', '')}\")\n",
    "        print(f\"  Dates: {edu.get('dates: ', '')}\")\n",
    "        print(f\"  Remaining: {edu.get('remaining: ', '')}\")\n",
    "    print(f\"Profile Distance: {profile['profile_dist']}\")\n",
    "    print(f\"Mutuals: {profile['mutuals']}\")\n",
    "    print(f\"Description: {profile['profile_description']}\")\n",
    "    print(f\"Link: {profile['profile_link']}\\n\")\n",
    "\n",
    "    return profile\n",
    "\n",
    "\n",
    "def scrape_all_profiles(driver, historyPickle, toScrapePickle, resultsPickle, failedURLsTextFile):\n",
    "    \"\"\"\n",
    "    Scrape multiple LinkedIn profiles from a list of URLs.\n",
    "\n",
    "    Parameters:\n",
    "    - driver: The WebDriver instance\n",
    "    - historyPickle: Path to the pickle file containing previously scraped URLs\n",
    "    - toScrapePickle: Path to the pickle file containing URLs to be scraped\n",
    "    - resultsPickle: Path to the pickle file to store scraped profile data\n",
    "    - failedURLsTextFile: Path to a text file to record URLs that failed to scrape\n",
    "\n",
    "    Returns:\n",
    "    - A list of dictionaries containing scraped profile information\n",
    "\n",
    "    Prints:\n",
    "    - Progress updates and error messages during the scraping process\n",
    "    \"\"\"\n",
    "    results = readPickle(resultsPickle)\n",
    "    scraped_urls = []\n",
    "    history = []\n",
    "    to_scrape_urls = []\n",
    "    to_scrape_urls = readPickle(toScrapePickle)\n",
    "    to_scrape_urls = []\n",
    "    to_scrape_urls = readPickle(toScrapePickle)\n",
    "        \n",
    "    totalCount = len(to_scrape_urls)\n",
    "\n",
    "    print(f'# of profiles to scrape: {totalCount}')\n",
    "    \n",
    "\n",
    "    for i in range(len(to_scrape_urls) - 1, -1, -1):\n",
    "        url = to_scrape_urls[i]\n",
    "        print(f'At index: {totalCount - i} - url: {url}')\n",
    "        history = readPickle(historyPickle)\n",
    "        print(f'At index: {totalCount - i} - url: {url}')\n",
    "        history = readPickle(historyPickle)\n",
    "\n",
    "        try:\n",
    "            profile = scrape_profile(driver, url)\n",
    "            if profile != None:\n",
    "                print(\"saving profile info\")\n",
    "                results.append(profile)\n",
    "                writePickle(resultsPickle, results)\n",
    "                writePickle(resultsPickle, results)\n",
    "                \n",
    "                print(\"adding to history\")\n",
    "                history.append(url)\n",
    "                writePickle(historyPickle, history)\n",
    "                writePickle(historyPickle, history)\n",
    "                \n",
    "                print(\"recording scraped url\")\n",
    "                scraped_urls.append(url)\n",
    "            else:\n",
    "                print(\"profile filtered out\")\n",
    "            \n",
    "            print(\"removing from to-scrape\")\n",
    "            to_scrape_urls.remove(url)\n",
    "            writePickle(toScrapePickle, to_scrape_urls)\n",
    "            writePickle(toScrapePickle, to_scrape_urls)\n",
    "\n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "            print('Failed to scrape profile: ', url)\n",
    "\n",
    "            with open(failedURLsTextFile, 'a') as f:\n",
    "            with open(failedURLsTextFile, 'a') as f:\n",
    "                f.write(url + '\\n')\n",
    "\n",
    "        print(((totalCount - i)/totalCount) * 100, '% Done - at index:', totalCount - i)\n",
    "        print('\\n----------------------------------------------------------------------------------------------------------\\n')\n",
    "    return results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdd18909",
   "metadata": {},
   "source": [
    "# EXPORTING HELPER FUNCTIONS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56e0af4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def authenticate():\n",
    "    \"\"\"\n",
    "    Authenticate with Google Sheets and Drive APIs.\n",
    "\n",
    "    Returns:\n",
    "    - client: An authorized gspread client\n",
    "    - drive_service: An authorized Google Drive service object\n",
    "\n",
    "    This function uses a service account file 'google_auth.json' for authentication.\n",
    "    \"\"\"\n",
    "    scope = ['https://spreadsheets.google.com/feeds', 'https://www.googleapis.com/auth/drive']\n",
    "    creds = Credentials.from_service_account_file('google_auth.json', scopes=scope)\n",
    "    client = gspread.authorize(creds)\n",
    "    drive_service = build('drive', 'v3', credentials=creds)\n",
    "    return client, drive_service\n",
    "\n",
    "def create_sheet(folder_id):\n",
    "    \"\"\"\n",
    "    Create a new Google Sheet with multiple worksheets and move it to a specified folder.\n",
    "\n",
    "    Parameters:\n",
    "    - folder_id: The ID of the Google Drive folder where the sheet will be moved\n",
    "\n",
    "    Returns:\n",
    "    - spreadsheet: The created Google Sheets spreadsheet object\n",
    "\n",
    "    This function creates a new sheet with the current date in its name and\n",
    "    adds several worksheets: Main, Unicorn, Acquired, Portfolio, and Infra.\n",
    "    \"\"\"\n",
    "    client, drive_service = authenticate()\n",
    "\n",
    "    # Create sheet\n",
    "    today_date = datetime.today().strftime('%Y-%m-%d')\n",
    "    sheet_name = f'Linkedin Scraper Results {today_date}'\n",
    "    spreadsheet = client.create(sheet_name)\n",
    "\n",
    "    # Add additional worksheets\n",
    "    main_sheet = spreadsheet.add_worksheet(title='Main', rows=1000, cols=100)\n",
    "    unicorn_sheet = spreadsheet.add_worksheet(title='Unicorn', rows=1000, cols=100)\n",
    "    acquired_sheet = spreadsheet.add_worksheet(title='Acquired', rows=1000, cols=100)\n",
    "    portfolio_sheet = spreadsheet.add_worksheet(title='Portfolio', rows=1000, cols=100)\n",
    "    infra_sheet = spreadsheet.add_worksheet(title='Infra', rows=1000, cols=100)\n",
    "\n",
    "    # Delete the default sheet\n",
    "    default_sheet = spreadsheet.sheet1\n",
    "    spreadsheet.del_worksheet(default_sheet)\n",
    "\n",
    "    # Move the spreadsheet to the specified folder\n",
    "    file_id = spreadsheet.id\n",
    "    drive_service.files().update(fileId=file_id, addParents=folder_id, removeParents='root').execute()\n",
    "\n",
    "    return spreadsheet\n",
    "\n",
    "\n",
    "def export_sheet(profiles, spreadsheet, worksheet_title):\n",
    "    \"\"\"\n",
    "    Export profile data to a specific worksheet in a Google Sheet.\n",
    "\n",
    "    Parameters:\n",
    "    - profiles: A list of dictionaries containing profile data\n",
    "    - spreadsheet: The Google Sheets spreadsheet object\n",
    "    - worksheet_title: The title of the worksheet to export data to\n",
    "\n",
    "    Returns:\n",
    "    - worksheet_url: The URL of the specific worksheet where data was exported\n",
    "\n",
    "    This function creates a header row based on the maximum number of education\n",
    "    and experience entries across all profiles, then exports each profile's data\n",
    "    as a row in the specified worksheet.\n",
    "    \"\"\"\n",
    "    # Access the specified worksheet by title\n",
    "    sheet = spreadsheet.worksheet(worksheet_title)\n",
    "    \n",
    "    # Prepare the header for the combined sheet\n",
    "    max_edu = max(len(profile['education']) for profile in profiles)\n",
    "    max_exp = max(len(profile['experiences']) for profile in profiles)\n",
    "    header = [\"Link\", \"Name\", \"Description\", \"Distance\", \"Mutuals\"]\n",
    "    for i in range(max_edu):\n",
    "        header.extend([f\"Edu{i} School\", f\"Edu{i} Degree\", f\"Edu{i} Dates\", f\"Edu{i} Remaining\"])\n",
    "    for i in range(max_exp):\n",
    "        header.extend([f\"Exp{i} Title\", f\"Exp{i} Company\", f\"Exp{i} Dates\", f\"Exp{i} Location\", f\"Exp{i} Summary\", f\"Exp{i} Remaining\"])\n",
    "    sheet.append_row(header)\n",
    "\n",
    "    # Prepare the rows\n",
    "    for profile in profiles:\n",
    "        row = [profile['profile_link'], profile['name'], profile['profile_description'], profile['profile_dist'], profile['mutuals']]\n",
    "        \n",
    "        # Add education data\n",
    "        for edu in profile['education']:\n",
    "            row.extend([edu.get('school: ', ''), edu.get('degree: ', ''), edu.get('dates: ', ''), edu.get('remaining: ', '')])\n",
    "        # Pad with empty strings if there are fewer education entries than the maximum\n",
    "        row.extend([''] * ((max_edu - len(profile['education'])) * 4))\n",
    "        \n",
    "        # Add experiences data\n",
    "        for exp in profile['experiences']:\n",
    "            row.extend([exp.get('title: ', ''), exp.get('company: ', ''), exp.get('dates: ', ''), exp.get('location: ', ''), exp.get('summary: ', ''), exp.get('remaining: ', '')])\n",
    "        # Pad with empty strings if there are fewer experience entries than the maximum\n",
    "        row.extend([''] * ((max_exp - len(profile['experiences'])) * 6))\n",
    "\n",
    "        # Append the row to the sheet\n",
    "        sheet.append_row(row)\n",
    "\n",
    "    # Construct and return the specific worksheet's URL\n",
    "    worksheet_url = f\"{spreadsheet.url}#gid={sheet.id}\"\n",
    "    return worksheet_url"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1a5b649",
   "metadata": {},
   "source": [
    "# CREATE NEW GOOGLE SHEET"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e89bda1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create google sheet\n",
    "folder_id = '1vtHm_M0hwADZX76MLHXNL5WqhmE310Vo'\n",
    "spreadsheet = create_sheet(folder_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "095b1a61-bfdb-4ec3-b5f5-8bc81d74a68d",
   "metadata": {},
   "source": [
    "# QUERY 1: MAIN COMPANIES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aca7a719",
   "metadata": {},
   "outputs": [],
   "source": [
    "historyPickle = 'db/main_history.pickle'\n",
    "toScrapePickle = 'db/main_to_scrape.pickle'\n",
    "resultsPickle = 'db/main_results.pickle'\n",
    "failedURLsTextFile = 'db/main_failed_urls.txt'\n",
    "failedURLsTextFile = 'db/main_failed_urls.txt'\n",
    "SALES_NAV_SEARCH_URL = \"\"\"\n",
    "https://www.linkedin.com/sales/search/people?savedSearchId=1833971580&sessionId=jFoOWCqiTl2JVJ3QPZrL4g%3D%3D\n",
    "https://www.linkedin.com/sales/search/people?savedSearchId=1833971580&sessionId=jFoOWCqiTl2JVJ3QPZrL4g%3D%3D\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93127f7e",
   "metadata": {},
   "source": [
    "### Sales nav scraper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e0ea7d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "already_scraped_urls, to_scrape_urls = check_and_load_pickles(historyPickle, toScrapePickle)"
    "already_scraped_urls, to_scrape_urls = check_and_load_pickles(historyPickle, toScrapePickle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e83345e3-d166-4e30-a7d6-2634b0db34a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    updated_to_scrape_urls = scrape_profiles(driver, SALES_NAV_SEARCH_URL, already_scraped_urls, to_scrape_urls, toScrapePickle)\n",
    "    print(updated_to_scrape_urls)\n",
    "except Exception as e:\n",
    "    print(f\"An unexpected error occurred: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b1c8979-0304-4621-ae96-59108d44bada",
   "metadata": {},
   "source": [
    "### Deduplicate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ea0fe49",
   "metadata": {},
   "outputs": [],
   "source": [
    "deduplicate(toScrapePickle, to_scrape_urls, already_scraped_urls)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6da7736-3078-4e03-8be6-967f48640cdc",
   "metadata": {},
   "source": [
    "### Profile scraper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7187f14e",
   "metadata": {
    "scrolled": true
   },
   "id": "7187f14e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "results = scrape_all_profiles(driver, historyPickle, toScrapePickle, resultsPickle, failedURLsTextFile)\n",
    "print(\"Successfully scraped: \" + str(len(results)) + \" profiles\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b2ff05d",
   "metadata": {},
   "source": [
    "### Export data to Google Sheet"
    "results = scrape_all_profiles(driver, historyPickle, toScrapePickle, resultsPickle, failedURLsTextFile)\n",
    "print(\"Successfully scraped: \" + str(len(results)) + \" profiles\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b2ff05d",
   "metadata": {},
   "source": [
    "### Export data to Google Sheet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "801b0bdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "worksheet_title = 'Main'\n",
    "\n",
    "results = readPickle(resultsPickle)\n",
    "if results:\n",
    "    worksheet_url = export_sheet(results, spreadsheet, worksheet_title)\n",
    "    print(f\"Worksheet updated: {worksheet_url}\")\n",
    "else:\n",
    "    print(\"No data to export.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "340fc211",
   "metadata": {},
   "source": [
    "### Reset results pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36f6aa11",
   "metadata": {},
   "id": "801b0bdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "worksheet_title = 'Main'\n",
    "\n",
    "results = readPickle(resultsPickle)\n",
    "if results:\n",
    "    worksheet_url = export_sheet(results, spreadsheet, worksheet_title)\n",
    "    print(f\"Worksheet updated: {worksheet_url}\")\n",
    "else:\n",
    "    print(\"No data to export.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "340fc211",
   "metadata": {},
   "source": [
    "### Reset results pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36f6aa11",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = []\n",
    "writePickle(resultsPickle, results)"
    "results = []\n",
    "writePickle(resultsPickle, results)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0df469c1",
   "metadata": {},
   "source": [
    "# QUERY 2: UNICORN COMPANIES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df63090d",
   "metadata": {},
   "outputs": [],
   "source": [
    "historyPickle = 'db/unicorn_history.pickle'\n",
    "toScrapePickle = 'db/unicorn_to_scrape.pickle'\n",
    "resultsPickle = 'db/unicorn_results.pickle'\n",
    "failedURLsTextFile = 'db/unicorn_failed_urls.txt'\n",
    "failedURLsTextFile = 'db/unicorn_failed_urls.txt'\n",
    "\n",
    "SALES_NAV_SEARCH_URL = \"\"\"\n",
    "https://www.linkedin.com/sales/search/people?savedSearchId=1833971588&sessionId=zVKTgf2GQk2owfFVzL9aFQ%3D%3D\n",
    "https://www.linkedin.com/sales/search/people?savedSearchId=1833971588&sessionId=zVKTgf2GQk2owfFVzL9aFQ%3D%3D\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27b37d84",
   "id": "27b37d84",
   "metadata": {},
   "source": [
    "### Sales nav scraper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0acb66e5",
   "id": "0acb66e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "already_scraped_urls, to_scrape_urls = check_and_load_pickles(historyPickle, toScrapePickle)"
    "already_scraped_urls, to_scrape_urls = check_and_load_pickles(historyPickle, toScrapePickle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e042389a",
   "id": "e042389a",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    updated_to_scrape_urls = scrape_profiles(driver, SALES_NAV_SEARCH_URL, already_scraped_urls, to_scrape_urls, toScrapePickle)\n",
    "    print(updated_to_scrape_urls)\n",
    "except Exception as e:\n",
    "    print(f\"An unexpected error occurred: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d974886",
   "id": "2d974886",
   "metadata": {},
   "source": [
    "### Deduplicate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2be3d288",
   "id": "2be3d288",
   "metadata": {},
   "outputs": [],
   "source": [
    "deduplicate(toScrapePickle, to_scrape_urls, already_scraped_urls)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "533babda",
   "id": "533babda",
   "metadata": {},
   "source": [
    "### Profile scraper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4037c0f5",
   "id": "4037c0f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = scrape_all_profiles(driver, historyPickle, toScrapePickle, resultsPickle, failedURLsTextFile)\n",
    "print(\"Successfully scraped: \" + len(results) + \" profiles\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84a58203",
   "metadata": {},
   "source": [
    "### Export data to Google Sheet"
    "results = scrape_all_profiles(driver, historyPickle, toScrapePickle, resultsPickle, failedURLsTextFile)\n",
    "print(\"Successfully scraped: \" + len(results) + \" profiles\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84a58203",
   "metadata": {},
   "source": [
    "### Export data to Google Sheet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "394d56bf",
   "id": "394d56bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "worksheet_title = 'Unicorn'\n",
    "\n",
    "results = readPickle(resultsPickle)\n",
    "if results:\n",
    "    worksheet_url = export_sheet(results, spreadsheet, worksheet_title)\n",
    "    print(f\"Worksheet updated: {worksheet_url}\")\n",
    "else:\n",
    "    print(\"No data to export.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0c47e94",
   "metadata": {},
   "source": [
    "### Reset results pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75387cb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = []\n",
    "writePickle(resultsPickle, results)"
    "worksheet_title = 'Unicorn'\n",
    "\n",
    "results = readPickle(resultsPickle)\n",
    "if results:\n",
    "    worksheet_url = export_sheet(results, spreadsheet, worksheet_title)\n",
    "    print(f\"Worksheet updated: {worksheet_url}\")\n",
    "else:\n",
    "    print(\"No data to export.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0c47e94",
   "metadata": {},
   "source": [
    "### Reset results pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75387cb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = []\n",
    "writePickle(resultsPickle, results)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d71f2e26",
   "metadata": {},
   "source": [
    "# QUERY 3: ACQUIRED COMPANIES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22414fd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "historyPickle = 'db/acquired_history.pickle'\n",
    "toScrapePickle = 'db/acquired_to_scrape.pickle'\n",
    "resultsPickle = 'db/acquired_results.pickle'\n",
    "failedURLsTextFile = 'db/acquired_failed_urls.txt'\n",
    "failedURLsTextFile = 'db/acquired_failed_urls.txt'\n",
    "\n",
    "SALES_NAV_SEARCH_URL = \"\"\"\n",
    "https://www.linkedin.com/sales/search/people?savedSearchId=1833971612&sessionId=R7vv8AiYRXiqMseY1m3CwQ%3D%3D\n",
    "https://www.linkedin.com/sales/search/people?savedSearchId=1833971612&sessionId=R7vv8AiYRXiqMseY1m3CwQ%3D%3D\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "230480d3",
   "id": "230480d3",
   "metadata": {},
   "source": [
    "### Sales nav scraper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "144ec8c4",
   "id": "144ec8c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "already_scraped_urls, to_scrape_urls = check_and_load_pickles(historyPickle, toScrapePickle)"
    "already_scraped_urls, to_scrape_urls = check_and_load_pickles(historyPickle, toScrapePickle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d2186a4",
   "id": "0d2186a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    updated_to_scrape_urls = scrape_profiles(driver, SALES_NAV_SEARCH_URL, already_scraped_urls, to_scrape_urls, toScrapePickle)\n",
    "    print(updated_to_scrape_urls)\n",
    "except Exception as e:\n",
    "    print(f\"An unexpected error occurred: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f10b921",
   "id": "4f10b921",
   "metadata": {},
   "source": [
    "### Deduplicate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6407aaf",
   "id": "c6407aaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "deduplicate(toScrapePickle, to_scrape_urls, already_scraped_urls)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91bb56ec",
   "id": "91bb56ec",
   "metadata": {},
   "source": [
    "### Profile scraper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8db500aa",
   "id": "8db500aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = scrape_all_profiles(driver, historyPickle, toScrapePickle, resultsPickle, failedURLsTextFile)\n",
    "print(\"Successfully scraped: \" + len(results) + \" profiles\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b95b9ac",
   "metadata": {},
   "source": [
    "### Export data to Google Sheet"
    "results = scrape_all_profiles(driver, historyPickle, toScrapePickle, resultsPickle, failedURLsTextFile)\n",
    "print(\"Successfully scraped: \" + len(results) + \" profiles\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b95b9ac",
   "metadata": {},
   "source": [
    "### Export data to Google Sheet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b51cc3ea",
   "id": "b51cc3ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "worksheet_title = 'Acquired'\n",
    "\n",
    "results = readPickle(resultsPickle)\n",
    "if results:\n",
    "    worksheet_url = export_sheet(results, spreadsheet, worksheet_title)\n",
    "    print(f\"Worksheet updated: {worksheet_url}\")\n",
    "else:\n",
    "    print(\"No data to export.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5eb1822b",
   "metadata": {},
   "source": [
    "### Reset results pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b20b6b74",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = []\n",
    "writePickle(resultsPickle, results)"
    "worksheet_title = 'Acquired'\n",
    "\n",
    "results = readPickle(resultsPickle)\n",
    "if results:\n",
    "    worksheet_url = export_sheet(results, spreadsheet, worksheet_title)\n",
    "    print(f\"Worksheet updated: {worksheet_url}\")\n",
    "else:\n",
    "    print(\"No data to export.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5eb1822b",
   "metadata": {},
   "source": [
    "### Reset results pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b20b6b74",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = []\n",
    "writePickle(resultsPickle, results)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b637904",
   "metadata": {},
   "source": [
    "# QUERY 4: VC PORTFOLIO COMPANIES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4efcd7ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "historyPickle = 'db/portfolio_history.pickle'\n",
    "toScrapePickle = 'db/portfolio_to_scrape.pickle'\n",
    "resultsPickle = 'db/portfolio_results.pickle'\n",
    "failedURLsTextFile = 'db/portfolio_failed_urls.txt'\n",
    "failedURLsTextFile = 'db/portfolio_failed_urls.txt'\n",
    "\n",
    "SALES_NAV_SEARCH_URL = \"\"\"\n",
    "https://www.linkedin.com/sales/search/people?savedSearchId=1833971604&sessionId=zVKTgf2GQk2owfFVzL9aFQ%3D%3D\n",
    "https://www.linkedin.com/sales/search/people?savedSearchId=1833971604&sessionId=zVKTgf2GQk2owfFVzL9aFQ%3D%3D\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ea62a01",
   "id": "0ea62a01",
   "metadata": {},
   "source": [
    "### Sales nav scraper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4ed30ad",
   "id": "f4ed30ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "already_scraped_urls, to_scrape_urls = check_and_load_pickles(historyPickle, toScrapePickle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03d091ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    updated_to_scrape_urls = scrape_profiles(driver, SALES_NAV_SEARCH_URL, already_scraped_urls, to_scrape_urls, toScrapePickle)\n",
    "    print(updated_to_scrape_urls)\n",
    "except Exception as e:\n",
    "    print(f\"An unexpected error occurred: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c30f56a2",
   "metadata": {},
   "source": [
    "### Deduplicate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7493b98f",
   "metadata": {},
   "outputs": [],
   "source": [
    "deduplicate(toScrapePickle, to_scrape_urls, already_scraped_urls)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbab3208",
   "metadata": {},
   "source": [
    "### Profile scraper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "034195b0",
   "id": "034195b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = scrape_all_profiles(driver, historyPickle, toScrapePickle, resultsPickle, failedURLsTextFile)\n",
    "print(\"Successfully scraped: \" + len(results) + \" profiles\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "765b48ab",
   "metadata": {},
   "source": [
    "### Export data to Google Sheet"
    "results = scrape_all_profiles(driver, historyPickle, toScrapePickle, resultsPickle, failedURLsTextFile)\n",
    "print(\"Successfully scraped: \" + len(results) + \" profiles\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "765b48ab",
   "metadata": {},
   "source": [
    "### Export data to Google Sheet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0598e466",
   "id": "0598e466",
   "metadata": {},
   "outputs": [],
   "source": [
    "worksheet_title = 'Portfolio'\n",
    "\n",
    "results = readPickle(resultsPickle)\n",
    "if results:\n",
    "    worksheet_url = export_sheet(results, spreadsheet, worksheet_title)\n",
    "    print(f\"Worksheet updated: {worksheet_url}\")\n",
    "else:\n",
    "    print(\"No data to export.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4adf6ef",
   "metadata": {},
   "source": [
    "### Reset results pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a872f11",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = []\n",
    "writePickle(resultsPickle, results)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47d73a6f",
   "metadata": {},
   "source": [
    "# QUERY 5: INFRA COMPANIES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa93ce00",
   "metadata": {},
   "outputs": [],
   "source": [
    "historyPickle = 'db/infra_history.pickle'\n",
    "toScrapePickle = 'db/infra_to_scrape.pickle'\n",
    "resultsPickle = 'db/infra_results.pickle'\n",
    "failedURLsTextFile = 'db/infra_failed_urls.txt'\n",
    "\n",
    "SALES_NAV_SEARCH_URL = \"\"\"\n",
    "https://www.linkedin.com/sales/search/people?savedSearchId=1833971620&sessionId=R7vv8AiYRXiqMseY1m3CwQ%3D%3D\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbe71600",
   "metadata": {},
   "source": [
    "### Sales nav scraper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0314a14",
   "metadata": {},
   "outputs": [],
   "source": [
    "already_scraped_urls, to_scrape_urls = check_and_load_pickles(historyPickle, toScrapePickle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fc220f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    updated_to_scrape_urls = scrape_profiles(driver, SALES_NAV_SEARCH_URL, already_scraped_urls, to_scrape_urls, toScrapePickle)\n",
    "    print(updated_to_scrape_urls)\n",
    "except Exception as e:\n",
    "    print(f\"An unexpected error occurred: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d43c9f14",
   "metadata": {},
   "source": [
    "### Deduplicate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a0ae70c",
   "metadata": {},
   "outputs": [],
   "source": [
    "deduplicate(toScrapePickle, to_scrape_urls, already_scraped_urls)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79887681",
   "metadata": {},
   "source": [
    "### Profile scraper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a88d5b40",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = scrape_all_profiles(driver, historyPickle, toScrapePickle, resultsPickle, failedURLsTextFile)\n",
    "print(\"Successfully scraped: \" + len(results) + \" profiles\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b7e21ab",
   "metadata": {},
   "source": [
    "### Export data to Google Sheet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51a26025",
   "metadata": {},
   "outputs": [],
   "source": [
    "worksheet_title = 'Infra'\n",
    "\n",
    "results = readPickle(resultsPickle)\n",
    "if results:\n",
    "    worksheet_url = export_sheet(results, spreadsheet, worksheet_title)\n",
    "    print(f\"Worksheet updated: {worksheet_url}\")\n",
    "else:\n",
    "    print(\"No data to export.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "277fd0e0",
   "metadata": {},
   "source": [
    "### Reset results pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86ebade9",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = []\n",
    "writePickle(resultsPickle, results)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47185763-6f2b-4f89-9acc-1651887008f6",
   "metadata": {},
   "source": [
    "# EXPORTING RESULTS AS CSV IF NEEDED"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0906d83",
   "metadata": {},
   "outputs": [],
   "source": [
    "def export_to_csv(profiles, file_name):\n",
    "    \"\"\"\n",
    "    Export profile data to a CSV file.\n",
    "\n",
    "    Parameters:\n",
    "    - profiles: A list of dictionaries containing profile data\n",
    "    - file_name: The name of the CSV file to create\n",
    "\n",
    "    This function creates a CSV file with a header row based on the maximum number \n",
    "    of education and experience entries across all profiles, then exports each \n",
    "    profile's data as a row in the CSV file.\n",
    "\n",
    "    Prints:\n",
    "    - A message confirming the export and the file name\n",
    "    \"\"\"\n",
    "    # Prepare the header for the CSV\n",
    "    max_edu = max(len(profile['education']) for profile in profiles)\n",
    "    max_exp = max(len(profile['experiences']) for profile in profiles)\n",
    "    header = [\"Link\", \"Name\", \"Description\", \"Distance\", \"Mutuals\"]\n",
    "    for i in range(max_edu):\n",
    "        header.extend([f\"Edu{i} School\", f\"Edu{i} Degree\", f\"Edu{i} Dates\", f\"Edu{i} Remaining\"])\n",
    "    for i in range(max_exp):\n",
    "        header.extend([f\"Exp{i} Title\", f\"Exp{i} Company\", f\"Exp{i} Dates\", f\"Exp{i} Location\", f\"Exp{i} Summary\", f\"Exp{i} Remaining\"])\n",
    "\n",
    "    # Open a CSV file for writing\n",
    "    with open(file_name, mode='w', newline='', encoding='utf-8') as file:\n",
    "        writer = csv.writer(file)\n",
    "        writer.writerow(header)\n",
    "\n",
    "        # Prepare the rows\n",
    "        for profile in profiles:\n",
    "            row = [profile['profile_link'], profile['name'], profile['profile_description'], profile['profile_dist'], profile['mutuals']]\n",
    "            \n",
    "            # Add education data\n",
    "            for edu in profile['education']:\n",
    "                row.extend([edu.get('school: ', ''), edu.get('degree: ', ''), edu.get('dates: ', ''), edu.get('remaining: ', '')])\n",
    "            # Pad with empty strings if there are fewer education entries than the maximum\n",
    "            row.extend([''] * ((max_edu - len(profile['education'])) * 4))\n",
    "            \n",
    "            # Add experiences data\n",
    "            for exp in profile['experiences']:\n",
    "                row.extend([exp.get('title: ', ''), exp.get('company: ', ''), exp.get('dates: ', ''), exp.get('location: ', ''), exp.get('summary: ', ''), exp.get('remaining: ', '')])\n",
    "            # Pad with empty strings if there are fewer experience entries than the maximum\n",
    "            row.extend([''] * ((max_exp - len(profile['experiences'])) * 6))\n",
    "\n",
    "            # Write the row to the CSV file\n",
    "            writer.writerow(row)\n",
    "\n",
    "    print(f\"Data exported to {file_name}\")\n",
    "\n",
    "# Example usage\n",
    "resultsPickle = 'path/to/your/results.pickle'  # Update with the path to your pickle file\n",
    "main_results = readPickle(resultsPickle)\n",
    "\n",
    "if main_results:\n",
    "    today_date = datetime.today().strftime('%Y-%m-%d')\n",
    "    file_name = f'Linkedin_Scraper_Results_{today_date}.csv'\n",
    "    export_to_csv(main_results, file_name)\n",
    "else:\n",
    "    print(\"No data to export.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ee20a9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# categories = COMPANY_CATEGORIES\n",
    "\n",
    "# # Example color mapping for categories\n",
    "# category_colors = {\n",
    "#     \"SECURITY\": 'red',\n",
    "#     \"OTHER\": 'blue',\n",
    "#     \"PUBLIC\": 'green',\n",
    "#     \"INFRA\": 'yellow',\n",
    "#     \"FINTECH\": 'orange',\n",
    "#     \"CRYPTO\": 'purple',\n",
    "#     \"FRONTIER\": 'cyan',\n",
    "#     \"AI\": 'magenta'\n",
    "# }\n",
    "\n",
    "# # Create a reverse dictionary for easier lookup: {company: category}\n",
    "# company_category = {}\n",
    "# for category, companies in categories.items():\n",
    "#     for company in companies:\n",
    "#         company_category[company] = category\n",
    "\n",
    "# # Modify the style function\n",
    "# def highlight_by_category(val):\n",
    "#     category = company_category.get(val)\n",
    "#     if category:\n",
    "#         color = category_colors.get(category, 'none')  # default to 'none' if no color is specified\n",
    "#     else:\n",
    "#         color = 'none'\n",
    "#     return f'background-color: {color}'\n",
    "\n",
    "# import re\n",
    "# ILLEGAL_CHARACTERS_RE = re.compile(r'[\\000-\\010]|[\\013-\\014]|[\\016-\\037]')\n",
    "# def find_illegal_characters(df):\n",
    "#     for column in df.columns:\n",
    "#         for idx, item in enumerate(df[column]):\n",
    "#             if isinstance(item, str) and ILLEGAL_CHARACTERS_RE.search(item):\n",
    "#                 # replace illegal characters with an empty string\n",
    "#                 df[column][idx] = ILLEGAL_CHARACTERS_RE.sub('', item)\n",
    "#     return df\n",
    "\n",
    "# styled_df = df\n",
    "# styled_df = find_illegal_characters(styled_df)\n",
    "# styled_df = df.style.applymap(highlight_by_category)\n",
    "\n",
    "# # Save the styled DataFrame to an Excel file\n",
    "# # get today's date in MM-DD-YYYY format\n",
    "# import datetime\n",
    "# now = datetime.datetime.now()\n",
    "# date = now.strftime(\"%m-%d-%Y\")\n",
    "# styled_df.to_excel(f'results/{date}_{start}-{end}.xlsx', engine='openpyxl', index=False)\n"
    "# # Save the styled DataFrame to an Excel file\n",
    "# # get today's date in MM-DD-YYYY format\n",
    "# import datetime\n",
    "# now = datetime.datetime.now()\n",
    "# date = now.strftime(\"%m-%d-%Y\")\n",
    "# styled_df.to_excel(f'results/{date}_{start}-{end}.xlsx', engine='openpyxl', index=False)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
