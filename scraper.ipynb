{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "063ef315",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "\n",
    "import os\n",
    "import random\n",
    "import time\n",
    "from linkedin_scraper import Person, actions\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.firefox.options import Options as FirefoxOptions\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2cdabfa0",
   "metadata": {},
   "outputs": [],
   "source": [
    "options = FirefoxOptions()\n",
    "options.add_argument(\"--headless\")\n",
    "driver = webdriver.Firefox(options=options)\n",
    "driver.get(\"https://dev.to\")\n",
    "# actions.login(driver, 'ljiangfbla@gmail.com', 'Sunf1owerC@pit@1!')\n",
    "actions.login(driver, 'jchao2001@gmail.com', 'Spoiler.Neurology.Primarily.Sandstorm.Laziness')\n",
    "\n",
    "def reinstantiate_driver(driver):\n",
    "    print(\"Reinstantiating driver...\")\n",
    "    options = FirefoxOptions()\n",
    "    # options.add_argument(\"--headless\")\n",
    "    driver = webdriver.Firefox(options=options)\n",
    "    driver.get(\"https://dev.to\")\n",
    "    # actions.login(driver, 'ljiangfbla@gmail.com', 'Sunf1owerC@pit@1!')\n",
    "    actions.login(driver, 'jchao2001@gmail.com', 'Spoiler.Neurology.Primarily.Sandstorm.Laziness')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "43859799",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "958\n",
      "https://www.linkedin.com/in/mkrecny\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "\n",
    "with open('already_scraped.pickle', 'rb') as f:\n",
    "  scraped_urls = pickle.load(f)\n",
    "\n",
    "print(len(scraped_urls))\n",
    "print(scraped_urls[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "027fd1fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class Experience():\n",
    "    position_title: str = None\n",
    "    from_date: str = None\n",
    "    to_date: str = None\n",
    "    description: str = None\n",
    "    position_title: str = None\n",
    "    duration: str = None\n",
    "    location: str = None\n",
    "    institution_name: str = None\n",
    "    linkedin_url: str = None\n",
    "\n",
    "class ScrapedProfile:\n",
    "    def __init__(self, profile_name, experiences, profile_school, profile_dist, profile_description, profile_link):\n",
    "        self.profile_name = profile_name\n",
    "        self.experiences = experiences\n",
    "        self.profile_school = profile_school\n",
    "        self.profile_dist = profile_dist\n",
    "        self.profile_description = profile_description\n",
    "        self.profile_link = profile_link\n",
    "\n",
    "def wait_for_element_to_load(by=By.CLASS_NAME, name=\"pv-top-card\", base=None):\n",
    "    base = base or driver\n",
    "    return WebDriverWait(base, 180).until(\n",
    "        EC.presence_of_element_located(\n",
    "            (\n",
    "                by,\n",
    "                name\n",
    "            )\n",
    "        )\n",
    "    )\n",
    "\n",
    "def wait_for_all_elements_to_load(by=By.CLASS_NAME, name=\"pv-top-card\", base=None):\n",
    "    base = base or driver\n",
    "    return WebDriverWait(base, 180).until(\n",
    "        EC.presence_of_all_elements_located(\n",
    "            (\n",
    "                by,\n",
    "                name\n",
    "            )\n",
    "        )\n",
    "    )\n",
    "\n",
    "def get_pvs_list_element(position_summary_text):\n",
    "    if not position_summary_text:\n",
    "        return None\n",
    "    \n",
    "    try:\n",
    "        return position_summary_text.find_element(By.CLASS_NAME,\"pvs-list\").find_element(By.CLASS_NAME,\"pvs-list\")\n",
    "    except:\n",
    "        return position_summary_text.find_element(By.CLASS_NAME,\"pvs-list\")\n",
    "    \n",
    "    return position_summary_text.find_element(By.CLASS_NAME,\"pvs-list\").find_element(By.CLASS_NAME,\"pvs-list\")\n",
    "\n",
    "def get_experiences(driver):\n",
    "    driver.execute_script('alert(\"Focus window\")')\n",
    "    driver.switch_to.alert.accept()\n",
    "    try:\n",
    "        WebDriverWait(driver, 240).until(lambda d: d.execute_script('return document.readyState') == 'complete')\n",
    "        main = wait_for_element_to_load(by=By.TAG_NAME, name=\"main\")\n",
    "    except:\n",
    "        driver = reinstantiate_driver(driver)\n",
    "    \n",
    "    driver.execute_script(\n",
    "                \"window.scrollTo(0, Math.ceil(document.body.scrollHeight/2));\"\n",
    "            )\n",
    "    driver.execute_script(\n",
    "                \"window.scrollTo(0, document.body.scrollHeight);\"\n",
    "            )\n",
    "\n",
    "    main_list = wait_for_element_to_load(name=\"pvs-list\", base=main)\n",
    "    experiences = []\n",
    "\n",
    "    for position in main_list.find_elements(By.XPATH,\"li\"):\n",
    "        position = position.find_element(By.CLASS_NAME,\"pvs-entity\")\n",
    "        company_logo_elem, position_details = position.find_elements(By.XPATH,\"*\")\n",
    "\n",
    "        # company elem\n",
    "        company_linkedin_url = company_logo_elem.find_element(By.XPATH,\"*\").get_attribute(\"href\")\n",
    "\n",
    "        # position details\n",
    "        position_details_list = position_details.find_elements(By.XPATH,\"*\")\n",
    "        position_summary_details = position_details_list[0] if len(position_details_list) > 0 else None\n",
    "        position_summary_text = position_details_list[1] if len(position_details_list) > 1 else None\n",
    "        outer_positions = position_summary_details.find_element(By.XPATH,\"*\").find_elements(By.XPATH,\"*\")\n",
    "        work_times = ''\n",
    "        \n",
    "        if len(outer_positions) == 4:\n",
    "            # position_title = outer_positions[0].find_element(By.TAG_NAME,\"span\").find_element(By.TAG_NAME,\"span\").text\n",
    "            position_title = outer_positions[0].find_element(By.TAG_NAME,\"span\").text\n",
    "            company = outer_positions[1].find_element(By.TAG_NAME,\"span\").text\n",
    "            work_times = outer_positions[2].find_element(By.TAG_NAME,\"span\").text\n",
    "            location = outer_positions[3].find_element(By.TAG_NAME,\"span\").text\n",
    "        elif len(outer_positions) == 3:\n",
    "            if \"·\" in outer_positions[2].text:\n",
    "                # position_title = outer_positions[0].find_element(By.TAG_NAME,\"span\").find_element(By.TAG_NAME,\"span\").text\n",
    "                position_title = outer_positions[0].find_element(By.TAG_NAME,\"span\").text                \n",
    "                company = outer_positions[1].find_element(By.TAG_NAME,\"span\").text\n",
    "                work_times = outer_positions[2].find_element(By.TAG_NAME,\"span\").text\n",
    "                location = \"\"\n",
    "            else:\n",
    "                position_title = \"\"\n",
    "                # company = outer_positions[0].find_element(By.TAG_NAME,\"span\").find_element(By.TAG_NAME,\"span\").text\n",
    "                company = outer_positions[0].find_element(By.TAG_NAME,\"span\").text\n",
    "                work_times = outer_positions[1].find_element(By.TAG_NAME,\"span\").text\n",
    "                location = outer_positions[2].find_element(By.TAG_NAME,\"span\").text\n",
    "        elif len(outer_positions) == 2:\n",
    "            company = outer_positions[0].text.split('\\n')[0]\n",
    "            # duration = outer_positions[1].text.split('\\n')[0].split(\"·\")[1].strip()\n",
    "\n",
    "        company = company.split(' · ')[0] # 6/14/23 added this line to handle showing \"full-time\" in company name\n",
    "        times = work_times.split(\"·\")[0].strip() if work_times else \"\"\n",
    "        duration = work_times.split(\"·\")[1].strip() if len(work_times.split(\"·\")) > 1 else None\n",
    "\n",
    "        from_date = \" \".join(times.split(\" \")[:2]) if times else \"\"\n",
    "        to_date = \" \".join(times.split(\" \")[3:]) if times else \"\"\n",
    "        \n",
    "        pvs_list_element = get_pvs_list_element(position_summary_text)\n",
    "\n",
    "        if position_summary_text and len(pvs_list_element.find_elements(By.XPATH,\"li\")) > 1:\n",
    "            descriptions = pvs_list_element.find_elements(By.XPATH,\"li\")\n",
    "            for description in descriptions:\n",
    "                res = description.find_element(By.TAG_NAME,\"a\").find_elements(By.XPATH,\"*\")\n",
    "                position_title_elem = res[0] if len(res) > 0 else None\n",
    "                work_times_elem = res[1] if len(res) > 1 else None\n",
    "                location_elem = res[2] if len(res) > 2 else None\n",
    "\n",
    "                location = location_elem.find_element(By.XPATH,\"*\").text if location_elem else None\n",
    "                position_title = position_title_elem.find_element(By.XPATH,\"*\").find_element(By.TAG_NAME,\"*\").text if position_title_elem else \"\"\n",
    "                work_times = work_times_elem.find_element(By.XPATH,\"*\").text if work_times_elem else \"\"\n",
    "                times = work_times.split(\"·\")[0].strip() if work_times else \"\"\n",
    "                duration = work_times.split(\"·\")[1].strip() if len(work_times.split(\"·\")) > 1 else None\n",
    "                from_date = \" \".join(times.split(\" \")[:2]) if times else \"\"\n",
    "                to_date = \" \".join(times.split(\" \")[3:]) if times else \"\"\n",
    "\n",
    "                experience = Experience(\n",
    "                    position_title=position_title,\n",
    "                    from_date=from_date,\n",
    "                    to_date=to_date,\n",
    "                    duration=duration,\n",
    "                    location=location,\n",
    "                    description=description,\n",
    "                    institution_name=company,\n",
    "                    linkedin_url=company_linkedin_url\n",
    "                )\n",
    "                experiences.append(experience)\n",
    "        else:\n",
    "            description = position_summary_text.text if position_summary_text else \"\"\n",
    "\n",
    "            experience = Experience(\n",
    "                position_title=position_title,\n",
    "                from_date=from_date,\n",
    "                to_date=to_date,\n",
    "                duration=duration,\n",
    "                location=location,\n",
    "                description=description,\n",
    "                institution_name=company,\n",
    "                linkedin_url=company_linkedin_url\n",
    "            )\n",
    "            experiences.append(experience)\n",
    "    return experiences\n",
    "\n",
    "def scrape_profile_live_filtering(driver, profile_link):\n",
    "    \n",
    "    experiences_url = os.path.join(profile_link, \"details/experience\")\n",
    "    print(experiences_url)\n",
    "    driver.get(experiences_url)\n",
    "    try:\n",
    "        WebDriverWait(driver, 240).until(lambda d: d.execute_script('return document.readyState') == 'complete')\n",
    "    except:\n",
    "        driver = reinstantiate_driver(driver)\n",
    "    time.sleep(5 + random.random() * 10)\n",
    "    experiences = get_experiences(driver)\n",
    "    \n",
    "    # FILTERING\n",
    "    # found_target_company = False\n",
    "    # if len(experiences) > 1 and experiences[0].duration in RECENT_LIST and is_likely_startup(experiences[0]):\n",
    "    #     for experience in experiences[1:5]:\n",
    "    #         company = experience.institution_name.lower()\n",
    "    #         for target_company in COMPANY_LIST:\n",
    "    #             if target_company.lower() in company:\n",
    "    #                 filter_company_match_dict[profile_link] = target_company\n",
    "    #                 found_target_company = True\n",
    "    #                 break\n",
    "    # if not found_target_company:\n",
    "    #     return None\n",
    "    \n",
    "    person_obj = Person(profile_link, driver = driver, scrape=False, experiences = [None])\n",
    "    try:\n",
    "        WebDriverWait(driver, 240).until(lambda d: d.execute_script('return document.readyState') == 'complete')\n",
    "    except:\n",
    "        driver = reinstantiate_driver(driver)\n",
    "    time.sleep(2 + random.random() * 7)\n",
    "    \n",
    "    # name\n",
    "    profile_name = driver.find_element(By.CLASS_NAME, \"text-heading-xlarge\").text\n",
    "    time.sleep(1 + random.random())\n",
    "\n",
    "    # education\n",
    "    education = []\n",
    "    edu_section = driver.find_element(By.ID, \"education\")\n",
    "    parent_element = edu_section.find_element(By.XPATH, \"./..\")\n",
    "    entries = parent_element.find_elements(By.CLASS_NAME, \"pvs-entity\")\n",
    "    for entry in entries:\n",
    "        elem = entry.find_elements(By.CLASS_NAME, \"visually-hidden\")\n",
    "        education.append({\"school\": elem[0].text, \"degree\": elem[1].text})\n",
    "    time.sleep(1 + random.random())\n",
    "\n",
    "    # degree of connection\n",
    "    profile_dist = driver.find_element(By.CLASS_NAME, \"dist-value\").text\n",
    "    time.sleep(1 + random.random())\n",
    "\n",
    "    # description\n",
    "    profile_description = driver.find_element(By.CLASS_NAME, \"text-body-medium\").text\n",
    "    time.sleep(1 + random.random())\n",
    "\n",
    "    # profile link\n",
    "    profile_link = driver.current_url\n",
    "    time.sleep(1 + random.random())\n",
    "    \n",
    "    profile = ScrapedProfile(profile_name,\n",
    "                   experiences,\n",
    "                   education,\n",
    "                   profile_dist,\n",
    "                   profile_description,\n",
    "                   profile_link)\n",
    "    return profile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "5e0ea7d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('to_scrape.pickle', 'rb') as f:\n",
    "  to_scrape_urls = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "9365f664",
   "metadata": {},
   "outputs": [],
   "source": [
    "url = \"https://www.linkedin.com/sales/search/people#query=(recentSearchParam%3A(id%3A2703706810%2CdoLogHistory%3Atrue)%2Cfilters%3AList((type%3ACURRENT_COMPANY%2Cvalues%3AList((id%3Aurn%253Ali%253Aorganization%253A18583501%2Ctext%3AStealth%2520Startup%2CselectionType%3AINCLUDED%2Cparent%3A(id%3A0))%2C(id%3Aurn%253Ali%253Aorganization%253A18016269%2Ctext%3AStealth%2520Mode%2CselectionType%3AINCLUDED%2Cparent%3A(id%3A0))%2C(id%3Aurn%253Ali%253Aorganization%253A91313799%2Ctext%3AStealth%2CselectionType%3AINCLUDED%2Cparent%3A(id%3A0))))%2C(type%3APAST_COMPANY%2Cvalues%3AList((id%3Aurn%253Ali%253Aorganization%253A1815218%2Ctext%3AUber%2CselectionType%3AINCLUDED%2Cparent%3A(id%3A0))%2C(id%3Aurn%253Ali%253Aorganization%253A309694%2Ctext%3AAirbnb%2CselectionType%3AINCLUDED%2Cparent%3A(id%3A0))%2C(id%3Aurn%253Ali%253Aorganization%253A2857634%2Ctext%3ACoinbase%2CselectionType%3AINCLUDED%2Cparent%3A(id%3A0))%2C(id%3Aurn%253Ali%253Aorganization%253A2135371%2Ctext%3AStripe%2CselectionType%3AINCLUDED%2Cparent%3A(id%3A0))%2C(id%3Aurn%253Ali%253Aorganization%253A20708%2Ctext%3APalantir%2520Technologies%2CselectionType%3AINCLUDED%2Cparent%3A(id%3A0))%2C(id%3Aurn%253Ali%253Aorganization%253A3608%2Ctext%3ANVIDIA%2CselectionType%3AINCLUDED%2Cparent%3A(id%3A0))%2C(id%3Aurn%253Ali%253Aorganization%253A3205573%2Ctext%3ADoorDash%2CselectionType%3AINCLUDED%2Cparent%3A(id%3A0))%2C(id%3Aurn%253Ali%253Aorganization%253A675562%2Ctext%3ASquare%2CselectionType%3AINCLUDED%2Cparent%3A(id%3A0))%2C(id%3Aurn%253Ali%253Aorganization%253A30846%2Ctext%3ASpaceX%2CselectionType%3AINCLUDED%2Cparent%3A(id%3A0))%2C(id%3Aurn%253Ali%253Aorganization%253A30086%2Ctext%3APalo%2520Alto%2520Networks%2CselectionType%3AINCLUDED%2Cparent%3A(id%3A0))%2C(id%3Aurn%253Ali%253Aorganization%253A3131483%2Ctext%3AFlexport%2CselectionType%3AINCLUDED%2Cparent%3A(id%3A0))%2C(id%3Aurn%253Ali%253Aorganization%253A3477522%2Ctext%3ADatabricks%2CselectionType%3AINCLUDED%2Cparent%3A(id%3A0))%2C(id%3Aurn%253Ali%253Aorganization%253A748731%2Ctext%3AKlarna%2CselectionType%3AINCLUDED%2Cparent%3A(id%3A0))%2C(id%3Aurn%253Ali%253Aorganization%253A3254263%2Ctext%3ARobinhood%2CselectionType%3AINCLUDED%2Cparent%3A(id%3A0))%2C(id%3Aurn%253Ali%253Aorganization%253A6575553%2Ctext%3AByteDance%2CselectionType%3AINCLUDED%2Cparent%3A(id%3A0))%2C(id%3Aurn%253Ali%253Aorganization%253A18505670%2Ctext%3ABrex%2CselectionType%3AINCLUDED%2Cparent%3A(id%3A0))%2C(id%3Aurn%253Ali%253Aorganization%253A2732417%2Ctext%3AInstacart%2CselectionType%3AINCLUDED%2Cparent%3A(id%3A0))%2C(id%3Aurn%253Ali%253Aorganization%253A17998520%2Ctext%3AScale%2520AI%2CselectionType%3AINCLUDED%2Cparent%3A(id%3A0))%2C(id%3Aurn%253Ali%253Aorganization%253A2684737%2Ctext%3APlaid%2CselectionType%3AINCLUDED%2Cparent%3A(id%3A0))%2C(id%3Aurn%253Ali%253Aorganization%253A3767529%2Ctext%3ANubank%2CselectionType%3AINCLUDED%2Cparent%3A(id%3A0))%2C(id%3Aurn%253Ali%253Aorganization%253A3991822%2Ctext%3AAirtable%2CselectionType%3AINCLUDED%2Cparent%3A(id%3A0))%2C(id%3Aurn%253Ali%253Aorganization%253A10043614%2Ctext%3ASnyk%2CselectionType%3AINCLUDED%2Cparent%3A(id%3A0))%2C(id%3Aurn%253Ali%253Aorganization%253A10607336%2Ctext%3AChainalysis%2CselectionType%3AINCLUDED%2Cparent%3A(id%3A0))%2C(id%3Aurn%253Ali%253Aorganization%253A10893210%2Ctext%3Adbt%2520Labs%2CselectionType%3AINCLUDED%2Cparent%3A(id%3A0))%2C(id%3Aurn%253Ali%253Aorganization%253A11062162%2Ctext%3AGrafana%2520Labs%2CselectionType%3AINCLUDED%2Cparent%3A(id%3A0))%2C(id%3Aurn%253Ali%253Aorganization%253A11130470%2Ctext%3AOpenAI%2CselectionType%3AINCLUDED%2Cparent%3A(id%3A0))%2C(id%3Aurn%253Ali%253Aorganization%253A11193683%2Ctext%3AHugging%2520Face%2CselectionType%3AINCLUDED%2Cparent%3A(id%3A0))%2C(id%3Aurn%253Ali%253Aorganization%253A11247457%2Ctext%3ASolugen%2CselectionType%3AINCLUDED%2Cparent%3A(id%3A0))%2C(id%3Aurn%253Ali%253Aorganization%253A11741116%2Ctext%3ARunway%2CselectionType%3AINCLUDED%2Cparent%3A(id%3A0))%2C(id%3Aurn%253Ali%253Aorganization%253A11869260%2Ctext%3ARetool%2CselectionType%3AINCLUDED%2Cparent%3A(id%3A0))%2C(id%3Aurn%253Ali%253Aorganization%253A1406226%2Ctext%3ARamp%2CselectionType%3AINCLUDED%2Cparent%3A(id%3A0))%2C(id%3Aurn%253Ali%253Aorganization%253A14824547%2Ctext%3AFireblocks%2CselectionType%3AINCLUDED%2Cparent%3A(id%3A0))%2C(id%3Aurn%253Ali%253Aorganization%253A1594050%2Ctext%3AGoogle%2520DeepMind%2CselectionType%3AINCLUDED%2Cparent%3A(id%3A0))%2C(id%3Aurn%253Ali%253Aorganization%253A16181286%2Ctext%3AVercel%2CselectionType%3AINCLUDED%2Cparent%3A(id%3A0))%2C(id%3Aurn%253Ali%253Aorganization%253A17932068%2Ctext%3ALacework%2CselectionType%3AINCLUDED%2Cparent%3A(id%3A0))%2C(id%3Aurn%253Ali%253Aorganization%253A17988315%2Ctext%3ARippling%2CselectionType%3AINCLUDED%2Cparent%3A(id%3A0))%2C(id%3Aurn%253Ali%253Aorganization%253A18013280%2Ctext%3AFaire%2CselectionType%3AINCLUDED%2Cparent%3A(id%3A0))%2C(id%3Aurn%253Ali%253Aorganization%253A18293159%2Ctext%3AAnduril%2520Industries%2CselectionType%3AINCLUDED%2Cparent%3A(id%3A0))%2C(id%3Aurn%253Ali%253Aorganization%253A18309569%2Ctext%3ASemgrep%2CselectionType%3AINCLUDED%2Cparent%3A(id%3A0))%2C(id%3Aurn%253Ali%253Aorganization%253A18586257%2Ctext%3AAbnormal%2520Security%2CselectionType%3AINCLUDED%2Cparent%3A(id%3A0))%2C(id%3Aurn%253Ali%253Aorganization%253A18593641%2Ctext%3AWeights%2520%2526%2520Biases%2CselectionType%3AINCLUDED%2Cparent%3A(id%3A0))%2C(id%3Aurn%253Ali%253Aorganization%253A18742807%2Ctext%3ATRM%2520Labs%2CselectionType%3AINCLUDED%2Cparent%3A(id%3A0))%2C(id%3Aurn%253Ali%253Aorganization%253A18769344%2Ctext%3AModern%2520Treasury%2CselectionType%3AINCLUDED%2Cparent%3A(id%3A0))%2C(id%3Aurn%253Ali%253Aorganization%253A18777798%2Ctext%3ACribl%2CselectionType%3AINCLUDED%2Cparent%3A(id%3A0))%2C(id%3Aurn%253Ali%253Aorganization%253A18922914%2Ctext%3ADeel%2CselectionType%3AINCLUDED%2Cparent%3A(id%3A0))%2C(id%3Aurn%253Ali%253Aorganization%253A19107985%2Ctext%3AMercury%2CselectionType%3AINCLUDED%2Cparent%3A(id%3A0))%2C(id%3Aurn%253Ali%253Aorganization%253A24024765%2Ctext%3ACohere%2CselectionType%3AINCLUDED%2Cparent%3A(id%3A0))%2C(id%3Aurn%253Ali%253Aorganization%253A2418251%2Ctext%3AZapier%2CselectionType%3AINCLUDED%2Cparent%3A(id%3A0))%2C(id%3Aurn%253Ali%253Aorganization%253A2497653%2Ctext%3ACrowdStrike%2CselectionType%3AINCLUDED%2Cparent%3A(id%3A0))%2C(id%3Aurn%253Ali%253Aorganization%253A27159855%2Ctext%3AStarburst%2CselectionType%3AINCLUDED%2Cparent%3A(id%3A0))%2C(id%3Aurn%253Ali%253Aorganization%253A2850862%2Ctext%3ACanva%2CselectionType%3AINCLUDED%2Cparent%3A(id%3A0))%2C(id%3Aurn%253Ali%253Aorganization%253A30898036%2Ctext%3ANotion%2CselectionType%3AINCLUDED%2Cparent%3A(id%3A0))%2C(id%3Aurn%253Ali%253Aorganization%253A3502352%2Ctext%3AWebflow%2CselectionType%3AINCLUDED%2Cparent%3A(id%3A0))%2C(id%3Aurn%253Ali%253Aorganization%253A35462987%2Ctext%3AVanta%2CselectionType%3AINCLUDED%2Cparent%3A(id%3A0))%2C(id%3Aurn%253Ali%253Aorganization%253A3650502%2Ctext%3AFigma%2CselectionType%3AINCLUDED%2Cparent%3A(id%3A0))%2C(id%3Aurn%253Ali%253Aorganization%253A37564254%2Ctext%3APersona%2CselectionType%3AINCLUDED%2Cparent%3A(id%3A0))%2C(id%3Aurn%253Ali%253Aorganization%253A3769390%2Ctext%3ABenchling%2CselectionType%3AINCLUDED%2Cparent%3A(id%3A0))%2C(id%3Aurn%253Ali%253Aorganization%253A3954657%2Ctext%3AFivetran%2CselectionType%3AINCLUDED%2Cparent%3A(id%3A0))%2C(id%3Aurn%253Ali%253Aorganization%253A40671813%2Ctext%3ARobust%2520Intelligence%2CselectionType%3AINCLUDED%2Cparent%3A(id%3A0))%2C(id%3Aurn%253Ali%253Aorganization%253A4803356%2Ctext%3ASourcegraph%2CselectionType%3AINCLUDED%2Cparent%3A(id%3A0))%2C(id%3Aurn%253Ali%253Aorganization%253A6424460%2Ctext%3ASentry%2520%2528sentry.io%2529%2CselectionType%3AINCLUDED%2Cparent%3A(id%3A0))%2C(id%3Aurn%253Ali%253Aorganization%253A64890982%2Ctext%3AWiz%2CselectionType%3AINCLUDED%2Cparent%3A(id%3A0))%2C(id%3Aurn%253Ali%253Aorganization%253A65281968%2Ctext%3ATecton%2CselectionType%3AINCLUDED%2Cparent%3A(id%3A0))%2C(id%3Aurn%253Ali%253Aorganization%253A65638805%2Ctext%3AMaterial%2520Security%2CselectionType%3AINCLUDED%2Cparent%3A(id%3A0))%2C(id%3Aurn%253Ali%253Aorganization%253A67081245%2Ctext%3ATemporal%2520Technologies%2CselectionType%3AINCLUDED%2Cparent%3A(id%3A0))%2C(id%3Aurn%253Ali%253Aorganization%253A68023390%2Ctext%3AIsland%2CselectionType%3AINCLUDED%2Cparent%3A(id%3A0))%2C(id%3Aurn%253Ali%253Aorganization%253A68047275%2Ctext%3AUniswap%2520Labs%2CselectionType%3AINCLUDED%2Cparent%3A(id%3A0))%2C(id%3Aurn%253Ali%253Aorganization%253A70975817%2Ctext%3AVarda%2520Space%2520Industries%2CselectionType%3AINCLUDED%2Cparent%3A(id%3A0))%2C(id%3Aurn%253Ali%253Aorganization%253A71668100%2Ctext%3AHadrian%2CselectionType%3AINCLUDED%2Cparent%3A(id%3A0))%2C(id%3Aurn%253Ali%253Aorganization%253A74126343%2Ctext%3AAnthropic%2CselectionType%3AINCLUDED%2Cparent%3A(id%3A0))%2C(id%3Aurn%253Ali%253Aorganization%253A74882602%2Ctext%3AGlean%2CselectionType%3AINCLUDED%2Cparent%3A(id%3A0))%2C(id%3Aurn%253Ali%253Aorganization%253A7602863%2Ctext%3AZipline%2CselectionType%3AINCLUDED%2Cparent%3A(id%3A0))%2C(id%3Aurn%253Ali%253Aorganization%253A76262108%2Ctext%3AKumo.AI%2CselectionType%3AINCLUDED%2Cparent%3A(id%3A0))%2C(id%3Aurn%253Ali%253Aorganization%253A80114151%2Ctext%3AClickHouse%2CselectionType%3AINCLUDED%2Cparent%3A(id%3A0))%2C(id%3Aurn%253Ali%253Aorganization%253A81330326%2Ctext%3AAdept%2CselectionType%3AINCLUDED%2Cparent%3A(id%3A0))%2C(id%3Aurn%253Ali%253Aorganization%253A81491861%2Ctext%3APredibase%2CselectionType%3AINCLUDED%2Cparent%3A(id%3A0))%2C(id%3Aurn%253Ali%253Aorganization%253A82318617%2Ctext%3AMidjourney%2CselectionType%3AINCLUDED%2Cparent%3A(id%3A0))%2C(id%3Aurn%253Ali%253Aorganization%253A83019124%2Ctext%3AEigenLayer%2CselectionType%3AINCLUDED%2Cparent%3A(id%3A0))%2C(id%3Aurn%253Ali%253Aorganization%253A89486558%2Ctext%3ACharacter.AI%2CselectionType%3AINCLUDED%2Cparent%3A(id%3A0))%2C(id%3Aurn%253Ali%253Aorganization%253A89962189%2Ctext%3AThe%2520Arbitrum%2520Foundation%2CselectionType%3AINCLUDED%2Cparent%3A(id%3A0))%2C(id%3Aurn%253Ali%253Aorganization%253A926041%2Ctext%3AOkta%2CselectionType%3AINCLUDED%2Cparent%3A(id%3A0))%2C(id%3Aurn%253Ali%253Aorganization%253A9309408%2Ctext%3ACockroach%2520Labs%2CselectionType%3AINCLUDED%2Cparent%3A(id%3A0))))%2C(type%3ALEAD_INTERACTIONS%2Cvalues%3AList((id%3ALIVP%2Ctext%3AViewed%2520profile%2CselectionType%3AEXCLUDED)%2C(id%3ALIMP%2Ctext%3AMessaged%2CselectionType%3AEXCLUDED)))%2C(type%3AYEARS_IN_CURRENT_POSITION%2Cvalues%3AList((id%3A1%2Ctext%3ALess%2520than%25201%2520year%2CselectionType%3AINCLUDED)))%2C(type%3AYEARS_AT_CURRENT_COMPANY%2Cvalues%3AList((id%3A1%2Ctext%3ALess%2520than%25201%2520year%2CselectionType%3AINCLUDED)))))&sessionId=kRjHJsRMT2W5DI61EZQXnQ%3D%3D&viewAllFilters=true\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "7c0da654",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "skipping (error)\n",
      "skipping (error)\n",
      "skipping (error)\n",
      "skipping (error)\n",
      "skipping (error)\n",
      "skipping (error)\n",
      "skipping (error)\n",
      "skipping (error)\n",
      "['https://www.linkedin.com/in/ancoyle', 'https://www.linkedin.com/in/tyler-lehman', 'https://www.linkedin.com/in/aman0456', 'https://www.linkedin.com/in/jovanaj', 'https://www.linkedin.com/in/trujano', 'https://www.linkedin.com/in/sdhn15', 'https://www.linkedin.com/in/sairaamv', 'https://www.linkedin.com/in/sagarkhadabadi', 'https://www.linkedin.com/in/jad-zeineddine-3a723bba', 'https://www.linkedin.com/in/yi-li-1281a228', 'https://www.linkedin.com/in/pankajsinha5', 'https://www.linkedin.com/in/cpbarton', 'https://www.linkedin.com/in/prescott-rynewicz-36295182', 'https://www.linkedin.com/in/tedghaffarian', 'https://www.linkedin.com/in/lizlaird', 'https://www.linkedin.com/in/kashav', 'https://www.linkedin.com/in/kaiserjane', 'https://www.linkedin.com/in/fahd-ahmed-11b890103', 'https://www.linkedin.com/in/sharon-naftaly', 'https://www.linkedin.com/in/rishabhgupta42', 'https://www.linkedin.com/in/maxence-goupilleau-0042b948', 'https://www.linkedin.com/in/xiaoxi-celia-lyu', 'https://www.linkedin.com/in/venkatks', 'https://www.linkedin.com/in/fariz-rahman', 'https://www.linkedin.com/in/mohita-arora-45864886']\n"
     ]
    }
   ],
   "source": [
    "driver.get(url)\n",
    "\n",
    "try:\n",
    "    WebDriverWait(driver, 240).until(lambda d: d.execute_script(\n",
    "        'return document.readyState') == 'complete')\n",
    "except:\n",
    "    print(\"webdriver error\")\n",
    "time.sleep(2 + random.random() * 6)\n",
    "\n",
    "for i in range(0, 2):\n",
    "    try:\n",
    "        while True:\n",
    "            profiles = driver.find_elements(By.CLASS_NAME, \"artdeco-list__item\")\n",
    "\n",
    "            for profile in profiles:\n",
    "                # scroll to the profile\n",
    "                driver.execute_script(\"arguments[0].scrollIntoView();\", profile)\n",
    "                wait_for_element_to_load(name=\"artdeco-entity-lockup__title\")\n",
    "\n",
    "                # click the profile\n",
    "                salesNavOpenProfileButton = profile.find_element(By.CLASS_NAME, \"artdeco-entity-lockup__title\")\n",
    "                salesNavOpenProfileButton.click()\n",
    "                wait_for_element_to_load(name=\"_actions-container_1dg5u8\")\n",
    "                time.sleep(2 + random.random() * 6)\n",
    "                \n",
    "                try:\n",
    "                    # click the three dots button on the salesnav popout\n",
    "                    actionContainer = driver.find_element(By.CLASS_NAME, \"_actions-container_1dg5u8\")\n",
    "                    threeDotsButton = actionContainer.find_element(By.CLASS_NAME, \"_icon_ps32ck\")\n",
    "                    threeDotsButton.click()\n",
    "                    wait_for_element_to_load(name=\"_visible_x5gf48\")\n",
    "                    time.sleep(2 + random.random() * 6)\n",
    "\n",
    "                    # get an <a> tag which is a child of dropdown menu\n",
    "                    dropdownContainer = driver.find_element(By.CLASS_NAME, \"_visible_x5gf48\")\n",
    "                    normalLinkedInUrl = dropdownContainer.find_elements(By.TAG_NAME, \"a\")[1].get_attribute(\"href\")\n",
    "                    wait_for_element_to_load(name=\"artdeco-pagination__button--next\")\n",
    "\n",
    "                    if (normalLinkedInUrl in scraped_urls):\n",
    "                        print(\"skipping (already scraped) \" + normalLinkedInUrl)\n",
    "                    else:\n",
    "                        to_scrape_urls.append(normalLinkedInUrl)\n",
    "                        with open('to_scrape.pickle', 'wb') as f:\n",
    "                            pickle.dump(to_scrape_urls, f)\n",
    "                        print(normalLinkedInUrl)\n",
    "\n",
    "                    # close the popout\n",
    "                    header = driver.find_element(By.CLASS_NAME, \"_inline-sidesheet-header-actions_1cn7lg\")\n",
    "                    button = header.find_elements(By.CLASS_NAME, \"_button_ps32ck\")[1]\n",
    "                    button.click()\n",
    "                except:\n",
    "                    print(\"skipping (error)\")\n",
    "\n",
    "            # navigate to next page\n",
    "            if (len(profiles) < 25):\n",
    "                break\n",
    "            nextPageButton = driver.find_element(By.CLASS_NAME, \"artdeco-pagination__button--next\")\n",
    "            nextPageButton.click()\n",
    "            wait_for_element_to_load(name=\"artdeco-list__item\")\n",
    "            time.sleep(2 + random.random() * 6)\n",
    "    except:\n",
    "        print(\"looping...\")\n",
    "        driver = reinstantiate_driver(driver)\n",
    "\n",
    "print(to_scrape_urls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0ea0fe49",
   "metadata": {},
   "outputs": [],
   "source": [
    "# deduplicate to_scrape_urls\n",
    "with open('to_scrape.pickle', 'wb') as f:\n",
    "    to_scrape_urls = list(set(to_scrape_urls))\n",
    "    pickle.dump(to_scrape_urls, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a74ec3c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "my_list = []\n",
    "\n",
    "with open('scraped.pickle', 'wb') as f:\n",
    "    pickle.dump(my_list, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92e50d5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "candidates = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "7187f14e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scraped Urls: ['https://www.linkedin.com/in/sharon-naftaly', 'https://www.linkedin.com/in/mohita-arora-45864886', 'https://www.linkedin.com/in/prescott-rynewicz-36295182', 'https://www.linkedin.com/in/fahd-ahmed-11b890103', 'https://www.linkedin.com/in/yi-li-1281a228', 'https://www.linkedin.com/in/tyler-lehman', 'https://www.linkedin.com/in/fariz-rahman', 'https://www.linkedin.com/in/sairaamv', 'https://www.linkedin.com/in/kaiserjane', 'https://www.linkedin.com/in/jad-zeineddine-3a723bba', 'https://www.linkedin.com/in/maxence-goupilleau-0042b948', 'https://www.linkedin.com/in/trujano', 'https://www.linkedin.com/in/rishabhgupta42', 'https://www.linkedin.com/in/sdhn15', 'https://www.linkedin.com/in/sagarkhadabadi', 'https://www.linkedin.com/in/kashav', 'https://www.linkedin.com/in/venkatks', 'https://www.linkedin.com/in/ancoyle', 'https://www.linkedin.com/in/jovanaj']\n",
      "To Scrape Urls: len:2 ['https://www.linkedin.com/in/cpbarton', 'https://www.linkedin.com/in/tedghaffarian']\n",
      "At index: 0 - url: https://www.linkedin.com/in/cpbarton\n",
      "https://www.linkedin.com/in/cpbarton/details/experience\n",
      "list index out of range\n",
      "Failed to scrape profile:  https://www.linkedin.com/in/cpbarton\n",
      "At index: 1 - url: https://www.linkedin.com/in/tedghaffarian\n",
      "https://www.linkedin.com/in/tedghaffarian/details/experience\n",
      "list index out of range\n",
      "Failed to scrape profile:  https://www.linkedin.com/in/tedghaffarian\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import pickle\n",
    "\n",
    "scraped_urls = []\n",
    "to_scrape_urls = []\n",
    "\n",
    "with open('to_scrape.pickle', 'rb') as f:\n",
    "    to_scrape_urls = pickle.load(f)\n",
    "with open('scraped.pickle', 'rb') as f:\n",
    "    scraped_urls = pickle.load(f)\n",
    "\n",
    "print(f'Scraped Urls: {scraped_urls}')\n",
    "print(f'To Scrape Urls: len:{len(to_scrape_urls)} {to_scrape_urls}')\n",
    "\n",
    "start = 0\n",
    "end = len(to_scrape_urls)\n",
    "\n",
    "for idx, url in enumerate(to_scrape_urls.copy()):\n",
    "  if start > idx:\n",
    "    continue\n",
    "  if idx >= end:\n",
    "    break\n",
    "  print(f'At index: {idx} - url: {url}') \n",
    "  \n",
    "  # scrape profiles, and write results to a file\n",
    "  try:\n",
    "    profile = scrape_profile_live_filtering(driver, url)\n",
    "\n",
    "    print(\"saving profile info\", end=\"\")\n",
    "    candidates.append(profile)\n",
    "    \n",
    "    print(\"; recording scraped url\", end=\"\")\n",
    "    url = url.strip().strip('/')\n",
    "    scraped_urls.append(url)\n",
    "    with open('scraped.pickle', 'wb') as f:\n",
    "      pickle.dump(scraped_urls, f)\n",
    "    \n",
    "    print(\"; removing from to-scrape\", end=\"\")\n",
    "    to_scrape_urls.remove(url)\n",
    "    with open('to_scrape.pickle', 'wb') as f:\n",
    "      pickle.dump(to_scrape_urls, f)\n",
    "\n",
    "    print(\"; success!\")\n",
    "    print(((idx+1)/len(to_scrape_urls)) * 100, '% Done - at index:', idx)\n",
    "  except Exception as e:\n",
    "    print(e)\n",
    "    print('Failed to scrape profile: ', url)\n",
    "    with open('failed_urls.txt', 'a') as f:\n",
    "      f.write(url + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "8bd44fc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.DataFrame(columns=[\"url\", \"name\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "f0906d83",
   "metadata": {},
   "outputs": [],
   "source": [
    "def parseCandidate(x):\n",
    "    res = {}\n",
    "    res['url'] = x.profile_link\n",
    "    res['name'] = x.profile_name\n",
    "    res['dist'] = x.profile_dist\n",
    "    res['description'] = x.profile_description\n",
    "    schoolIndex = 0\n",
    "    for i, e in enumerate(x.profile_school):\n",
    "        res[f'edu{i} school'] = e[\"school\"]\n",
    "        res[f'edu{i} degree'] = e[\"degree\"]\n",
    "        schoolIndex += 1\n",
    "    exp = 0\n",
    "    for i, e in enumerate(x.experiences):\n",
    "        res[f'exp{i} title'] = e.position_title\n",
    "        res[f'exp{i} company'] = e.institution_name.split(\" ·\")[0]\n",
    "        res[f'exp{i} duration'] = e.duration\n",
    "        res[f'exp{i} start'] = e.from_date\n",
    "        exp += 1\n",
    "    return res\n",
    "\n",
    "for candidate in candidates:\n",
    "    row = parseCandidate(candidate)\n",
    "    df = pd.concat([df, pd.DataFrame([row])], ignore_index=True)\n",
    "\n",
    "# update already_scraped.pickle\n",
    "with open('already_scraped.pickle', 'rb') as f:\n",
    "    already_scraped = pickle.load(f)\n",
    "    already_scraped = already_scraped + scraped_urls\n",
    "    already_scraped = list(set(already_scraped))\n",
    "with open('already_scraped.pickle', 'wb') as f:\n",
    "    pickle.dump(already_scraped, f)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "6ee20a9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "categories = {\n",
    "    \"SECURITY\": [\n",
    "        \"Okta\",\n",
    "        \"Snyk\",\n",
    "        \"R2C/Semgrep\",\n",
    "        \"Wiz\",\n",
    "        \"Lacework\",\n",
    "        \"Crowdstrike\",\n",
    "        \"Palo Alto Networks\",\n",
    "        \"Island\",\n",
    "        \"Vanta\",\n",
    "        \"Material Security\",\n",
    "        \"Abnormal Security\"\n",
    "    ],\n",
    "    \"OTHER\": [\n",
    "        \"Figma\",\n",
    "        \"Airtable\",\n",
    "        \"Notion\",\n",
    "        \"Canva\",\n",
    "        \"Webflow\",\n",
    "        \"Faire\",\n",
    "        \"Deel\",\n",
    "        \"Rippling\",\n",
    "        \"Flexport\",\n",
    "        \"Benchling\",\n",
    "        \"Solugen\"\n",
    "    ],\n",
    "    \"PUBLIC\": [\n",
    "        \"Doordash\",\n",
    "        \"Uber\",\n",
    "        \"Palantir\",\n",
    "        \"Airbnb\",\n",
    "        \"Instacart\"\n",
    "    ],\n",
    "    \"INFRA\": [\n",
    "        \"Fivetran\",\n",
    "        \"DBT\",\n",
    "        \"Temporal\",\n",
    "        \"Cockroach Labs\",\n",
    "        \"Grafana\",\n",
    "        \"Zapier\",\n",
    "        \"Starburst\",\n",
    "        \"Retool\",\n",
    "        \"Sentry\",\n",
    "        \"Sourcegraph\",\n",
    "        \"Cribl\",\n",
    "        \"Vercel\",\n",
    "        \"Clickhouse\",\n",
    "        \"Github,\"\n",
    "    ],\n",
    "    \"FINTECH\": [\n",
    "        \"Robinhood\",\n",
    "        \"Square\",\n",
    "        \"Stripe\",\n",
    "        \"Ramp\",\n",
    "        \"Brex\",\n",
    "        \"Plaid\",\n",
    "        \"Modern Treasury\",\n",
    "        \"Mercury\",\n",
    "        \"Persona\",\n",
    "        \"Klarna\",\n",
    "        \"Nubank\"\n",
    "    ],\n",
    "    \"CRYPTO\": [\n",
    "        \"Coinbase\",\n",
    "        \"Uniswap\",\n",
    "        \"Chainalysis\",\n",
    "        \"Arbitrum\",\n",
    "        \"TRM\",\n",
    "        \"Fireblocks\",\n",
    "        \"Eigenlayer\"\n",
    "    ],\n",
    "    \"FRONTIER\": [\n",
    "        \"Anduril\",\n",
    "        \"SpaceX\",\n",
    "        \"Zipline\",\n",
    "        \"Varda\",\n",
    "        \"Hadrian\"\n",
    "    ],\n",
    "    \"AI\": [\n",
    "        \"Bytedance\",\n",
    "        \"Scale AI\",\n",
    "        \"Anthropic\",\n",
    "        \"Robust intelligence\",\n",
    "        \"OpenAI\",\n",
    "        \"Predibase\",\n",
    "        \"Cohere\",\n",
    "        \"Databricks\",\n",
    "        \"Hugging Face\",\n",
    "        \"RunwayML\",\n",
    "        \"Tecton\",\n",
    "        \"Weights & Biases\",\n",
    "        \"Kumo AI\",\n",
    "        \"NVIDIA\",\n",
    "        \"Adept\",\n",
    "        \"Glean\",\n",
    "        \"Character.ai\",\n",
    "        \"Midjourney\",\n",
    "        \"Facebook AI\",\n",
    "        \"FAIR\",\n",
    "        \"Google brain\"\n",
    "    ]\n",
    "}\n",
    "\n",
    "# Example color mapping for categories\n",
    "category_colors = {\n",
    "    \"SECURITY\": 'red',\n",
    "    \"OTHER\": 'blue',\n",
    "    \"PUBLIC\": 'green',\n",
    "    \"INFRA\": 'yellow',\n",
    "    \"FINTECH\": 'orange',\n",
    "    \"CRYPTO\": 'purple',\n",
    "    \"FRONTIER\": 'cyan',\n",
    "    \"AI\": 'magenta'\n",
    "}\n",
    "\n",
    "# Create a reverse dictionary for easier lookup: {company: category}\n",
    "company_category = {}\n",
    "for category, companies in categories.items():\n",
    "    for company in companies:\n",
    "        company_category[company] = category\n",
    "\n",
    "# Modify the style function\n",
    "def highlight_by_category(val):\n",
    "    category = company_category.get(val)\n",
    "    if category:\n",
    "        color = category_colors.get(category, 'none')  # default to 'none' if no color is specified\n",
    "    else:\n",
    "        color = 'none'\n",
    "    return f'background-color: {color}'\n",
    "\n",
    "import re\n",
    "ILLEGAL_CHARACTERS_RE = re.compile(r'[\\000-\\010]|[\\013-\\014]|[\\016-\\037]')\n",
    "def find_illegal_characters(df):\n",
    "    for column in df.columns:\n",
    "        for idx, item in enumerate(df[column]):\n",
    "            if isinstance(item, str) and ILLEGAL_CHARACTERS_RE.search(item):\n",
    "                # replace illegal characters with an empty string\n",
    "                df[column][idx] = ILLEGAL_CHARACTERS_RE.sub('', item)\n",
    "    return df\n",
    "\n",
    "styled_df = df\n",
    "styled_df = find_illegal_characters(styled_df)\n",
    "styled_df = df.style.applymap(highlight_by_category)\n",
    "\n",
    "# Save the styled DataFrame to an Excel file\n",
    "styled_df.to_excel(f'results_{start}-{end}.xlsx', engine='openpyxl', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "id": "b0119511",
   "metadata": {},
   "outputs": [],
   "source": [
    "html = \"\"\"\n",
    "<html>\n",
    "<head>\n",
    "</head>\n",
    "<body style=\"font-family: Arial, sans-serif;\">\n",
    "<h2>Hi Liu,</h2>\n",
    "\n",
    "<p>Here are the latest sourcing updates from Linkedin:</p>\n",
    "\n",
    "<table style=\"border-collapse: collapse; width: 100%; margin-bottom: 25px;\">\n",
    "<tr style=\"background-color: #76bbef;\">\n",
    "  <th style=\"border: 1px solid #ddd; padding: 8px; text-align: left; background-color: #76bbef; color: white;\">Name</th>\n",
    "  <th style=\"border: 1px solid #ddd; padding: 8px; text-align: left; background-color: #76bbef; color: white;\">Role</th>\n",
    "  <th style=\"border: 1px solid #ddd; padding: 8px; text-align: left; background-color: #76bbef; color: white;\">New Company</th>\n",
    "  <th style=\"border: 1px solid #ddd; padding: 8px; text-align: left; background-color: #76bbef; color: white;\">Time in Role</th> \n",
    "  <th style=\"border: 1px solid #ddd; padding: 8px; text-align: left; background-color: #76bbef; color: white;\">Past Company</th> \n",
    "  <th style=\"border: 1px solid #ddd; padding: 8px; text-align: left; background-color: #76bbef; color: white;\">Profile Link</th>\n",
    "</tr>\n",
    "\"\"\"\n",
    "\n",
    "row_counter = 0\n",
    "for candidate in candidates:\n",
    "    bg_color = \"#f2f2f2\" if row_counter % 2 else \"#ffffff\"\n",
    "    html += \"\"\"\n",
    "    <tr style=\"background-color: {bg_color};\">\n",
    "      <td>{}</td>\n",
    "      <td>{}</td>\n",
    "      <td>{}</td>\n",
    "      <td>{}</td>\n",
    "      <td>{}</td>\n",
    "      <td><a href=\"{}\">Linkedin</a></td>\n",
    "    </tr>\n",
    "    \"\"\".format(\n",
    "        candidate.profile_name,\n",
    "        candidate.experiences[0].position_title,\n",
    "        candidate.experiences[0].institution_name.split(\" ·\")[0],\n",
    "        candidate.experiences[0].duration,\n",
    "        candidate.experiences[1].institution_name.split(\" ·\")[0],\n",
    "        candidate.profile_link,\n",
    "        bg_color=bg_color,\n",
    "    )\n",
    "    row_counter += 1\n",
    "\n",
    "html += \"\"\"\n",
    "</table>\n",
    ".......\n",
    "\n",
    "<p>Best,</p>\n",
    "<p>Sourcing Bot</p>\n",
    "</body>\n",
    "</html>\n",
    "\"\"\"\n",
    "\n",
    "# print(html)\n",
    "# save html to file\n",
    "with open(\"sourcing_updates.html\", \"w\") as f:\n",
    "    f.write(html)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
