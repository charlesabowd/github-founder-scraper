{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5b930f8e",
   "metadata": {},
   "source": [
    "#  SETUP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "b9e6fd8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# if the script finishes very quickly (and generates an empty excel file), click run again\n",
    "# if the script errors on the \"Login Cell\" (added a comment to indicate which cell that is below), set IS_HEADLESS to \"False\" and run again. The scraper will automatically launch a page and attempt to login to LinkedIn. It's likely erroring because LinkedIn is asking for a captcha to verify the user is not a bot. Solve the captch/challenge and login. Once successfully logged in, set IS_HEADLESS back to \"True\" and run again.\n",
    "\n",
    "import pickle\n",
    "import csv\n",
    "import os\n",
    "import random\n",
    "import time\n",
    "from linkedin_scraper import actions\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.firefox.options import Options as FirefoxOptions\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.common.exceptions import TimeoutException, StaleElementReferenceException, NoSuchElementException, ElementClickInterceptedException\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# for google docs upload\n",
    "import pickle\n",
    "import gspread\n",
    "from google.oauth2.service_account import Credentials\n",
    "from googleapiclient.discovery import build\n",
    "from datetime import datetime\n",
    "\n",
    "IS_HEADLESS = False\n",
    "\n",
    "# Set environment variables to email and password\n",
    "load_dotenv()\n",
    "LINKEDIN_EMAIL = os.environ.get('LINKEDIN_EMAIL')\n",
    "LINKEDIN_PASSWORD = os.environ.get('LINKEDIN_PASSWORD')\n",
    "# Check if the environment variables are set\n",
    "if not LINKEDIN_EMAIL or not LINKEDIN_PASSWORD:\n",
    "    raise ValueError(\"LinkedIn credentials not set in environment variables\")\n",
    "\n",
    "\n",
    "COMPANY_CATEGORIES = {\n",
    "    \"SECURITY\": [\n",
    "        \"Okta\",\n",
    "        \"Snyk\",\n",
    "        \"R2C/Semgrep\", \n",
    "        \"Wiz\",\n",
    "        \"Lacework\",\n",
    "        \"Crowdstrike\",\n",
    "        \"Palo Alto Networks\",\n",
    "        \"Island\",\n",
    "        \"Vanta\",\n",
    "        \"Material Security\",\n",
    "        \"Abnormal Security\",\n",
    "        \"Samsara\",\n",
    "    ],\n",
    "    \"OTHER\": [\n",
    "        \"Figma\",\n",
    "        \"Airtable\",\n",
    "        \"Notion\",\n",
    "        \"Canva\",\n",
    "        \"Webflow\",\n",
    "        \"Faire\",\n",
    "        \"Deel\",\n",
    "        \"Rippling\",\n",
    "        \"Flexport\",\n",
    "        \"Benchling\",\n",
    "        \"Solugen\"\n",
    "    ],\n",
    "    \"PUBLIC\": [\n",
    "        \"Doordash\",\n",
    "        \"Uber\",\n",
    "        \"Palantir\",\n",
    "        \"Airbnb\",\n",
    "        \"Instacart\"\n",
    "    ],\n",
    "    \"INFRA\": [\n",
    "        \"Fivetran\",\n",
    "        \"DBT\",\n",
    "        \"Temporal\",\n",
    "        \"Cockroach Labs\",\n",
    "        \"Grafana\",\n",
    "        \"Zapier\",\n",
    "        \"Starburst\",\n",
    "        \"Retool\",\n",
    "        \"Sentry\",\n",
    "        \"Sourcegraph\",\n",
    "        \"Cribl\",\n",
    "        \"Vercel\",\n",
    "        \"Clickhouse\",\n",
    "        \"Github,\"\n",
    "        \"Cisco Meraki\",\n",
    "    ],\n",
    "    \"FINTECH\": [\n",
    "        \"Robinhood\",\n",
    "        \"Square\",\n",
    "        \"Stripe\",\n",
    "        \"Ramp\",\n",
    "        \"Brex\",\n",
    "        \"Plaid\",\n",
    "        \"Modern Treasury\",\n",
    "        \"Mercury\",\n",
    "        \"Persona\",\n",
    "        \"Klarna\",\n",
    "        \"Nubank\"\n",
    "    ],\n",
    "    \"CRYPTO\": [\n",
    "        \"Coinbase\",\n",
    "        \"Uniswap\",\n",
    "        \"Chainalysis\",\n",
    "        \"Arbitrum\",\n",
    "        \"TRM\",\n",
    "        \"Fireblocks\",\n",
    "        \"Eigenlayer\"\n",
    "    ],\n",
    "    \"FRONTIER\": [\n",
    "        \"Anduril\",\n",
    "        \"SpaceX\",\n",
    "        \"Zipline\",\n",
    "        \"Varda\",\n",
    "        \"Hadrian\"\n",
    "    ],\n",
    "    \"AI\": [\n",
    "        \"Bytedance\",\n",
    "        \"Scale AI\",\n",
    "        \"Anthropic\",\n",
    "        \"Robust intelligence\",\n",
    "        \"OpenAI\",\n",
    "        \"Predibase\",\n",
    "        \"Cohere\",\n",
    "        \"Databricks\",\n",
    "        \"Hugging Face\",\n",
    "        \"RunwayML\",\n",
    "        \"Tecton\",\n",
    "        \"Weights & Biases\",\n",
    "        \"Kumo AI\",\n",
    "        \"NVIDIA\",\n",
    "        \"Adept\",\n",
    "        \"Glean\",\n",
    "        \"Character.ai\",\n",
    "        \"Midjourney\",\n",
    "        \"Facebook AI\",\n",
    "        \"FAIR\",\n",
    "        \"Google brain\"\n",
    "    ]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "adbde830",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Library/Developer/CommandLineTools/usr/bin/python3\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "print(sys.executable)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7957b23f",
   "metadata": {},
   "source": [
    "# LOGIN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3596a5f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# IF THIS CELL ERRORS DUE TO CAPTCHA, do this:\n",
    "# manually complete the captcha, click on the next cell, and select Run menu, select \"run selected cell and all below\" \n",
    "\n",
    "def instantiate_driver():\n",
    "    options = FirefoxOptions()\n",
    "    if IS_HEADLESS:\n",
    "        options.add_argument(\"--headless\")\n",
    "    driver = webdriver.Firefox(options=options)\n",
    "    \n",
    "    try:\n",
    "        actions.login(driver, LINKEDIN_EMAIL, LINKEDIN_PASSWORD)\n",
    "    except Exception as e:\n",
    "        print(\"Error logging in. Please complete the captcha challenge and login manually.\")\n",
    "        print(e)\n",
    "\n",
    "    time.sleep(15)\n",
    "    return driver\n",
    "    \n",
    "driver = instantiate_driver()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d58f36b7",
   "metadata": {},
   "source": [
    "# General Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "cb0c197a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\"\"\"\n",
    "Wait for an element to be present on the page and return it.\n",
    "\n",
    "Parameters:\n",
    "- driver: The WebDriver instance\n",
    "- by: The method to locate the element (default: By.CLASS_NAME)\n",
    "- name: The name or identifier of the element to wait for\n",
    "- base: The base element to search from (default: None, which uses the driver)\n",
    "- timeout: Maximum time to wait for the element (default: 180 seconds)\n",
    "\n",
    "Returns:\n",
    "- The WebElement if found\n",
    "- None if the element is not found within the timeout period\n",
    "\"\"\"\n",
    "def wait_for_element_to_load(driver, by=By.CLASS_NAME, name=\"pv-top-card\", base=None, timeout=100):\n",
    "    base = base or driver\n",
    "    try:\n",
    "        element = WebDriverWait(base, timeout).until(\n",
    "            EC.presence_of_element_located((by, name))\n",
    "        )\n",
    "        return element\n",
    "    except TimeoutException:\n",
    "        print(f\"Timed out waiting for element: {by}={name}\")\n",
    "        return None\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred while waiting for element {by}={name}: {str(e)}\")\n",
    "        return None\n",
    "        \n",
    "\n",
    "def loadPickle(input):\n",
    "    with open(input, 'rb') as f:\n",
    "        try:\n",
    "            results = pickle.load(f)\n",
    "            print(\"Loaded pickle file: \" + input + \" with \" + str(len(results)) + \" results\")\n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "            print(\"Error loading the pickle file: \" + input)\n",
    "    return results\n",
    "\n",
    "def savePickle(output, data):\n",
    "    with open(output, 'wb') as f:\n",
    "        try:\n",
    "            pickle.dump(data, f)\n",
    "            print(\"Saved pickle file: \" + output + \" with \" + str(len(data)) + \" results\")\n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "            print(\"Error saving the pickle file: \" + output)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "515b071f",
   "metadata": {},
   "source": [
    "# SALES NAV HELPER FUNCTIONS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "027fd1fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_pickles(historyPickle, toScrapePickle):\n",
    "    scraped_urls = []\n",
    "    with open(historyPickle, 'rb') as f:\n",
    "        try:\n",
    "            scraped_urls = pickle.load(f)\n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "            print(\"Error loading history pickle file\")\n",
    "        print(len(scraped_urls))\n",
    "\n",
    "    to_scrape_urls = []\n",
    "    with open(toScrapePickle, 'rb') as f:\n",
    "        try:\n",
    "            to_scrape_urls = pickle.load(f)\n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "            print(\"Error loading to scrape url pickle file\")\n",
    "        print(len(to_scrape_urls))\n",
    "\n",
    "    return scraped_urls, to_scrape_urls\n",
    "    \n",
    "\n",
    "def deduplicate(toScrapePickle, to_scrape_urls, already_scraped_urls):\n",
    "    to_scrape_urls = list(set(to_scrape_urls))\n",
    "    originalCount = len(to_scrape_urls)\n",
    "    to_scrape_urls = [url for url in to_scrape_urls if url not in already_scraped_urls]\n",
    "    print(f\"Removed {originalCount - len(to_scrape_urls)} duplicates\")\n",
    "\n",
    "    with open(toScrapePickle, 'wb') as to_scrape_file:\n",
    "        pickle.dump(to_scrape_urls, to_scrape_file)\n",
    "    \n",
    "    print(len(to_scrape_urls))\n",
    "\n",
    "\n",
    "# Scrolls the page to bring the specified profile element into view\n",
    "# Returns True if successful, False if an error occurs\n",
    "def scroll_to_profile(driver, profile):\n",
    "    try:\n",
    "        driver.execute_script(\"arguments[0].scrollIntoView();\", profile)\n",
    "        return True\n",
    "    except Exception as e:\n",
    "        print(f\"Error scrolling to profile: {e}\")\n",
    "        return False\n",
    "\n",
    "# Clicks on the profile element to open its details\n",
    "# Returns True if successful, False if an error occurs\n",
    "def click_profile(profile):\n",
    "    try:\n",
    "        salesNavOpenProfileButton = profile.find_element(By.CLASS_NAME, \"artdeco-entity-lockup__title\")\n",
    "        salesNavOpenProfileButton.click()\n",
    "        return True\n",
    "    except (NoSuchElementException, ElementClickInterceptedException) as e:\n",
    "        print(f\"Error clicking profile: {e}\")\n",
    "        return False\n",
    "\n",
    "# Clicks the three dots button to open the dropdown menu\n",
    "# Returns True if successful, False if an error occurs\n",
    "def click_three_dots_button(driver):\n",
    "    try:\n",
    "        actionContainer = driver.find_element(By.CLASS_NAME, \"_actions-container_1dg5u8\")\n",
    "        threeDotsButton = actionContainer.find_element(By.CLASS_NAME, \"_icon_ps32ck\")\n",
    "        threeDotsButton.click()\n",
    "        return True\n",
    "    except (NoSuchElementException, ElementClickInterceptedException) as e:\n",
    "        print(f\"Error clicking three dots button\")\n",
    "        return False\n",
    "\n",
    "# Retrieves the LinkedIn URL from the dropdown menu\n",
    "# Returns the URL if successful, None if an error occurs\n",
    "def get_linkedin_url(driver):\n",
    "    try:\n",
    "        dropdownContainer = driver.find_element(By.CLASS_NAME, \"_visible_x5gf48\")\n",
    "        normalLinkedInUrl = dropdownContainer.find_elements(By.TAG_NAME, \"a\")[1].get_attribute(\"href\")\n",
    "        return normalLinkedInUrl\n",
    "    except (NoSuchElementException, IndexError) as e:\n",
    "        print(f\"Error getting LinkedIn URL: {e}\")\n",
    "        return None\n",
    "\n",
    "# Closes the profile popout\n",
    "# Returns True if successful, False if an error occurs\n",
    "def close_popout(driver):\n",
    "    try:\n",
    "        header = driver.find_element(By.CLASS_NAME, \"_inline-sidesheet-header-actions_1cn7lg\")\n",
    "        button = header.find_elements(By.CLASS_NAME, \"_button_ps32ck\")[1]\n",
    "        button.click()\n",
    "        return True\n",
    "    except (NoSuchElementException, ElementClickInterceptedException, IndexError) as e:\n",
    "        print(f\"Error closing popout: {e}\")\n",
    "        return False\n",
    "\n",
    "# Navigates to the next page of search results\n",
    "# Returns True if successful, False if there are no more pages or an error occurs\n",
    "def navigate_to_next_page(driver):\n",
    "    try:\n",
    "        nextPageButton = driver.find_element(By.CLASS_NAME, \"artdeco-pagination__button--next\")\n",
    "        nextPageButton.click()\n",
    "        return True\n",
    "    except (NoSuchElementException, ElementClickInterceptedException) as e:\n",
    "        print(f\"No more pages or error navigating: {e}\")\n",
    "        return False\n",
    "\n",
    "# Main scraping function\n",
    "def scrape_profiles(driver, SALES_NAV_SEARCH_URL, already_scraped_urls, to_scrape_urls, toScrapePickle):\n",
    "    # reinstantiate_driver(driver)\n",
    "    driver.get(SALES_NAV_SEARCH_URL)\n",
    "    \n",
    "    while True:\n",
    "        wait_for_element_to_load(driver, By.ID, \"search-results-container\")\n",
    "        profiles = driver.find_elements(By.CLASS_NAME, \"artdeco-list__item\")\n",
    "        \n",
    "        for profile in profiles:\n",
    "            if not scroll_to_profile(driver, profile):\n",
    "                continue\n",
    "\n",
    "            # time.sleep(2 + random.random() * 6)\n",
    "\n",
    "\n",
    "            if not wait_for_element_to_load(driver, By.CLASS_NAME, \"artdeco-entity-lockup__title\"):\n",
    "                continue\n",
    "\n",
    "            if not click_profile(profile):\n",
    "                continue\n",
    "            \n",
    "            # time.sleep(2 + random.random() * 6)\n",
    "\n",
    "            if not wait_for_element_to_load(driver, By.CLASS_NAME, \"_actions-container_1dg5u8\"):\n",
    "                continue\n",
    "\n",
    "            if not click_three_dots_button(driver):\n",
    "                continue\n",
    "\n",
    "            # time.sleep(2 + random.random() * 6)\n",
    "\n",
    "\n",
    "            if not wait_for_element_to_load(driver, By.CLASS_NAME, \"_visible_x5gf48\"):\n",
    "                continue\n",
    "\n",
    "            normalLinkedInUrl = get_linkedin_url(driver)\n",
    "            if normalLinkedInUrl:\n",
    "                if normalLinkedInUrl in already_scraped_urls:\n",
    "                    print(\"Skipping (already scraped): \" + normalLinkedInUrl)\n",
    "                else:\n",
    "                    to_scrape_urls.append(normalLinkedInUrl)\n",
    "                    with open(toScrapePickle, 'wb') as f:\n",
    "                        pickle.dump(to_scrape_urls, f)\n",
    "                    print(\"Successfully scraped: \" + normalLinkedInUrl)\n",
    "\n",
    "            if not close_popout(driver):\n",
    "                continue\n",
    "\n",
    "            # time.sleep(2 + random.random() * 6)\n",
    "\n",
    "\n",
    "        next_button = wait_for_element_to_load(driver, By.CLASS_NAME, \"artdeco-pagination__button--next\")\n",
    "        if not next_button or not next_button.is_enabled():\n",
    "            break\n",
    "\n",
    "        next_button.click()\n",
    "\n",
    "        if not wait_for_element_to_load(driver, By.CLASS_NAME, \"artdeco-list__item\"):\n",
    "            break\n",
    "\n",
    "    return to_scrape_urls"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d9c83c0",
   "metadata": {},
   "source": [
    "# PROFILE SCRAPING HELPER FUNCTIONS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "ce900eb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter():\n",
    "    pass     \n",
    "    # try:\n",
    "        #     WebDriverWait(driver, 240).until(lambda d: d.execute_script('return document.readyState') == 'complete')\n",
    "        # except:\n",
    "        #     driver = reinstantiate_driver()\n",
    "\n",
    "        # # FILTERING\n",
    "        \n",
    "        # likely_founder = True\n",
    "        # relevant_exp = True\n",
    "\n",
    "        # cur_exp = experiences[0]\n",
    "        # relevant_companies = [\"stealth\", \"new\"]\n",
    "        # if any(company in cur_exp.institution_name.split(\" ·\")[0].lower() for company in relevant_companies) or \"present\" not in cur_exp.to_date.lower():\n",
    "        #     likely_founder = True\n",
    "\n",
    "        # relevant_titles = [\"product\", \"engineer\", \"sales\", \"business development\", \"founder\", \"head\", \"lead\", \"senior\", \"staff\", \"chief\", \"growth\"]\n",
    "        # for experience in experiences[1:5]:\n",
    "        #     if any(title in experience.position_title.lower() for title in relevant_titles):\n",
    "        #         relevant_exp = True\n",
    "        #         break\n",
    "        # relevant_exp = True\n",
    "\n",
    "        # if not (likely_founder and relevant_exp):\n",
    "        #     print(likely_founder, relevant_exp)\n",
    "        #     return None\n",
    "\n",
    "        # person_obj = Person(profile_link, driver = driver, scrape=False, experiences = [None])\n",
    "        # try:\n",
    "        #     WebDriverWait(driver, 240).until(lambda d: d.execute_script('return document.readyState') == 'complete')\n",
    "        # except:\n",
    "        #     driver = reinstantiate_driver(driver)\n",
    "        # time.sleep(2 + random.random() * 7)\n",
    "        \n",
    "# Helper function to scrape experiences\n",
    "# Returns scraped experiences if successful, otherwise returns empty list\n",
    "def get_experiences(driver):\n",
    "    scraped_experiences = []\n",
    "\n",
    "    try:\n",
    "        wait_for_element_to_load(driver, By.CSS_SELECTOR, \"li.pvs-list__paged-list-item.artdeco-list__item.pvs-list__item--line-separated.pvs-list__item--one-column\")\n",
    "        experience_items = driver.find_elements(By.CSS_SELECTOR, \"li.pvs-list__paged-list-item.artdeco-list__item.pvs-list__item--line-separated.pvs-list__item--one-column\")\n",
    "\n",
    "        if len(experience_items) > 0:\n",
    "            for item in experience_items:\n",
    "                hidden_spans = item.find_elements(By.CSS_SELECTOR, \"span.visually-hidden\")\n",
    "                experience_texts = [span.text for span in hidden_spans]\n",
    "                experience = {\n",
    "                    \"title: \": experience_texts[0],\n",
    "                    \"company: \": experience_texts[1],\n",
    "                    \"dates: \": experience_texts[2],\n",
    "                }\n",
    "                \n",
    "                if len(experience_texts) > 3:\n",
    "                    experience[\"location: \"] = experience_texts[3]\n",
    "                if len(experience_texts) > 4:\n",
    "                    experience[\"summary: \"] = experience_texts[4]\n",
    "                if len(experience_texts) > 5:\n",
    "                    experience[\"remaining: \"] = (\", \").join(experience_texts[5:])\n",
    "                scraped_experiences.append(experience)\n",
    "\n",
    "            print(\"Successfully scraped experiences\")\n",
    "        else:\n",
    "            print(\"No experiences found\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        print(\"ERROR: No experiences found\")\n",
    "\n",
    "    return scraped_experiences\n",
    "\n",
    "\n",
    "# Helper function to scrape education\n",
    "# Returns scraped education if successful, otherwise returns empty list\n",
    "def get_education(driver):\n",
    "    scraped_education = []\n",
    "\n",
    "    try:\n",
    "        wait_for_element_to_load(driver, By.CSS_SELECTOR, \"li.pvs-list__paged-list-item.artdeco-list__item.pvs-list__item--line-separated.pvs-list__item--one-column\")\n",
    "        education_items = driver.find_elements(By.CSS_SELECTOR, \"li.pvs-list__paged-list-item.artdeco-list__item.pvs-list__item--line-separated.pvs-list__item--one-column\")\n",
    "\n",
    "        if len(education_items) > 0:\n",
    "            for item in education_items:\n",
    "                hidden_spans = item.find_elements(By.CSS_SELECTOR, \"span.visually-hidden\")\n",
    "                education_texts = [span.text for span in hidden_spans]\n",
    "                education = {\n",
    "                    \"school: \": education_texts[0],\n",
    "                }\n",
    "                \n",
    "                if len(education_texts) > 1:\n",
    "                    education[\"degree: \"] = education_texts[1]\n",
    "                if len(education_texts) > 2:\n",
    "                    education[\"dates: \"] = education_texts[2]\n",
    "                if len(education_texts) > 3:\n",
    "                    education[\"remaining: \"] = (\", \").join(education_texts[5:])\n",
    "                scraped_education.append(education)\n",
    "\n",
    "            print(\"Successfully scraped education\")\n",
    "        else:\n",
    "            print(\"No education found\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        print(\"ERROR: No education found\")\n",
    "\n",
    "    return scraped_education\n",
    "\n",
    "\n",
    "# Helper function to scrape degree of connection and mutuals\n",
    "# Returns scraped degree of connection and mutuals if successful, otherwise returns N/A\n",
    "def get_degree_of_connection_and_mutuals(driver):\n",
    "    scraped_profile_dist = \"4+\"\n",
    "    scraped_mutuals = \"N/A\"\n",
    "\n",
    "    try:\n",
    "        scraped_profile_dist = wait_for_element_to_load(driver, By.CSS_SELECTOR, \"span.dist-value\").text\n",
    "        if scraped_profile_dist == \"1st\" or scraped_profile_dist == \"2nd\":\n",
    "            try:\n",
    "                span_element = wait_for_element_to_load(driver, By.CSS_SELECTOR, \"span.t-normal.t-black--light.t-14.hoverable-link-text\")\n",
    "                scraped_mutuals = span_element.text.split('\\n')[0]\n",
    "                print(\"Successfully found mutual connections: \" + scraped_mutuals)\n",
    "            except:\n",
    "                print(\"ERROR: mutuals not found\")\n",
    "        print(\"Successfully scraped degree of connection: \" + scraped_profile_dist)\n",
    "\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        print(\"No degree of connection found\")\n",
    "\n",
    "    return scraped_profile_dist, scraped_mutuals\n",
    "\n",
    "# Helper function to scrape description\n",
    "# Returns scraped description if successful, otherwise returns N/A\n",
    "def get_description(driver):\n",
    "    scraped_description = \"N/A\"\n",
    "    \n",
    "    try:\n",
    "        scraped_description = driver.find_element(By.CLASS_NAME, \"text-body-medium.break-words\").text\n",
    "        print(\"Successfully scraped description: \" + scraped_description)\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        print(\"ERROR: description not found\")\n",
    "        \n",
    "    return scraped_description\n",
    "\n",
    "# Main function to scrape degree of connection and mutuals\n",
    "# Returns scraped degree of connection and mutuals if successful, otherwise returns N/A\n",
    "def scrape_profile(driver, scraped_link):\n",
    "\n",
    "    # Scrape Name\n",
    "    driver.get(scraped_link)\n",
    "    scraped_profile_name = wait_for_element_to_load(driver, By.CSS_SELECTOR, \"h1.text-heading-xlarge.inline.t-24.v-align-middle.break-words\").text\n",
    "    time.sleep(2 + random.random() * 6)\n",
    "\n",
    "    # Scrape Experiences\n",
    "    experiences_url = os.path.join(scraped_link, \"details/experience\")\n",
    "    driver.get(experiences_url)\n",
    "    scraped_experiences = get_experiences(driver)\n",
    "    time.sleep(2 + random.random() * 6)\n",
    "\n",
    "\n",
    "    # Scrape Education\n",
    "    education_url = os.path.join(scraped_link, \"details/education\")\n",
    "    driver.get(education_url)\n",
    "    scraped_education = get_education(driver)\n",
    "    time.sleep(2 + random.random() * 6)\n",
    "\n",
    "\n",
    "    # Scrape degree of connection and mutuals if available\n",
    "    driver.get(scraped_link)\n",
    "    scraped_profile_dist, scraped_mutuals = get_degree_of_connection_and_mutuals(driver)\n",
    "    time.sleep(2 + random.random() * 6)\n",
    "\n",
    "    # Scrape description\n",
    "    scraped_description = get_description(driver)\n",
    "    time.sleep(2 + random.random() * 6)\n",
    "\n",
    "\n",
    "    # Scape profile link\n",
    "    scraped_link = driver.current_url\n",
    "    print(\"Successfully scraped profile link: \" + scraped_link)\n",
    "    time.sleep(2 + random.random() * 6)\n",
    "\n",
    "    profile = {}\n",
    "    profile[\"name\"] = scraped_profile_name\n",
    "    profile[\"experiences\"] = scraped_experiences\n",
    "    profile[\"education\"] = scraped_education\n",
    "    profile[\"profile_dist\"] = scraped_profile_dist\n",
    "    profile[\"mutuals\"] = scraped_mutuals\n",
    "    profile[\"profile_description\"] = scraped_description\n",
    "    profile[\"profile_link\"] = scraped_link\n",
    "\n",
    "    print(\"\\nSuccess!\")\n",
    "    print(f\"Name: {profile['name']}\")\n",
    "    print(\"Experiences:\")\n",
    "    for exp in profile[\"experiences\"]:\n",
    "        print(f\"  Title: {exp.get('title: ', '')}\")\n",
    "        print(f\"  Company: {exp.get('company: ', '')}\")\n",
    "        print(f\"  Dates: {exp.get('dates: ', '')}\")\n",
    "        print(f\"  Location: {exp.get('location: ', '')}\")\n",
    "        print(f\"  Summary: {exp.get('summary: ', '')}\")\n",
    "        print(f\"  Remaining: {exp.get('remaining: ', '')}\")\n",
    "    print(\"Education:\")\n",
    "    for edu in profile[\"education\"]:\n",
    "        print(f\"  School: {edu.get('school: ', '')}\")\n",
    "        print(f\"  Degree: {edu.get('degree: ', '')}\")\n",
    "        print(f\"  Dates: {edu.get('dates: ', '')}\")\n",
    "        print(f\"  Remaining: {edu.get('remaining: ', '')}\")\n",
    "    print(f\"Profile Distance: {profile['profile_dist']}\")\n",
    "    print(f\"Mutuals: {profile['mutuals']}\")\n",
    "    print(f\"Description: {profile['profile_description']}\")\n",
    "    print(f\"Link: {profile['profile_link']}\\n\")\n",
    "\n",
    "    return profile\n",
    "\n",
    "\n",
    "def scrape_all_profiles(driver, historyPickle, toScrapePickle, resultsPickle, failedURLsTextFile):\n",
    "    results = []\n",
    "    scraped_urls = []\n",
    "    history = []\n",
    "    to_scrape_urls = []\n",
    "\n",
    "    with open(toScrapePickle, 'rb') as f:\n",
    "        try:\n",
    "            to_scrape_urls = pickle.load(f)\n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "            print(\"Error loading to scrape url pickle file\")\n",
    "        \n",
    "    totalCount = len(to_scrape_urls)\n",
    "\n",
    "    print(f'# of profiles to scrape: {totalCount}')\n",
    "    \n",
    "\n",
    "    for i in range(len(to_scrape_urls) - 1, -1, -1):\n",
    "        url = to_scrape_urls[i]\n",
    "        print(f'At index: {totalCount - i} - url: {url}')\n",
    "\n",
    "        with open(historyPickle, 'rb') as f:\n",
    "            try:\n",
    "                history = pickle.load(f)\n",
    "            except Exception as e:\n",
    "                print(e)\n",
    "                print(\"Error loading history pickle file\")\n",
    "\n",
    "        try:\n",
    "            profile = scrape_profile(driver, url)\n",
    "            if profile != None:\n",
    "                print(\"saving profile info\")\n",
    "                results.append(profile)\n",
    "\n",
    "                with open(resultsPickle, 'wb') as f:\n",
    "                    pickle.dump(results, f)\n",
    "                \n",
    "                print(\"adding to history\")\n",
    "                history.append(url)\n",
    "                with open(historyPickle, 'wb') as f:\n",
    "                    pickle.dump(history, f)\n",
    "                \n",
    "                print(\"recording scraped url\")\n",
    "                scraped_urls.append(url)\n",
    "            else:\n",
    "                print(\"profile filtered out\")\n",
    "            \n",
    "            print(\"removing from to-scrape\")\n",
    "            to_scrape_urls.remove(url)\n",
    "            with open(toScrapePickle, 'wb') as f:\n",
    "                pickle.dump(to_scrape_urls, f)\n",
    "\n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "            print('Failed to scrape profile: ', url)\n",
    "\n",
    "            # TODO: CHANGED FAILED URLS\n",
    "            with open(failedURLsTextFile, 'a') as f:\n",
    "                f.write(url + '\\n')\n",
    "\n",
    "        print(((totalCount - i)/totalCount) * 100, '% Done - at index:', totalCount - i)\n",
    "        print('\\n----------------------------------------------------------------------------------------------------------\\n')\n",
    "    return results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdd18909",
   "metadata": {},
   "source": [
    "# EXPORTING HELPER FUNCTIONS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "56e0af4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def authenticate():\n",
    "    scope = ['https://spreadsheets.google.com/feeds', 'https://www.googleapis.com/auth/drive']\n",
    "    creds = Credentials.from_service_account_file('google_auth.json', scopes=scope)\n",
    "    client = gspread.authorize(creds)\n",
    "    drive_service = build('drive', 'v3', credentials=creds)\n",
    "    return client, drive_service\n",
    "\n",
    "def create_and_export_profiles_to_sheets(profiles, folder_id):\n",
    "    client, drive_service = authenticate()\n",
    "\n",
    "    # Create sheet\n",
    "    today_date = datetime.today().strftime('%Y-%m-%d')\n",
    "    sheet_name = f'Linkedin Scraper Results {today_date}'\n",
    "    spreadsheet = client.create(sheet_name)\n",
    "    sheet = spreadsheet.sheet1\n",
    "    \n",
    "    # Move the spreadsheet to the specified folder\n",
    "    file_id = spreadsheet.id\n",
    "    drive_service.files().update(fileId=file_id, addParents=folder_id, removeParents='root').execute()\n",
    "    \n",
    "    # Prepare the header for the combined sheet\n",
    "    max_edu = max(len(profile['education']) for profile in profiles)\n",
    "    max_exp = max(len(profile['experiences']) for profile in profiles)\n",
    "    header = [\"Link\", \"Name\", \"Description\", \"Distance\", \"Mutuals\"]\n",
    "    for i in range(max_edu):\n",
    "        header.extend([f\"Edu{i} School\", f\"Edu{i} Degree\", f\"Edu{i} Dates\", f\"Edu{i} Remaining\"])\n",
    "    for i in range(max_exp):\n",
    "        header.extend([f\"Exp{i} Title\", f\"Exp{i} Company\", f\"Exp{i} Dates\", f\"Exp{i} Location\", f\"Exp{i} Summary\", f\"Exp{i} Remaining\"])\n",
    "    sheet.append_row(header)\n",
    "\n",
    "    # Prepare the rows\n",
    "    for profile in profiles:\n",
    "        row = [profile['profile_link'], profile['name'], profile['profile_description'], profile['profile_dist'], profile['mutuals']]\n",
    "        \n",
    "        # Add education data\n",
    "        for edu in profile['education']:\n",
    "            row.extend([edu.get('school: ', ''), edu.get('degree: ', ''), edu.get('dates: ', ''), edu.get('remaining: ', '')])\n",
    "        # Pad with empty strings if there are fewer education entries than the maximum\n",
    "        row.extend([''] * ((max_edu - len(profile['education'])) * 4))\n",
    "        \n",
    "        # Add experiences data\n",
    "        for exp in profile['experiences']:\n",
    "            row.extend([exp.get('title: ', ''), exp.get('company: ', ''), exp.get('dates: ', ''), exp.get('location: ', ''), exp.get('summary: ', ''), exp.get('remaining: ', '')])\n",
    "        # Pad with empty strings if there are fewer experience entries than the maximum\n",
    "        row.extend([''] * ((max_exp - len(profile['experiences'])) * 6))\n",
    "        sheet.append_row(row)\n",
    "\n",
    "    return spreadsheet.url"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "095b1a61-bfdb-4ec3-b5f5-8bc81d74a68d",
   "metadata": {},
   "source": [
    "# QUERY 1: MAIN COMPANIES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "aca7a719",
   "metadata": {},
   "outputs": [],
   "source": [
    "historyPickle = 'db/main_history.pickle'\n",
    "toScrapePickle = 'db/main_to_scrape.pickle'\n",
    "resultsPickle = 'db/main_results.pickle'\n",
    "failedURLsTextFile = 'db/main_failed_urls.txt'\n",
    "SALES_NAV_SEARCH_URL = \"\"\"\n",
    "https://www.linkedin.com/sales/search/people?savedSearchId=1833971580&sessionId=jFoOWCqiTl2JVJ3QPZrL4g%3D%3D\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93127f7e",
   "metadata": {},
   "source": [
    "### Sales nav scraper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e0ea7d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "already_scraped_urls, to_scrape_urls = check_pickles(historyPickle, toScrapePickle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e83345e3-d166-4e30-a7d6-2634b0db34a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    updated_to_scrape_urls = scrape_profiles(driver, SALES_NAV_SEARCH_URL, already_scraped_urls, to_scrape_urls, toScrapePickle)\n",
    "    print(updated_to_scrape_urls)\n",
    "except Exception as e:\n",
    "    print(f\"An unexpected error occurred: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b1c8979-0304-4621-ae96-59108d44bada",
   "metadata": {},
   "source": [
    "### Deduplicate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ea0fe49",
   "metadata": {},
   "outputs": [],
   "source": [
    "deduplicate(toScrapePickle, to_scrape_urls, already_scraped_urls)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6da7736-3078-4e03-8be6-967f48640cdc",
   "metadata": {},
   "source": [
    "### Profile scraper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7187f14e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "results = scrape_all_profiles(driver, historyPickle, toScrapePickle, resultsPickle, failedURLsTextFile)\n",
    "print(len(results))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "801b0bdc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'name': 'Gina Fromme', 'experiences': [{'title: ': 'Legacy Steakhouse', 'company: ': '1 yr 1 mo', 'dates: ': 'Front of House Manager', 'location: ': 'Jan 2024 to Present · 8 mos', 'summary: ': 'Leadership · Bartending · Hospitality · Customer Satisfaction · Public Safety · Sales · Independent Thinking · Quality Assurance · Workplace Safety · Team Management · Marketing Strategy · Management', 'remaining: ': 'Assistant Manager, Aug 2023 to Dec 2023 · 5 mos, Leadership · Bartending · Hospitality · Customer Satisfaction · Public Safety · Sales · Independent Thinking · Workplace Safety · Team Management · Marketing Strategy · Management'}, {'title: ': 'Server', 'company: ': 'Back 9 Grill · Seasonal', 'dates: ': 'May 2022 to Jul 2023 · 1 yr 3 mos', 'location: ': 'Santa Claus, Indiana, United States · On-site', 'summary: ': 'Customer Service · Bartending · Hospitality · Customer Satisfaction · Sales'}, {'title: ': 'Delivery Driver', 'company: ': 'DoorDash · Self-employed', 'dates: ': 'Feb 2021 to May 2023 · 2 yrs 4 mos', 'location: ': 'Muncie, Indiana, United States', 'summary: ': 'Customer Service · Hospitality · Self-management'}, {'title: ': 'Cafe Server', 'company: ': 'Brew · Seasonal', 'dates: ': 'May 2021 to Jan 2022 · 9 mos', 'location: ': 'Jasper, Indiana, United States', 'summary: ': 'Customer Service · Hospitality · Customer Satisfaction · Sales · Marketing Strategy'}, {'title: ': 'Teaching Assistant', 'company: ': 'Ball State University · Part-time', 'dates: ': 'Aug 2020 to Dec 2020 · 5 mos', 'location: ': 'Muncie, Indiana, United States', 'summary: ': 'People Development · Workplace Safety · Teaching · Lesson Planning'}, {'title: ': 'Ride Operator', 'company: ': \"Holiday World & Splashin' Safari · Seasonal\", 'dates: ': 'May 2020 to Aug 2020 · 4 mos', 'location: ': 'Santa Claus, Indiana, United States', 'summary: ': 'Customer Service · Hospitality · Public Safety · Working With Children'}, {'title: ': 'Machine Operator', 'company: ': 'Universal Technologies LLC · Part-time', 'dates: ': 'May 2015 to May 2019 · 4 yrs 1 mo', 'location: ': 'Jasper, Indiana, United States', 'summary: ': 'Computer-Aided Design (CAD) · Independent Thinking · Quality Assurance'}], 'education': [{'school: ': 'Ball State University - Miller College of Business', 'degree: ': 'Bachelor of Business Administration - BBA, Business Administration and Management, General', 'dates: ': 'Aug 2019 - May 2023', 'remaining: ': ''}], 'profile_dist': '3rd', 'mutuals': 'N/A', 'profile_description': 'Manager at Legacy Steakhouse', 'profile_link': 'https://www.linkedin.com/in/ginafromme/'}, {'name': 'Shivangi Sharma', 'experiences': [{'title: ': 'Founder', 'company: ': 'Stealth Startup · Self-employed', 'dates: ': 'May 2024 to Present · 4 mos'}, {'title: ': 'Machine Learning Engineer', 'company: ': 'Zelus Analytics · Full-time', 'dates: ': 'Aug 2023 to Apr 2024 · 9 mos', 'location: ': 'Remote', 'summary: ': 'Machine Learning · docker · GitHub · Google Cloud Platform (GCP) · Git · Kubernetes · Software Development'}, {'title: ': 'Machine Learning Engineer - Co Op', 'company: ': 'Sanofi · Full-time', 'dates: ': 'Jan 2023 to Jul 2023 · 7 mos', 'location: ': 'Boston, Massachusetts, United States', 'summary: ': '- Created a Generative Transformer model to generate novel protein sequences from a given molecular structure - Reimagined AAV evolution based on environmental and genetic factors using sequence models - Created an ensemble model using Convolutional and Transformer layers to predict scaffolds that could assemble into non-viral capsids', 'remaining: ': 'Machine Learning · Natural Language Processing (NLP) · Amazon Web Services (AWS) · Deep Learning · Generative AI · PyTorch'}, {'title: ': 'Northwestern University', 'company: ': '1 yr 4 mos', 'dates: ': 'Teaching Assistant', 'location: ': 'Sep 2022 to Dec 2022 · 4 mos', 'summary: ': 'TA for the subject - Introduction to AI', 'remaining: ': 'Student, Full-time, Sep 2021 to Dec 2022 · 1 yr 4 mos, Evanston, Illinois, United States, Artificial Intelligence (AI) · Data Science · Generative AI'}, {'title: ': 'Machine Learning Intern - Capstone', 'company: ': 'Google · Full-time', 'dates: ': 'Sep 2022 to Dec 2022 · 4 mos', 'location: ': 'Chicago, Illinois, United States', 'summary: ': '- Led a team of 5 researchers at Northwestern University to develop a prototype revenue forecasting and inventory optimization Machine Learning pipeline for a global beverage chain using Google Cloud Tools such as AutoML, BQML and Vertex AI - Deployed the model on-cloud using Docker and Kubernetes which led to a 47% increase in ROI for the beverage company', 'remaining: ': 'Machine Learning · Python (Programming Language) · Google Cloud AutoML · Google BigQuery · ARIMA'}, {'title: ': 'Software Development Engineer - Computer Vision', 'company: ': 'Amazon · Internship', 'dates: ': 'Jun 2022 to Sep 2022 · 4 mos', 'location: ': 'New York, United States', 'summary: ': '- Designed a Computer Vision recommendation system for product videos using a PyTorch implementation of ResNet-50 for product categorization and HigherHRNet for human body part presence detection - Created a Sagemaker pipeline for end-to-end deployment of the model, used Sagemaker Model monitor to detect data and model drift, and Cloudwatch to track model performance on a monthly basis', 'remaining: ': 'Computer Vision · Amazon Web Services (AWS) · AWS SageMaker · Git · Amazon CloudWatch · PyTorch · Software Development'}, {'title: ': 'Machine Learning Engineer', 'company: ': 'AbbVie · Internship', 'dates: ': 'Apr 2022 to Jun 2022 · 3 mos', 'location: ': 'Chicago, Illinois, United States · Remote', 'summary: ': '- Led a team of 4 to predict the causes behind adverse drug reactions in patients using features such as drug structures, intake dosage and patient demographic information using a multi-class classification model - Experimented with a number of architectures for the model including Feed forward neural networks, Random Forests and XGBoost'}, {'title: ': 'Data Scientist', 'company: ': 'McAfee · Full-time', 'dates: ': 'Oct 2019 to Aug 2021 · 1 yr 11 mos', 'location: ': 'Bangalore', 'summary: ': '- Received the ‘McAfee bravo reward’ for creating a Multi-label classification NLP model using LSTMs to automate the process of categorizing customer comments into various categories, with an accuracy of ~89% - Executed the entire project development and deployment on AWS platform leveraging Docker - Performed end-to-end cleaning, modeling, dashboarding and reporting of data pertaining to surveys under McAfee’s ‘Digital Experience’ program (Tools and Technologies used – SQL, Python, Tableau, Power BI) - Collaborated with the Consumer and Enterprise teams to set Quarterly and Yearly goals for NPS, CSAT, PSAT for surveys using Machine Learning algorithms in Python', 'remaining: ': 'Data Analysis'}, {'title: ': 'Intern', 'company: ': 'Asteco Property Management', 'dates: ': 'Jan 2019 to Jul 2019 · 7 mos', 'location: ': 'Dubai, United Arab Emirates', 'summary: ': '- Analyzed the company’s sales using Time Series analysis in Python to give an estimated projection for the upcoming Quarterly sales - Trained a Convolutional Neural Network to classify real estate images on the company’s website into their respective categories - Created Tableau dashboards to analyze the sales of various types of properties in the UAE', 'remaining: ': 'Data Analysis'}, {'title: ': 'Intern', 'company: ': 'ShopClues', 'dates: ': 'Jun 2018 to Aug 2018 · 3 mos', 'location: ': 'Gurgaon, India', 'summary: ': 'Data Analysis'}, {'title: ': 'Intern', 'company: ': 'EY', 'dates: ': 'Jun 2017 to Aug 2017 · 3 mos', 'location: ': 'Bengaluru Area, India', 'summary: ': '- Worked with the Risk Advisory team to analyze the performance metrics for ML algorithms in place for various clients ( Tools and Technologies used –Python : Reinforcement learning, Natural language processing) - Analyzed the risks involved with the deployment of each of these models, and suggested ways in which they could be mitigated.', 'remaining: ': 'Data Analysis'}, {'title: ': 'Intern', 'company: ': 'ICICI Bank', 'dates: ': 'Jan 2017 to Mar 2017 · 3 mos', 'location: ': 'Mumbai Area, India'}], 'education': [{'school: ': 'Northwestern University', 'degree: ': 'Master of Science - MS, Artificial Intelligence', 'dates: ': 'Sep 2021 - Dec 2022', 'remaining: ': ''}, {'school: ': 'Birla Institute of Technology and Science, Pilani', 'degree: ': 'Bachelor of Engineering - BE, Computer Science', 'dates: ': '2015 - 2019', 'remaining: ': ''}], 'profile_dist': '2nd', 'mutuals': 'Tanay Jaipuria, Rak Garg, and 3 other mutual connections', 'profile_description': 'Building the future of AI powered healthcare', 'profile_link': 'https://www.linkedin.com/in/shivangi-sharma01/'}, {'name': 'Eyal Susser', 'experiences': [{'title: ': 'Head of Software Engineering', 'company: ': 'Lemurian Labs · Permanent Full-time', 'dates: ': 'Nov 2023 to Present · 10 mos', 'location: ': 'Toronto, Ontario, Canada · Remote', 'summary: ': 'Leading Development Teams · Software Management · C++ · Agile Methodologies · Python'}, {'title: ': 'Co-Founder and CTO', 'company: ': 'Stealth Startup · Permanent Full-time', 'dates: ': 'Nov 2022 to Oct 2023 · 1 yr', 'location: ': 'Toronto, Ontario, Canada', 'summary: ': '- Outlined engineering roadmap across mechanical engineering, electrical engineering, and software engineering. - Worked closely with mechanical and electrical engineers on prototype development. - Jack of all trades coding: wrote embedded software, mobile apps, API servers and ROS2 based nodes. - Set up cloud based infrastructure on AWS (databases, container management, secret management, etc). - Selected contract manufacturers and served as engineering point of contact.', 'remaining: ': 'Management · C++ · Engineering Management · Python'}, {'title: ': 'Head of Toronto Software', 'company: ': 'Nuro · Permanent Full-time', 'dates: ': 'Feb 2022 to Nov 2022 · 10 mos', 'location: ': 'Toronto, Ontario, Canada', 'summary: ': 'Nuro builds autonomous vehicles for last mile delivery, with a focus on food and grocery delivery. Nuro effectively shut down its Toronto presence in November ‘22. - Built and led a Toronto based team focused on increasing vehicle autonomy and reducing the need for teleoperation and human intervention through work across the perception and planning layers of the autonomy stack. - Worked across teams in across office locations to ensure cross team collaboration and effective coordination between perception and planning. - Collaborated with the robot operations team on on-road diagnostic improvements. - Collaborated with academic advisors on a perception roadmap. - Mentoring and guidance for all Toronto employees, across all teams (dotted line reports).', 'remaining: ': 'Engineering Management'}, {'title: ': 'Head of Software Engineering', 'company: ': 'Gatik · Full-time', 'dates: ': 'Feb 2021 to Dec 2021 · 11 mos', 'location: ': 'Toronto, Ontario, Canada', 'summary: ': 'Gatik provides autonomous logistics services for middle mile delivery. As Head of Software Engineering, my role was focussed on enhancing autonomous vehicle capabilities with a goal of creating a scalable, safe fleet solution. - Worked with all autonomy related teams on developing capabilities around mapping, localization, perception, controls and the middleware tying them all together. - Formed an infrastructure roadmap focussed on fleet scalability, then recruited a seasoned head of infrastructure to act as a peer, lead infrastructure efforts, grow the infrastructure team and refine the roadmap. - Mentor engineers and researchers across multiple office locations and multiple disciplines - Collaborate with third party providers on mapping, annotation and simulation - Formed a hiring process including interview panel kickoffs, interviewing guidelines and debriefs, in the absence of an HR and recruiting group. - Worked to expand the team, hiring for all the autonomy related teams, across research and software engineering.', 'remaining: ': 'Leading Development Teams · Software Management · Engineering Management'}, {'title: ': 'Sr Engineering Manager | Uber ATG', 'company: ': 'Uber · Full-time', 'dates: ': 'Jun 2020 to Feb 2021 · 9 mos', 'location: ': 'Toronto, Ontario, Canada', 'summary: ': 'Uber’s Advanced Technologies Group (ATG) focuses on delivering deep learning driven technologies for autonomous vehicles. Uber disbanded its autonomous vehicle unit in 2021. As an Engineering Manager, expanded a team working on a highly modular autonomous driving simulation harness, enabling quick iteration on novel models and algorithms; leading a team charged with building a system for on policy closed loop training; coordinated a cross team effort to introduce a novel camera simulator; together with Engineering Manager - Infrastructure Team, worked on being able to scale training jobs to multiple GPUs across multiple machines reliably. - Collaborated closely with research. - Built joint roadmaps with other groups (Infrastructure, Data Storage) to ensure that the research group had ample support for tools they depended on. - Helped Engineering teams find their cadence by introducing basic, lightweight Agile methodologies. - Helped Engineering teams build better tools for researchers by drawing requirements from researchers, and creating opportunities for engineers to gather requirements from researchers.', 'remaining: ': 'Engineering Management'}, {'title: ': 'Director, Machine Learning', 'company: ': 'Kindred AI', 'dates: ': 'Dec 2017 to May 2020 · 2 yrs 6 mos', 'location: ': 'Toronto, Ontario, Canada', 'summary: ': 'Kindred’s mission is to build machines with human like intelligence, to solve real world problems. Kindred’s first product, Sort, is able to learn how to pick objects in a noisy, real world environment, as part of a robotics-as-a-service fulfillment centre solution. As Director, SW/ML, grew a team of 12 to 25, while introducing Agile development processes, as well as building out management tiers; together with the Director, Robotics, oversaw the growth of the robot fleet by several orders of magnitude; consistently reduced the need for manual intervention in robot operation; introduced automated test harnesses for robotics; drove for measurable, predictable delivery; introduced processes for smooth productionization of ML research work.', 'remaining: ': 'Engineering Management'}, {'title: ': 'Director of Software Development', 'company: ': 'Ethoca', 'dates: ': 'Jul 2016 to Dec 2017 · 1 yr 6 mos', 'location: ': 'Toronto, Ontario, Canada', 'summary: ': 'Ethoca is a fin tech leader bringing change to the chargeback and fraud space. At Ethoca, focused on building out an innovation team and rolling out new products; strengthening Agile practices; led the largest code change in the company’s history as part of an effort to refactor core legacy systems; worked with other departments to improve deployment procedures and to streamline dev work in general through cross-functionality.', 'remaining: ': 'Engineering Management'}, {'title: ': 'VP, Development & Integration', 'company: ': 'WhoKnows', 'dates: ': 'Jun 2014 to Jun 2016 · 2 yrs 1 mo', 'location: ': 'Toronto, Canada Area', 'summary: ': 'Focussed on team growth, product/software synergy, and improved team productivity, my role straddles product, sales, business development, data science and SW development. A strong software company thrives when the dev teams mesh and interact with all the other teams in the company, and this has been my goal ay WhoKnows. By investing in strategic strengthening of the dev team, along with introduction of process to better facilitate communication between teams, I have helped the evolution of the product become a true, company wide, team effort.', 'remaining: ': 'AngularJS · Engineering Management, WhoKnows: The First Smart Enterprise Network (Short), '}, {'title: ': 'Ad Tech Integration Lead', 'company: ': 'Fuse Powered Inc.', 'dates: ': 'Nov 2013 to Mar 2014 · 5 mos', 'location: ': 'Toronto, Canada Area', 'summary: ': 'Following the acquisition of AppHero by Fuse Powered, lead integration of technologies and intellectual property developed at AppHero with the existing Fuse Powered code base. Assisted with adoption of agile and consulted on development best practices. Worked closely with the director of development, consulting on development strategy, from hiring plans to technology choices.', 'remaining: ': 'Engineering Management'}, {'title: ': 'CTO', 'company: ': 'AppHero', 'dates: ': 'Sep 2012 to Nov 2013 · 1 yr 3 mos', 'location: ': 'Toronto, Canada Area', 'summary: ': \"AppHero provides user's with timely, relevant app recommendations based on the user's interests and previous purchases. Based on cutting edge Machine Learning techniques, AppHero recommends the right apps to the right people in a matter of seconds. AppHero was acquired by Fuse Powered in Q4 2013.\", 'remaining: ': 'Engineering Management'}, {'title: ': 'Director of Software Development', 'company: ': 'Virgin Gaming', 'dates: ': 'Jan 2010 to Oct 2012 · 2 yrs 10 mos', 'location: ': 'Toronto, Canada Area', 'summary: ': \"Virgin Gaming is the leading provider for online skilled video gaming, offering users the opportunity to win cash, points and prizes based on their skill as video gamers. Reporting directly to the CTO, this role includes planning resources and allocation of work to several teams; strategic planning of software development goals and timelines together with the CTO; responsibility for architecture and technological decisions; and cross departmental, cross discipline collaboration with other directors to keep things on track. Received reports from multiple software team leads and coordinated the team efforts - Planned and executed development roadmap based on priorities received from CTO and the executive team - Guided design and architectural decisions - Formulated long term development strategy - Developed and enforced development process and introduced tools to increase visibility into the development teams' performance - Made decisions on budget allocations under heavy constraints - Mentored leads and developers - Assisted in delivery of features when development staff was overburdened\", 'remaining: ': 'Engineering Management'}, {'title: ': 'Team Lead - Software Development', 'company: ': 'Embarcadero', 'dates: ': 'Aug 2006 to Sep 2009 · 3 yrs 2 mos', 'location: ': \"Led a team charged with writing a digital market place server. Designed key elements of the server, including data access layers, web service publishing layers, and administration web UI. Implemented elements of the server using Java, JSP, Hibernate, Spring MVC and Javascript. Introduced improvements to team workflow, resulting in consistently timely delivery of software, despite a challenging timetable. Team Lead/Software Architect (C++) Led redesign of 24x7 database monitoring solution. The product was overhauled without halting the rate of version and feature releases. Redesigned key modules, such as the multi threaded task processing module, and the central task allocation module, ensuring both robustness of code and timely feature delivery. Began process of porting server modules to Java. This required redesign of several modules (such as the reporting module) in order to ensure module abstraction and decoupling of code. Led documentation effort of key architectural concepts. Mentored developers, senior, intermediate and junior. Managed a team of developers overseas, and guided the team leads of other teams. Team Lead Led a team of developers charged with the development of the company's flagship product. Designed specific components in the product, such a SQL editor that interfaced with JNI components to retrieve data and display it.. Mentored intermediate and junior developers. Managed two teams of developers overseas, one charged with development of the company's flagship product, and one charged with the maintenance of a legacy 24x7 data base monitoring tool.\", 'summary: ': 'Engineering Management'}, {'title: ': 'Senior Software Developer', 'company: ': 'Radcom', 'dates: ': 'May 2003 to Jul 2006 · 3 yrs 3 mos', 'location: ': 'Designed and implemented a telecommunicatiosn monitoring probe in an embedded Linux environment. Designed and developed a high load, high speed object oriented inter-process communication (IPC) framework, for processes on the same machine or on different machines. Designed and developed an object registration system to compliemnt the IPC framework mentioned above. Designed and developed serializer module to compliement the IPC framework. Designed and developed a memory allocation and garbage collection system. Designed and developed a SIP transaction analysis module, which included parsing, analysis and tracking of SIP packages. Designed an MGCP transaction Analysis module. Worked directly with the driver development team and the hardware developemnt team. Mentored a junior developer, set deadlines and assisted in design of a web interface for server debugging purposes. Maintained and optimized a SIP transaction generator used for load simulation.'}, {'title: ': 'Software Developer', 'company: ': 'Retalix', 'dates: ': 'Apr 2000 to Apr 2003 · 3 yrs 1 mo', 'location: ': 'Developed server side components for a software controller for petrol pumps that allowed convenience store cashiers to control pumps at a station from the cash register. Shifted code from older, C-style code to object oriented code by implementing design patterns and making heavy use of polymorphism. Developed a Microsoft SQL Server based Back Office solution for retailers. While working on this project, I optimized SQL scripts, improving runtime by as much as 60%. Developed a suite of monitoring scripts, designed to pre-emptively detect problems on customer systems.'}], 'education': [{'school: ': 'Tel Aviv University', 'degree: ': 'B.Sc., Computer Science & Philosophy', 'dates: ': '1998 - 2002'}], 'profile_dist': '2nd', 'mutuals': 'Matt Golden, Nicole Fitzgerald, and 3 other mutual connections', 'profile_description': 'Head of Software Engineering @ Lemurian Labs | Software Management', 'profile_link': 'https://www.linkedin.com/in/eyal-susser-2a35112/'}, {'name': 'Monica Shen', 'experiences': [{'title: ': 'Staff product manager', 'company: ': 'Walmart Global Tech', 'dates: ': '2024 to Present · 8 mos'}, {'title: ': 'Product', 'company: ': 'Stealth Startup · Full-time', 'dates: ': '2022 to 2024 · 2 yrs'}, {'title: ': 'Product @ Flexport', 'company: ': 'Flexport · Full-time', 'dates: ': '2020 to 2022 · 2 yrs'}, {'title: ': 'Amazon', 'company: ': '4 yrs 1 mo', 'dates: ': 'Product, Product Management', 'location: ': '2017 to 2020 · 3 yrs', 'summary: ': 'Amazon Expansion Product manager for global expansion strategies and efforts. These efforts include a few key charters for retail business: lead identification, SMB merchant onboarding, catalog management, selection/detail page localization, inventory planning and reconciliation, ordering, and marketing incentives. -Manage end-to-end product releases in 5 markets and 3 languages, successfully onboarding xK+ merchants with xM+ products annually -Launch a light-weight expansion flow for emerging markets, which demonstrated ability to accelerate selection expansion', 'remaining: ': 'Risk Manager, 2016 to 2017 · 1 yr, HS3C - Managed product integrity system to eliminate and remove poor experience for customers.Developed roadmap and use cases to prevent and mitigate abuses including taking bad products, actors, and behaviors off Amazon’s platform - Built end-to-end flows for handling abuse including reporting, inspection and auditing'}, {'title: ': 'Co-Founder', 'company: ': 'Bella Luna', 'dates: ': '2014 to 2016 · 2 yrs', 'location: ': 'Bella Luna is an e-commerce platform and personal styling service for custom apparels. It enables designers and manufacturers to create, and sell their designs or merchandise made-to-order apparels. Bella Luna makes the business risk free for sellers by using sew-on-demand technology to create the items when they are ordered.'}, {'title: ': 'Product Management Consultant', 'company: ': 'Intertek', 'dates: ': '2010 to 2014 · 4 yrs', 'location: ': 'Led developing product and operation strategies for clients in intelligent consumer products. Leader in user study, product design, and production planning to bring high quality products to market. - Partnered with client’s product team to create Go-to-Market strategy through lean user research and benchmark program to measure competitive position - Responsible for planning and executing through product launch cycle; Oversaw specification design, testing, production planning with R&D and product engineers to ensure timely product release - Installed a root cause and corrective action program with failure mode and effect Analysis to improve real-time diagnosis'}], 'education': [{'school: ': 'University of Waterloo', 'degree: ': 'Master of Science (M.Sc.)'}, {'school: ': 'Donghua University', 'degree: ': 'Material engineering (B.Eng)'}], 'profile_dist': '2nd', 'mutuals': 'Timothy Sehn, Emma Guo, and 1 other mutual connection', 'profile_description': 'Product in e-commerce and supply chain management', 'profile_link': 'https://www.linkedin.com/in/monica-shen-91815014/'}]\n",
      "4\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[26], line 4\u001b[0m\n\u001b[1;32m      2\u001b[0m main_results \u001b[38;5;241m=\u001b[39m openResults(resultsPickle)\n\u001b[1;32m      3\u001b[0m folder_id \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m1vtHm_M0hwADZX76MLHXNL5WqhmE310Vo\u001b[39m\u001b[38;5;124m'\u001b[39m  \u001b[38;5;66;03m# Replace with your specific folder ID\u001b[39;00m\n\u001b[0;32m----> 4\u001b[0m spreadsheet_url \u001b[38;5;241m=\u001b[39m \u001b[43mcreate_and_export_profiles_to_sheets\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmain_results\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfolder_id\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSpreadsheet created: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mspreadsheet_url\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[0;32mIn[24], line 14\u001b[0m, in \u001b[0;36mcreate_and_export_profiles_to_sheets\u001b[0;34m(profiles, folder_id)\u001b[0m\n\u001b[1;32m     12\u001b[0m today_date \u001b[38;5;241m=\u001b[39m datetime\u001b[38;5;241m.\u001b[39mtoday()\u001b[38;5;241m.\u001b[39mstrftime(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124mY-\u001b[39m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124mm-\u001b[39m\u001b[38;5;132;01m%d\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     13\u001b[0m sheet_name \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mLinkedin Scraper Results \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtoday_date\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m---> 14\u001b[0m spreadsheet \u001b[38;5;241m=\u001b[39m \u001b[43mclient\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcreate\u001b[49m\u001b[43m(\u001b[49m\u001b[43msheet_name\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     15\u001b[0m sheet \u001b[38;5;241m=\u001b[39m spreadsheet\u001b[38;5;241m.\u001b[39msheet1\n\u001b[1;32m     17\u001b[0m \u001b[38;5;66;03m# Move the spreadsheet to the specified folder\u001b[39;00m\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/gspread/client.py:209\u001b[0m, in \u001b[0;36mClient.create\u001b[0;34m(self, title, folder_id)\u001b[0m\n\u001b[1;32m    206\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m folder_id \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    207\u001b[0m     payload[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparents\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m [folder_id]\n\u001b[0;32m--> 209\u001b[0m r \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhttp_client\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    210\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mpost\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mDRIVE_FILES_API_V3_URL\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mjson\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpayload\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparams\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mparams\u001b[49m\n\u001b[1;32m    211\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    212\u001b[0m spreadsheet_id \u001b[38;5;241m=\u001b[39m r\u001b[38;5;241m.\u001b[39mjson()[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mid\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m    213\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mopen_by_key(spreadsheet_id)\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/gspread/http_client.py:98\u001b[0m, in \u001b[0;36mHTTPClient.request\u001b[0;34m(self, method, endpoint, params, data, json, files, headers)\u001b[0m\n\u001b[1;32m     88\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mrequest\u001b[39m(\n\u001b[1;32m     89\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m     90\u001b[0m     method: \u001b[38;5;28mstr\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     96\u001b[0m     headers: Optional[MutableMapping[\u001b[38;5;28mstr\u001b[39m, \u001b[38;5;28mstr\u001b[39m]] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m     97\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Response:\n\u001b[0;32m---> 98\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msession\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     99\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmethod\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    100\u001b[0m \u001b[43m        \u001b[49m\u001b[43murl\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mendpoint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    101\u001b[0m \u001b[43m        \u001b[49m\u001b[43mjson\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mjson\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    102\u001b[0m \u001b[43m        \u001b[49m\u001b[43mparams\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    103\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdata\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    104\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfiles\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfiles\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    105\u001b[0m \u001b[43m        \u001b[49m\u001b[43mheaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    106\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    107\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    109\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m response\u001b[38;5;241m.\u001b[39mok:\n\u001b[1;32m    110\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m response\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/google/auth/transport/requests.py:541\u001b[0m, in \u001b[0;36mAuthorizedSession.request\u001b[0;34m(self, method, url, data, headers, max_allowed_time, timeout, **kwargs)\u001b[0m\n\u001b[1;32m    538\u001b[0m remaining_time \u001b[38;5;241m=\u001b[39m guard\u001b[38;5;241m.\u001b[39mremaining_timeout\n\u001b[1;32m    540\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m TimeoutGuard(remaining_time) \u001b[38;5;28;01mas\u001b[39;00m guard:\n\u001b[0;32m--> 541\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mAuthorizedSession\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    542\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    543\u001b[0m \u001b[43m        \u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    544\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdata\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    545\u001b[0m \u001b[43m        \u001b[49m\u001b[43mheaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrequest_headers\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    546\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    547\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[1;32m    548\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    549\u001b[0m remaining_time \u001b[38;5;241m=\u001b[39m guard\u001b[38;5;241m.\u001b[39mremaining_timeout\n\u001b[1;32m    551\u001b[0m \u001b[38;5;66;03m# If the response indicated that the credentials needed to be\u001b[39;00m\n\u001b[1;32m    552\u001b[0m \u001b[38;5;66;03m# refreshed, then refresh the credentials and re-attempt the\u001b[39;00m\n\u001b[1;32m    553\u001b[0m \u001b[38;5;66;03m# request.\u001b[39;00m\n\u001b[1;32m    554\u001b[0m \u001b[38;5;66;03m# A stored token may expire between the time it is retrieved and\u001b[39;00m\n\u001b[1;32m    555\u001b[0m \u001b[38;5;66;03m# the time the request is made, so we may need to try twice.\u001b[39;00m\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/requests/sessions.py:587\u001b[0m, in \u001b[0;36mSession.request\u001b[0;34m(self, method, url, params, data, headers, cookies, files, auth, timeout, allow_redirects, proxies, hooks, stream, verify, cert, json)\u001b[0m\n\u001b[1;32m    582\u001b[0m send_kwargs \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m    583\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtimeout\u001b[39m\u001b[38;5;124m\"\u001b[39m: timeout,\n\u001b[1;32m    584\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mallow_redirects\u001b[39m\u001b[38;5;124m\"\u001b[39m: allow_redirects,\n\u001b[1;32m    585\u001b[0m }\n\u001b[1;32m    586\u001b[0m send_kwargs\u001b[38;5;241m.\u001b[39mupdate(settings)\n\u001b[0;32m--> 587\u001b[0m resp \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msend\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprep\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43msend_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    589\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m resp\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/requests/sessions.py:701\u001b[0m, in \u001b[0;36mSession.send\u001b[0;34m(self, request, **kwargs)\u001b[0m\n\u001b[1;32m    698\u001b[0m start \u001b[38;5;241m=\u001b[39m preferred_clock()\n\u001b[1;32m    700\u001b[0m \u001b[38;5;66;03m# Send the request\u001b[39;00m\n\u001b[0;32m--> 701\u001b[0m r \u001b[38;5;241m=\u001b[39m \u001b[43madapter\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msend\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    703\u001b[0m \u001b[38;5;66;03m# Total elapsed time of the request (approximately)\u001b[39;00m\n\u001b[1;32m    704\u001b[0m elapsed \u001b[38;5;241m=\u001b[39m preferred_clock() \u001b[38;5;241m-\u001b[39m start\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/requests/adapters.py:489\u001b[0m, in \u001b[0;36mHTTPAdapter.send\u001b[0;34m(self, request, stream, timeout, verify, cert, proxies)\u001b[0m\n\u001b[1;32m    487\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    488\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m chunked:\n\u001b[0;32m--> 489\u001b[0m         resp \u001b[38;5;241m=\u001b[39m \u001b[43mconn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43murlopen\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    490\u001b[0m \u001b[43m            \u001b[49m\u001b[43mmethod\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    491\u001b[0m \u001b[43m            \u001b[49m\u001b[43murl\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    492\u001b[0m \u001b[43m            \u001b[49m\u001b[43mbody\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbody\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    493\u001b[0m \u001b[43m            \u001b[49m\u001b[43mheaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    494\u001b[0m \u001b[43m            \u001b[49m\u001b[43mredirect\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    495\u001b[0m \u001b[43m            \u001b[49m\u001b[43massert_same_host\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    496\u001b[0m \u001b[43m            \u001b[49m\u001b[43mpreload_content\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    497\u001b[0m \u001b[43m            \u001b[49m\u001b[43mdecode_content\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    498\u001b[0m \u001b[43m            \u001b[49m\u001b[43mretries\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmax_retries\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    499\u001b[0m \u001b[43m            \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    500\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    502\u001b[0m     \u001b[38;5;66;03m# Send the request.\u001b[39;00m\n\u001b[1;32m    503\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    504\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(conn, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mproxy_pool\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/urllib3/connectionpool.py:703\u001b[0m, in \u001b[0;36mHTTPConnectionPool.urlopen\u001b[0;34m(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, **response_kw)\u001b[0m\n\u001b[1;32m    700\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_prepare_proxy(conn)\n\u001b[1;32m    702\u001b[0m \u001b[38;5;66;03m# Make the request on the httplib connection object.\u001b[39;00m\n\u001b[0;32m--> 703\u001b[0m httplib_response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_make_request\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    704\u001b[0m \u001b[43m    \u001b[49m\u001b[43mconn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    705\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    706\u001b[0m \u001b[43m    \u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    707\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout_obj\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    708\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbody\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbody\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    709\u001b[0m \u001b[43m    \u001b[49m\u001b[43mheaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    710\u001b[0m \u001b[43m    \u001b[49m\u001b[43mchunked\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mchunked\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    711\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    713\u001b[0m \u001b[38;5;66;03m# If we're going to release the connection in ``finally:``, then\u001b[39;00m\n\u001b[1;32m    714\u001b[0m \u001b[38;5;66;03m# the response doesn't need to know about the connection. Otherwise\u001b[39;00m\n\u001b[1;32m    715\u001b[0m \u001b[38;5;66;03m# it will also try to release it and we'll have a double-release\u001b[39;00m\n\u001b[1;32m    716\u001b[0m \u001b[38;5;66;03m# mess.\u001b[39;00m\n\u001b[1;32m    717\u001b[0m response_conn \u001b[38;5;241m=\u001b[39m conn \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m release_conn \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/urllib3/connectionpool.py:449\u001b[0m, in \u001b[0;36mHTTPConnectionPool._make_request\u001b[0;34m(self, conn, method, url, timeout, chunked, **httplib_request_kw)\u001b[0m\n\u001b[1;32m    444\u001b[0m             httplib_response \u001b[38;5;241m=\u001b[39m conn\u001b[38;5;241m.\u001b[39mgetresponse()\n\u001b[1;32m    445\u001b[0m         \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    446\u001b[0m             \u001b[38;5;66;03m# Remove the TypeError from the exception chain in\u001b[39;00m\n\u001b[1;32m    447\u001b[0m             \u001b[38;5;66;03m# Python 3 (including for exceptions like SystemExit).\u001b[39;00m\n\u001b[1;32m    448\u001b[0m             \u001b[38;5;66;03m# Otherwise it looks like a bug in the code.\u001b[39;00m\n\u001b[0;32m--> 449\u001b[0m             \u001b[43msix\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mraise_from\u001b[49m\u001b[43m(\u001b[49m\u001b[43me\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m    450\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (SocketTimeout, BaseSSLError, SocketError) \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    451\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_raise_timeout(err\u001b[38;5;241m=\u001b[39me, url\u001b[38;5;241m=\u001b[39murl, timeout_value\u001b[38;5;241m=\u001b[39mread_timeout)\n",
      "File \u001b[0;32m<string>:3\u001b[0m, in \u001b[0;36mraise_from\u001b[0;34m(value, from_value)\u001b[0m\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/urllib3/connectionpool.py:444\u001b[0m, in \u001b[0;36mHTTPConnectionPool._make_request\u001b[0;34m(self, conn, method, url, timeout, chunked, **httplib_request_kw)\u001b[0m\n\u001b[1;32m    441\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[1;32m    442\u001b[0m     \u001b[38;5;66;03m# Python 3\u001b[39;00m\n\u001b[1;32m    443\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 444\u001b[0m         httplib_response \u001b[38;5;241m=\u001b[39m \u001b[43mconn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgetresponse\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    445\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    446\u001b[0m         \u001b[38;5;66;03m# Remove the TypeError from the exception chain in\u001b[39;00m\n\u001b[1;32m    447\u001b[0m         \u001b[38;5;66;03m# Python 3 (including for exceptions like SystemExit).\u001b[39;00m\n\u001b[1;32m    448\u001b[0m         \u001b[38;5;66;03m# Otherwise it looks like a bug in the code.\u001b[39;00m\n\u001b[1;32m    449\u001b[0m         six\u001b[38;5;241m.\u001b[39mraise_from(e, \u001b[38;5;28;01mNone\u001b[39;00m)\n",
      "File \u001b[0;32m/Library/Developer/CommandLineTools/Library/Frameworks/Python3.framework/Versions/3.9/lib/python3.9/http/client.py:1349\u001b[0m, in \u001b[0;36mHTTPConnection.getresponse\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1347\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1348\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1349\u001b[0m         \u001b[43mresponse\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbegin\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1350\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mConnectionError\u001b[39;00m:\n\u001b[1;32m   1351\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclose()\n",
      "File \u001b[0;32m/Library/Developer/CommandLineTools/Library/Frameworks/Python3.framework/Versions/3.9/lib/python3.9/http/client.py:316\u001b[0m, in \u001b[0;36mHTTPResponse.begin\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    314\u001b[0m \u001b[38;5;66;03m# read until we get a non-100 response\u001b[39;00m\n\u001b[1;32m    315\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[0;32m--> 316\u001b[0m     version, status, reason \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_read_status\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    317\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m status \u001b[38;5;241m!=\u001b[39m CONTINUE:\n\u001b[1;32m    318\u001b[0m         \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "File \u001b[0;32m/Library/Developer/CommandLineTools/Library/Frameworks/Python3.framework/Versions/3.9/lib/python3.9/http/client.py:277\u001b[0m, in \u001b[0;36mHTTPResponse._read_status\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    276\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_read_status\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m--> 277\u001b[0m     line \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mstr\u001b[39m(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreadline\u001b[49m\u001b[43m(\u001b[49m\u001b[43m_MAXLINE\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124miso-8859-1\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    278\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(line) \u001b[38;5;241m>\u001b[39m _MAXLINE:\n\u001b[1;32m    279\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m LineTooLong(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstatus line\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m/Library/Developer/CommandLineTools/Library/Frameworks/Python3.framework/Versions/3.9/lib/python3.9/socket.py:704\u001b[0m, in \u001b[0;36mSocketIO.readinto\u001b[0;34m(self, b)\u001b[0m\n\u001b[1;32m    702\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[1;32m    703\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 704\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sock\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrecv_into\u001b[49m\u001b[43m(\u001b[49m\u001b[43mb\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    705\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m timeout:\n\u001b[1;32m    706\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_timeout_occurred \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "File \u001b[0;32m/Library/Developer/CommandLineTools/Library/Frameworks/Python3.framework/Versions/3.9/lib/python3.9/ssl.py:1241\u001b[0m, in \u001b[0;36mSSLSocket.recv_into\u001b[0;34m(self, buffer, nbytes, flags)\u001b[0m\n\u001b[1;32m   1237\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m flags \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m   1238\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m   1239\u001b[0m           \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnon-zero flags not allowed in calls to recv_into() on \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m\n\u001b[1;32m   1240\u001b[0m           \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m)\n\u001b[0;32m-> 1241\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnbytes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbuffer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1242\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1243\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39mrecv_into(buffer, nbytes, flags)\n",
      "File \u001b[0;32m/Library/Developer/CommandLineTools/Library/Frameworks/Python3.framework/Versions/3.9/lib/python3.9/ssl.py:1099\u001b[0m, in \u001b[0;36mSSLSocket.read\u001b[0;34m(self, len, buffer)\u001b[0m\n\u001b[1;32m   1097\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1098\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m buffer \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m-> 1099\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sslobj\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbuffer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1100\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1101\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sslobj\u001b[38;5;241m.\u001b[39mread(\u001b[38;5;28mlen\u001b[39m)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Example usage\n",
    "main_results = openResults(resultsPickle)\n",
    "folder_id = '1vtHm_M0hwADZX76MLHXNL5WqhmE310Vo'  # Replace with your specific folder ID\n",
    "spreadsheet_url = create_and_export_profiles_to_sheets(main_results, folder_id)\n",
    "print(f\"Spreadsheet created: {spreadsheet_url}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0df469c1",
   "metadata": {},
   "source": [
    "# QUERY 2: UNICORN COMPANIES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df63090d",
   "metadata": {},
   "outputs": [],
   "source": [
    "historyPickle = 'db/unicorn_history.pickle'\n",
    "toScrapePickle = 'db/unicorn_to_scrape.pickle'\n",
    "resultsPickle = 'db/unicorn_results.pickle'\n",
    "failedURLsTextFile = 'db/unicorn_failed_urls.txt'\n",
    "\n",
    "SALES_NAV_SEARCH_URL = \"\"\"\n",
    "https://www.linkedin.com/sales/search/people?savedSearchId=1833971588&sessionId=zVKTgf2GQk2owfFVzL9aFQ%3D%3D\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1a8a32b",
   "metadata": {},
   "source": [
    "### Sales nav scraper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc90b784",
   "metadata": {},
   "outputs": [],
   "source": [
    "already_scraped_urls, to_scrape_urls = check_pickles(historyPickle, toScrapePickle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82fb8467",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    updated_to_scrape_urls = scrape_profiles(driver, SALES_NAV_SEARCH_URL, already_scraped_urls, to_scrape_urls, toScrapePickle)\n",
    "    print(updated_to_scrape_urls)\n",
    "except Exception as e:\n",
    "    print(f\"An unexpected error occurred: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98ff5ea6",
   "metadata": {},
   "source": [
    "### Deduplicate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96c9a4c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "deduplicate(toScrapePickle, to_scrape_urls, already_scraped_urls)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29a8309e",
   "metadata": {},
   "source": [
    "### Profile scraper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77b3e90f",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = scrape_all_profiles(driver, historyPickle, toScrapePickle, resultsPickle, failedURLsTextFile)\n",
    "print(len(results))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d71f2e26",
   "metadata": {},
   "source": [
    "# QUERY 3: ACQUIRED COMPANIES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22414fd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "historyPickle = 'db/acquired_history.pickle'\n",
    "toScrapePickle = 'db/acquired_to_scrape.pickle'\n",
    "resultsPickle = 'db/acquired_results.pickle'\n",
    "failedURLsTextFile = 'db/acquired_failed_urls.txt'\n",
    "\n",
    "SALES_NAV_SEARCH_URL = \"\"\"\n",
    "https://www.linkedin.com/sales/search/people?savedSearchId=1833971612&sessionId=R7vv8AiYRXiqMseY1m3CwQ%3D%3D\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c88db61",
   "metadata": {},
   "source": [
    "### Sales nav scraper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "643faf85",
   "metadata": {},
   "outputs": [],
   "source": [
    "already_scraped_urls, to_scrape_urls = check_pickles(historyPickle, toScrapePickle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2ee22e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    updated_to_scrape_urls = scrape_profiles(driver, SALES_NAV_SEARCH_URL, already_scraped_urls, to_scrape_urls, toScrapePickle)\n",
    "    print(updated_to_scrape_urls)\n",
    "except Exception as e:\n",
    "    print(f\"An unexpected error occurred: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8eb29f7",
   "metadata": {},
   "source": [
    "### Deduplicate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d324c97",
   "metadata": {},
   "outputs": [],
   "source": [
    "deduplicate(toScrapePickle, to_scrape_urls, already_scraped_urls)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e12ff51",
   "metadata": {},
   "source": [
    "### Profile scraper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c357cf25",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = scrape_all_profiles(driver, historyPickle, toScrapePickle, resultsPickle, failedURLsTextFile)\n",
    "print(len(results))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b637904",
   "metadata": {},
   "source": [
    "# QUERY 4: VC PORTFOLIO COMPANIES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4efcd7ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "historyPickle = 'db/portfolio_history.pickle'\n",
    "toScrapePickle = 'db/portfolio_to_scrape.pickle'\n",
    "resultsPickle = 'db/portfolio_results.pickle'\n",
    "failedURLsTextFile = 'db/portfolio_failed_urls.txt'\n",
    "\n",
    "SALES_NAV_SEARCH_URL = \"\"\"\n",
    "https://www.linkedin.com/sales/search/people?savedSearchId=1833971604&sessionId=zVKTgf2GQk2owfFVzL9aFQ%3D%3D\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "457c988d",
   "metadata": {},
   "source": [
    "### Sales nav scraper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e3747a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "already_scraped_urls, to_scrape_urls = check_pickles(historyPickle, toScrapePickle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a411299f",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    updated_to_scrape_urls = scrape_profiles(driver, SALES_NAV_SEARCH_URL, already_scraped_urls, to_scrape_urls, toScrapePickle)\n",
    "    print(updated_to_scrape_urls)\n",
    "except Exception as e:\n",
    "    print(f\"An unexpected error occurred: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c4905aa",
   "metadata": {},
   "source": [
    "### Deduplicate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "baf6a7b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "deduplicate(toScrapePickle, to_scrape_urls, already_scraped_urls)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5281ff8",
   "metadata": {},
   "source": [
    "### Profile scraper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5ec701f",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = scrape_all_profiles(driver, historyPickle, toScrapePickle, resultsPickle, failedURLsTextFile)\n",
    "print(len(results))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47d73a6f",
   "metadata": {},
   "source": [
    "# QUERY 5: INFRA COMPANIES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa93ce00",
   "metadata": {},
   "outputs": [],
   "source": [
    "historyPickle = 'db/infra_history.pickle'\n",
    "toScrapePickle = 'db/infra_to_scrape.pickle'\n",
    "resultsPickle = 'db/infra_results.pickle'\n",
    "failedURLsTextFile = 'db/infra_failed_urls.txt'\n",
    "\n",
    "SALES_NAV_SEARCH_URL = \"\"\"\n",
    "https://www.linkedin.com/sales/search/people?savedSearchId=1833971620&sessionId=R7vv8AiYRXiqMseY1m3CwQ%3D%3D\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "924aae38",
   "metadata": {},
   "source": [
    "### Sales nav scraper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7961266",
   "metadata": {},
   "outputs": [],
   "source": [
    "already_scraped_urls, to_scrape_urls = check_pickles(historyPickle, toScrapePickle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2f9a6f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    updated_to_scrape_urls = scrape_profiles(driver, SALES_NAV_SEARCH_URL, already_scraped_urls, to_scrape_urls, toScrapePickle)\n",
    "    print(updated_to_scrape_urls)\n",
    "except Exception as e:\n",
    "    print(f\"An unexpected error occurred: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eec8b4b3",
   "metadata": {},
   "source": [
    "### Deduplicate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2df023a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "deduplicate(toScrapePickle, to_scrape_urls, already_scraped_urls)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f55d6e65",
   "metadata": {},
   "source": [
    "### Profile scraper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dffc900a",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = scrape_all_profiles(driver, historyPickle, toScrapePickle, resultsPickle, failedURLsTextFile)\n",
    "print(len(results))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47185763-6f2b-4f89-9acc-1651887008f6",
   "metadata": {},
   "source": [
    "# EXPORTING RESULTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0906d83",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create dataframe\n",
    "rows = []\n",
    "for candidate in results:\n",
    "    row = parseCandidate(candidate)\n",
    "    rows.append(row)\n",
    "\n",
    "# Write to CSV\n",
    "col_titles = rows[0].keys()\n",
    "\n",
    "try:\n",
    "    with open('candidates.csv', 'w', newline='') as output_file:\n",
    "        dict_writer = csv.DictWriter(output_file, col_titles)\n",
    "        dict_writer.writeheader()\n",
    "        dict_writer.writerows(rows)\n",
    "        print(\"Successfully exported to csv\")\n",
    "except Exception as e:\n",
    "    print(f\"Failed to export to csv. Error: {e}\")\n",
    "\n",
    "#Export to Excel\n",
    "try:\n",
    "    df.to_excel('candidates.xlsx', index=False)\n",
    "    print(\"Exported to Excel\")\n",
    "except:\n",
    "    print(\"Failed to export to Excel\")\n",
    "\n",
    "    \n",
    "# update db/already_scraped.pickle\n",
    "with open('db/already_scraped.pickle', 'rb') as f:\n",
    "    already_scraped = pickle.load(f)\n",
    "    print(f\"Previously scraped: {len(already_scraped)}\")\n",
    "    already_scraped = already_scraped + already_scraped_urls\n",
    "    already_scraped = list(set(already_scraped))\n",
    "    print(f\"Newly scraped: {len(already_scraped)}\")\n",
    "with open('db/already_scraped.pickle', 'wb') as f:\n",
    "    pickle.dump(already_scraped, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97efd509",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from google.auth.transport.requests import Request\n",
    "# from google.oauth2.credentials import Credentials\n",
    "# from google_auth_oauthlib.flow import InstalledAppFlow\n",
    "# from googleapiclient.discovery import build\n",
    "# from googleapiclient.http import MediaFileUpload\n",
    "\n",
    "# # If modifying these SCOPES, delete the file token.pickle.\n",
    "# SCOPES = ['https://www.googleapis.com/auth/drive.file']\n",
    "\n",
    "# def authenticate():\n",
    "#     creds = None\n",
    "#     # The file token.pickle stores the user's access and refresh tokens, and is created automatically when the authorization flow completes for the first time.\n",
    "#     if os.path.exists('token.pickle'):\n",
    "#         with open('token.pickle', 'rb') as token:\n",
    "#             creds = pickle.load(token)\n",
    "#     # If there are no (valid) credentials available, let the user log in.\n",
    "#     if not creds or not creds.valid:\n",
    "#         if creds and creds.expired and creds.refresh_token:\n",
    "#             creds.refresh(Request())\n",
    "#         else:\n",
    "#             flow = InstalledAppFlow.from_client_secrets_file(\n",
    "#                 'client_secrets.json', SCOPES)\n",
    "#             creds = flow.run_local_server(port=0)\n",
    "#         # Save the credentials for the next run\n",
    "#         with open('token.pickle', 'wb') as token:\n",
    "#             pickle.dump(creds, token)\n",
    "#     return creds\n",
    "\n",
    "# def upload_file_to_drive(file_path, file_name, mime_type):\n",
    "#     creds = authenticate()\n",
    "#     service = build('drive', 'v3', credentials=creds)\n",
    "\n",
    "#     file_metadata = {'name': file_name}\n",
    "#     media = MediaFileUpload(file_path, mimetype=mime_type)\n",
    "\n",
    "#     file = service.files().create(body=file_metadata, media_body=media, fields='id').execute()\n",
    "#     print('File ID: %s' % file.get('id'))\n",
    "\n",
    "# if __name__ == '__main__':\n",
    "#     file_path = 'path_to_your_local_excel_file.xlsx'  # Replace with the path to your local file\n",
    "#     file_name = 'your_excel_file.xlsx'  # Replace with the desired name for the file in Google Drive\n",
    "#     mime_type = 'application/vnd.openxmlformats-officedocument.spreadsheetml.sheet'\n",
    "\n",
    "#     upload_file_to_drive(file_path, file_name, mime_type)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ee20a9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# categories = COMPANY_CATEGORIES\n",
    "\n",
    "# # Example color mapping for categories\n",
    "# category_colors = {\n",
    "#     \"SECURITY\": 'red',\n",
    "#     \"OTHER\": 'blue',\n",
    "#     \"PUBLIC\": 'green',\n",
    "#     \"INFRA\": 'yellow',\n",
    "#     \"FINTECH\": 'orange',\n",
    "#     \"CRYPTO\": 'purple',\n",
    "#     \"FRONTIER\": 'cyan',\n",
    "#     \"AI\": 'magenta'\n",
    "# }\n",
    "\n",
    "# # Create a reverse dictionary for easier lookup: {company: category}\n",
    "# company_category = {}\n",
    "# for category, companies in categories.items():\n",
    "#     for company in companies:\n",
    "#         company_category[company] = category\n",
    "\n",
    "# # Modify the style function\n",
    "# def highlight_by_category(val):\n",
    "#     category = company_category.get(val)\n",
    "#     if category:\n",
    "#         color = category_colors.get(category, 'none')  # default to 'none' if no color is specified\n",
    "#     else:\n",
    "#         color = 'none'\n",
    "#     return f'background-color: {color}'\n",
    "\n",
    "# import re\n",
    "# ILLEGAL_CHARACTERS_RE = re.compile(r'[\\000-\\010]|[\\013-\\014]|[\\016-\\037]')\n",
    "# def find_illegal_characters(df):\n",
    "#     for column in df.columns:\n",
    "#         for idx, item in enumerate(df[column]):\n",
    "#             if isinstance(item, str) and ILLEGAL_CHARACTERS_RE.search(item):\n",
    "#                 # replace illegal characters with an empty string\n",
    "#                 df[column][idx] = ILLEGAL_CHARACTERS_RE.sub('', item)\n",
    "#     return df\n",
    "\n",
    "# styled_df = df\n",
    "# styled_df = find_illegal_characters(styled_df)\n",
    "# styled_df = df.style.applymap(highlight_by_category)\n",
    "\n",
    "# # Save the styled DataFrame to an Excel file\n",
    "# # get today's date in MM-DD-YYYY format\n",
    "# import datetime\n",
    "# now = datetime.datetime.now()\n",
    "# date = now.strftime(\"%m-%d-%Y\")\n",
    "# styled_df.to_excel(f'results/{date}_{start}-{end}.xlsx', engine='openpyxl', index=False)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
