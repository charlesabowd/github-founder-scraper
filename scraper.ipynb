{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5b930f8e",
   "metadata": {},
   "source": [
    "#  SETUP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9e6fd8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# if the script finishes very quickly (and generates an empty excel file), click run again\n",
    "# if the script errors on the \"Login Cell\" (added a comment to indicate which cell that is below), set IS_HEADLESS to \"False\" and run again. The scraper will automatically launch a page and attempt to login to LinkedIn. It's likely erroring because LinkedIn is asking for a captcha to verify the user is not a bot. Solve the captch/challenge and login. Once successfully logged in, set IS_HEADLESS back to \"True\" and run again.\n",
    "\n",
    "import pickle\n",
    "import csv\n",
    "import os\n",
    "import random\n",
    "import time\n",
    "from linkedin_scraper import actions\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.firefox.options import Options as FirefoxOptions\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.common.exceptions import TimeoutException, StaleElementReferenceException, NoSuchElementException, ElementClickInterceptedException\n",
    "from dotenv import load_dotenv\n",
    "import pandas as pd\n",
    "# for google docs upload\n",
    "import gspread\n",
    "from oauth2client.service_account import ServiceAccountCredentials\n",
    "from io import StringIO\n",
    "\n",
    "\n",
    "IS_HEADLESS = False\n",
    "\n",
    "# Set environment variables to email and password\n",
    "load_dotenv()\n",
    "LINKEDIN_EMAIL = os.environ.get('LINKEDIN_EMAIL')\n",
    "LINKEDIN_PASSWORD = os.environ.get('LINKEDIN_PASSWORD')\n",
    "# Check if the environment variables are set\n",
    "if not LINKEDIN_EMAIL or not LINKEDIN_PASSWORD:\n",
    "    raise ValueError(\"LinkedIn credentials not set in environment variables\")\n",
    "\n",
    "\n",
    "COMPANY_CATEGORIES = {\n",
    "    \"SECURITY\": [\n",
    "        \"Okta\",\n",
    "        \"Snyk\",\n",
    "        \"R2C/Semgrep\", \n",
    "        \"Wiz\",\n",
    "        \"Lacework\",\n",
    "        \"Crowdstrike\",\n",
    "        \"Palo Alto Networks\",\n",
    "        \"Island\",\n",
    "        \"Vanta\",\n",
    "        \"Material Security\",\n",
    "        \"Abnormal Security\",\n",
    "        \"Samsara\",\n",
    "    ],\n",
    "    \"OTHER\": [\n",
    "        \"Figma\",\n",
    "        \"Airtable\",\n",
    "        \"Notion\",\n",
    "        \"Canva\",\n",
    "        \"Webflow\",\n",
    "        \"Faire\",\n",
    "        \"Deel\",\n",
    "        \"Rippling\",\n",
    "        \"Flexport\",\n",
    "        \"Benchling\",\n",
    "        \"Solugen\"\n",
    "    ],\n",
    "    \"PUBLIC\": [\n",
    "        \"Doordash\",\n",
    "        \"Uber\",\n",
    "        \"Palantir\",\n",
    "        \"Airbnb\",\n",
    "        \"Instacart\"\n",
    "    ],\n",
    "    \"INFRA\": [\n",
    "        \"Fivetran\",\n",
    "        \"DBT\",\n",
    "        \"Temporal\",\n",
    "        \"Cockroach Labs\",\n",
    "        \"Grafana\",\n",
    "        \"Zapier\",\n",
    "        \"Starburst\",\n",
    "        \"Retool\",\n",
    "        \"Sentry\",\n",
    "        \"Sourcegraph\",\n",
    "        \"Cribl\",\n",
    "        \"Vercel\",\n",
    "        \"Clickhouse\",\n",
    "        \"Github,\"\n",
    "        \"Cisco Meraki\",\n",
    "    ],\n",
    "    \"FINTECH\": [\n",
    "        \"Robinhood\",\n",
    "        \"Square\",\n",
    "        \"Stripe\",\n",
    "        \"Ramp\",\n",
    "        \"Brex\",\n",
    "        \"Plaid\",\n",
    "        \"Modern Treasury\",\n",
    "        \"Mercury\",\n",
    "        \"Persona\",\n",
    "        \"Klarna\",\n",
    "        \"Nubank\"\n",
    "    ],\n",
    "    \"CRYPTO\": [\n",
    "        \"Coinbase\",\n",
    "        \"Uniswap\",\n",
    "        \"Chainalysis\",\n",
    "        \"Arbitrum\",\n",
    "        \"TRM\",\n",
    "        \"Fireblocks\",\n",
    "        \"Eigenlayer\"\n",
    "    ],\n",
    "    \"FRONTIER\": [\n",
    "        \"Anduril\",\n",
    "        \"SpaceX\",\n",
    "        \"Zipline\",\n",
    "        \"Varda\",\n",
    "        \"Hadrian\"\n",
    "    ],\n",
    "    \"AI\": [\n",
    "        \"Bytedance\",\n",
    "        \"Scale AI\",\n",
    "        \"Anthropic\",\n",
    "        \"Robust intelligence\",\n",
    "        \"OpenAI\",\n",
    "        \"Predibase\",\n",
    "        \"Cohere\",\n",
    "        \"Databricks\",\n",
    "        \"Hugging Face\",\n",
    "        \"RunwayML\",\n",
    "        \"Tecton\",\n",
    "        \"Weights & Biases\",\n",
    "        \"Kumo AI\",\n",
    "        \"NVIDIA\",\n",
    "        \"Adept\",\n",
    "        \"Glean\",\n",
    "        \"Character.ai\",\n",
    "        \"Midjourney\",\n",
    "        \"Facebook AI\",\n",
    "        \"FAIR\",\n",
    "        \"Google brain\"\n",
    "    ]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adbde830",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "print(sys.executable)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7957b23f",
   "metadata": {},
   "source": [
    "# LOGIN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3596a5f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# IF THIS CELL ERRORS DUE TO CAPTCHA, do this:\n",
    "# manually complete the captcha, click on the next cell, and select Run menu, select \"run selected cell and all below\" \n",
    "\n",
    "def instantiate_driver():\n",
    "    options = FirefoxOptions()\n",
    "    if IS_HEADLESS:\n",
    "        options.add_argument(\"--headless\")\n",
    "    driver = webdriver.Firefox(options=options)\n",
    "    \n",
    "    try:\n",
    "        actions.login(driver, LINKEDIN_EMAIL, LINKEDIN_PASSWORD)\n",
    "    except Exception as e:\n",
    "        print(\"Error logging in. Please complete the captcha challenge and login manually.\")\n",
    "        print(e)\n",
    "\n",
    "    time.sleep(15)\n",
    "    return driver\n",
    "    \n",
    "driver = instantiate_driver()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "515b071f",
   "metadata": {},
   "source": [
    "# SALES NAV HELPER FUNCTIONS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "027fd1fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ScrapedProfile:\n",
    "    def __init__(self, profile_name, experiences, education, profile_dist, mutuals, profile_description, profile_link):\n",
    "        self.profile_name = profile_name\n",
    "        self.experiences = experiences\n",
    "        self.education = education\n",
    "        self.profile_dist = profile_dist\n",
    "        self.mutuals = mutuals\n",
    "        self.profile_description = profile_description\n",
    "        self.profile_link = profile_link\n",
    "\n",
    "def check_pickles(historyPickle, toScrapePickle):\n",
    "    scraped_urls = []\n",
    "    with open(historyPickle, 'rb') as f:\n",
    "        try:\n",
    "            scraped_urls = pickle.load(f)\n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "            print(\"Error loading history pickle file\")\n",
    "        print(len(scraped_urls))\n",
    "\n",
    "    to_scrape_urls = []\n",
    "    with open(toScrapePickle, 'rb') as f:\n",
    "        try:\n",
    "            to_scrape_urls = pickle.load(f)\n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "            print(\"Error loading to scrape url pickle file\")\n",
    "        print(len(to_scrape_urls))\n",
    "\n",
    "    return scraped_urls, to_scrape_urls\n",
    "\n",
    "def deduplicate(toScrapePickle, to_scrape_urls, already_scraped_urls):\n",
    "    to_scrape_urls = list(set(to_scrape_urls))\n",
    "    originalCount = len(to_scrape_urls)\n",
    "    to_scrape_urls = [url for url in to_scrape_urls if url not in already_scraped_urls]\n",
    "    print(f\"Removed {originalCount - len(to_scrape_urls)} duplicates\")\n",
    "\n",
    "    with open(toScrapePickle, 'wb') as to_scrape_file:\n",
    "        pickle.dump(to_scrape_urls, to_scrape_file)\n",
    "    \n",
    "    print(len(to_scrape_urls))\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Wait for an element to be present on the page and return it.\n",
    "\n",
    "Parameters:\n",
    "- driver: The WebDriver instance\n",
    "- by: The method to locate the element (default: By.CLASS_NAME)\n",
    "- name: The name or identifier of the element to wait for\n",
    "- base: The base element to search from (default: None, which uses the driver)\n",
    "- timeout: Maximum time to wait for the element (default: 180 seconds)\n",
    "\n",
    "Returns:\n",
    "- The WebElement if found\n",
    "- None if the element is not found within the timeout period\n",
    "\"\"\"\n",
    "def wait_for_element_to_load(driver, by=By.CLASS_NAME, name=\"pv-top-card\", base=None, timeout=100):\n",
    "    base = base or driver\n",
    "    try:\n",
    "        element = WebDriverWait(base, timeout).until(\n",
    "            EC.presence_of_element_located((by, name))\n",
    "        )\n",
    "        return element\n",
    "    except TimeoutException:\n",
    "        print(f\"Timed out waiting for element: {by}={name}\")\n",
    "        return None\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred while waiting for element {by}={name}: {str(e)}\")\n",
    "        return None\n",
    "\n",
    "# Scrolls the page to bring the specified profile element into view\n",
    "# Returns True if successful, False if an error occurs\n",
    "def scroll_to_profile(driver, profile):\n",
    "    try:\n",
    "        driver.execute_script(\"arguments[0].scrollIntoView();\", profile)\n",
    "        return True\n",
    "    except Exception as e:\n",
    "        print(f\"Error scrolling to profile: {e}\")\n",
    "        return False\n",
    "\n",
    "# Clicks on the profile element to open its details\n",
    "# Returns True if successful, False if an error occurs\n",
    "def click_profile(profile):\n",
    "    try:\n",
    "        salesNavOpenProfileButton = profile.find_element(By.CLASS_NAME, \"artdeco-entity-lockup__title\")\n",
    "        salesNavOpenProfileButton.click()\n",
    "        return True\n",
    "    except (NoSuchElementException, ElementClickInterceptedException) as e:\n",
    "        print(f\"Error clicking profile: {e}\")\n",
    "        return False\n",
    "\n",
    "# Clicks the three dots button to open the dropdown menu\n",
    "# Returns True if successful, False if an error occurs\n",
    "def click_three_dots_button(driver):\n",
    "    try:\n",
    "        actionContainer = driver.find_element(By.CLASS_NAME, \"_actions-container_1dg5u8\")\n",
    "        threeDotsButton = actionContainer.find_element(By.CLASS_NAME, \"_icon_ps32ck\")\n",
    "        threeDotsButton.click()\n",
    "        return True\n",
    "    except (NoSuchElementException, ElementClickInterceptedException) as e:\n",
    "        print(f\"Error clicking three dots button\")\n",
    "        return False\n",
    "\n",
    "# Retrieves the LinkedIn URL from the dropdown menu\n",
    "# Returns the URL if successful, None if an error occurs\n",
    "def get_linkedin_url(driver):\n",
    "    try:\n",
    "        dropdownContainer = driver.find_element(By.CLASS_NAME, \"_visible_x5gf48\")\n",
    "        normalLinkedInUrl = dropdownContainer.find_elements(By.TAG_NAME, \"a\")[1].get_attribute(\"href\")\n",
    "        return normalLinkedInUrl\n",
    "    except (NoSuchElementException, IndexError) as e:\n",
    "        print(f\"Error getting LinkedIn URL: {e}\")\n",
    "        return None\n",
    "\n",
    "# Closes the profile popout\n",
    "# Returns True if successful, False if an error occurs\n",
    "def close_popout(driver):\n",
    "    try:\n",
    "        header = driver.find_element(By.CLASS_NAME, \"_inline-sidesheet-header-actions_1cn7lg\")\n",
    "        button = header.find_elements(By.CLASS_NAME, \"_button_ps32ck\")[1]\n",
    "        button.click()\n",
    "        return True\n",
    "    except (NoSuchElementException, ElementClickInterceptedException, IndexError) as e:\n",
    "        print(f\"Error closing popout: {e}\")\n",
    "        return False\n",
    "\n",
    "# Navigates to the next page of search results\n",
    "# Returns True if successful, False if there are no more pages or an error occurs\n",
    "def navigate_to_next_page(driver):\n",
    "    try:\n",
    "        nextPageButton = driver.find_element(By.CLASS_NAME, \"artdeco-pagination__button--next\")\n",
    "        nextPageButton.click()\n",
    "        return True\n",
    "    except (NoSuchElementException, ElementClickInterceptedException) as e:\n",
    "        print(f\"No more pages or error navigating: {e}\")\n",
    "        return False\n",
    "\n",
    "# Main scraping function\n",
    "def scrape_profiles(driver, SALES_NAV_SEARCH_URL, already_scraped_urls, to_scrape_urls, toScrapePickle):\n",
    "    # reinstantiate_driver(driver)\n",
    "    driver.get(SALES_NAV_SEARCH_URL)\n",
    "    \n",
    "    while True:\n",
    "        wait_for_element_to_load(driver, By.ID, \"search-results-container\")\n",
    "        profiles = driver.find_elements(By.CLASS_NAME, \"artdeco-list__item\")\n",
    "        \n",
    "        for profile in profiles:\n",
    "            if not scroll_to_profile(driver, profile):\n",
    "                continue\n",
    "\n",
    "            # time.sleep(2 + random.random() * 6)\n",
    "\n",
    "\n",
    "            if not wait_for_element_to_load(driver, By.CLASS_NAME, \"artdeco-entity-lockup__title\"):\n",
    "                continue\n",
    "\n",
    "            if not click_profile(profile):\n",
    "                continue\n",
    "            \n",
    "            # time.sleep(2 + random.random() * 6)\n",
    "\n",
    "            if not wait_for_element_to_load(driver, By.CLASS_NAME, \"_actions-container_1dg5u8\"):\n",
    "                continue\n",
    "\n",
    "            if not click_three_dots_button(driver):\n",
    "                continue\n",
    "\n",
    "            # time.sleep(2 + random.random() * 6)\n",
    "\n",
    "\n",
    "            if not wait_for_element_to_load(driver, By.CLASS_NAME, \"_visible_x5gf48\"):\n",
    "                continue\n",
    "\n",
    "            normalLinkedInUrl = get_linkedin_url(driver)\n",
    "            if normalLinkedInUrl:\n",
    "                if normalLinkedInUrl in already_scraped_urls:\n",
    "                    print(\"Skipping (already scraped): \" + normalLinkedInUrl)\n",
    "                else:\n",
    "                    to_scrape_urls.append(normalLinkedInUrl)\n",
    "                    with open(toScrapePickle, 'wb') as f:\n",
    "                        pickle.dump(to_scrape_urls, f)\n",
    "                    print(\"Successfully scraped: \" + normalLinkedInUrl)\n",
    "\n",
    "            if not close_popout(driver):\n",
    "                continue\n",
    "\n",
    "            # time.sleep(2 + random.random() * 6)\n",
    "\n",
    "\n",
    "        next_button = wait_for_element_to_load(driver, By.CLASS_NAME, \"artdeco-pagination__button--next\")\n",
    "        if not next_button or not next_button.is_enabled():\n",
    "            break\n",
    "\n",
    "        next_button.click()\n",
    "\n",
    "        if not wait_for_element_to_load(driver, By.CLASS_NAME, \"artdeco-list__item\"):\n",
    "            break\n",
    "\n",
    "    return to_scrape_urls"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d9c83c0",
   "metadata": {},
   "source": [
    "# PROFILE SCRAPING HELPER FUNCTIONS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce900eb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter():\n",
    "    pass     \n",
    "    # try:\n",
    "        #     WebDriverWait(driver, 240).until(lambda d: d.execute_script('return document.readyState') == 'complete')\n",
    "        # except:\n",
    "        #     driver = reinstantiate_driver()\n",
    "\n",
    "        # # FILTERING\n",
    "        \n",
    "        # likely_founder = True\n",
    "        # relevant_exp = True\n",
    "\n",
    "        # cur_exp = experiences[0]\n",
    "        # relevant_companies = [\"stealth\", \"new\"]\n",
    "        # if any(company in cur_exp.institution_name.split(\" Â·\")[0].lower() for company in relevant_companies) or \"present\" not in cur_exp.to_date.lower():\n",
    "        #     likely_founder = True\n",
    "\n",
    "        # relevant_titles = [\"product\", \"engineer\", \"sales\", \"business development\", \"founder\", \"head\", \"lead\", \"senior\", \"staff\", \"chief\", \"growth\"]\n",
    "        # for experience in experiences[1:5]:\n",
    "        #     if any(title in experience.position_title.lower() for title in relevant_titles):\n",
    "        #         relevant_exp = True\n",
    "        #         break\n",
    "        # relevant_exp = True\n",
    "\n",
    "        # if not (likely_founder and relevant_exp):\n",
    "        #     print(likely_founder, relevant_exp)\n",
    "        #     return None\n",
    "\n",
    "        # person_obj = Person(profile_link, driver = driver, scrape=False, experiences = [None])\n",
    "        # try:\n",
    "        #     WebDriverWait(driver, 240).until(lambda d: d.execute_script('return document.readyState') == 'complete')\n",
    "        # except:\n",
    "        #     driver = reinstantiate_driver(driver)\n",
    "        # time.sleep(2 + random.random() * 7)\n",
    "        \n",
    "# Helper function to scrape experiences\n",
    "# Returns scraped experiences if successful, otherwise returns empty list\n",
    "def get_experiences(driver):\n",
    "    scraped_experiences = []\n",
    "\n",
    "    try:\n",
    "        wait_for_element_to_load(driver, By.CSS_SELECTOR, \"li.pvs-list__paged-list-item.artdeco-list__item.pvs-list__item--line-separated.pvs-list__item--one-column\")\n",
    "        experience_items = driver.find_elements(By.CSS_SELECTOR, \"li.pvs-list__paged-list-item.artdeco-list__item.pvs-list__item--line-separated.pvs-list__item--one-column\")\n",
    "\n",
    "        if len(experience_items) > 0:\n",
    "            for item in experience_items:\n",
    "                hidden_spans = item.find_elements(By.CSS_SELECTOR, \"span.visually-hidden\")\n",
    "                experience_texts = [span.text for span in hidden_spans]\n",
    "                experience = {\n",
    "                    \"title: \": experience_texts[0],\n",
    "                    \"company: \": experience_texts[1],\n",
    "                    \"dates: \": experience_texts[2],\n",
    "                }\n",
    "                \n",
    "                if len(experience_texts) > 3:\n",
    "                    experience[\"location: \"] = experience_texts[3]\n",
    "                if len(experience_texts) > 4:\n",
    "                    experience[\"summary: \"] = experience_texts[4]\n",
    "                if len(experience_texts) > 5:\n",
    "                    experience[\"remaining: \"] = (\", \").join(experience_texts[5:])\n",
    "                scraped_experiences.append(experience)\n",
    "\n",
    "            print(\"Successfully scraped experiences\")\n",
    "        else:\n",
    "            print(\"No experiences found\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        print(\"ERROR: No experiences found\")\n",
    "\n",
    "    return scraped_experiences\n",
    "\n",
    "\n",
    "# Helper function to scrape education\n",
    "# Returns scraped education if successful, otherwise returns empty list\n",
    "def get_education(driver):\n",
    "    scraped_education = []\n",
    "\n",
    "    try:\n",
    "        wait_for_element_to_load(driver, By.CSS_SELECTOR, \"li.pvs-list__paged-list-item.artdeco-list__item.pvs-list__item--line-separated.pvs-list__item--one-column\")\n",
    "        education_items = driver.find_elements(By.CSS_SELECTOR, \"li.pvs-list__paged-list-item.artdeco-list__item.pvs-list__item--line-separated.pvs-list__item--one-column\")\n",
    "\n",
    "        if len(education_items) > 0:\n",
    "            for item in education_items:\n",
    "                hidden_spans = item.find_elements(By.CSS_SELECTOR, \"span.visually-hidden\")\n",
    "                education_texts = [span.text for span in hidden_spans]\n",
    "                education = {\n",
    "                    \"school: \": education_texts[0],\n",
    "                }\n",
    "                \n",
    "                if len(education_texts) > 1:\n",
    "                    education[\"degree: \"] = education_texts[1]\n",
    "                if len(education_texts) > 2:\n",
    "                    education[\"dates: \"] = education_texts[2]\n",
    "                if len(education_texts) > 3:\n",
    "                    education[\"remaining: \"] = (\", \").join(education_texts[5:])\n",
    "                scraped_education.append(education)\n",
    "\n",
    "            print(\"Successfully scraped education\")\n",
    "        else:\n",
    "            print(\"No education found\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        print(\"ERROR: No education found\")\n",
    "\n",
    "    return scraped_education\n",
    "\n",
    "\n",
    "# Helper function to scrape degree of connection and mutuals\n",
    "# Returns scraped degree of connection and mutuals if successful, otherwise returns N/A\n",
    "def get_degree_of_connection_and_mutuals(driver):\n",
    "    scraped_profile_dist = \"4+\"\n",
    "    scraped_mutuals = \"N/A\"\n",
    "\n",
    "    try:\n",
    "        scraped_profile_dist = wait_for_element_to_load(driver, By.CSS_SELECTOR, \"span.dist-value\").text\n",
    "        if scraped_profile_dist == \"1st\" or scraped_profile_dist == \"2nd\":\n",
    "            try:\n",
    "                span_element = wait_for_element_to_load(driver, By.CSS_SELECTOR, \"span.t-normal.t-black--light.t-14.hoverable-link-text\")\n",
    "                scraped_mutuals = span_element.text.split('\\n')[0]\n",
    "                print(\"Successfully found mutual connections: \" + scraped_mutuals)\n",
    "            except:\n",
    "                print(\"ERROR: mutuals not found\")\n",
    "        print(\"Successfully scraped degree of connection: \" + scraped_profile_dist)\n",
    "\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        print(\"No degree of connection found\")\n",
    "\n",
    "    return scraped_profile_dist, scraped_mutuals\n",
    "\n",
    "# Helper function to scrape description\n",
    "# Returns scraped description if successful, otherwise returns N/A\n",
    "def get_description(driver):\n",
    "    scraped_description = \"N/A\"\n",
    "    \n",
    "    try:\n",
    "        scraped_description = driver.find_element(By.CLASS_NAME, \"text-body-medium.break-words\").text\n",
    "        print(\"Successfully scraped description: \" + scraped_description)\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        print(\"ERROR: description not found\")\n",
    "        \n",
    "    return scraped_description\n",
    "\n",
    "# Main function to scrape degree of connection and mutuals\n",
    "# Returns scraped degree of connection and mutuals if successful, otherwise returns N/A\n",
    "def scrape_profile(driver, scraped_link):\n",
    "\n",
    "    # Scrape Name\n",
    "    driver.get(scraped_link)\n",
    "    scraped_profile_name = wait_for_element_to_load(driver, By.CSS_SELECTOR, \"h1.text-heading-xlarge.inline.t-24.v-align-middle.break-words\").text\n",
    "    time.sleep(2 + random.random() * 6)\n",
    "\n",
    "    # Scrape Experiences\n",
    "    experiences_url = os.path.join(scraped_link, \"details/experience\")\n",
    "    driver.get(experiences_url)\n",
    "    scraped_experiences = get_experiences(driver)\n",
    "    time.sleep(2 + random.random() * 6)\n",
    "\n",
    "\n",
    "    # Scrape Education\n",
    "    education_url = os.path.join(scraped_link, \"details/education\")\n",
    "    driver.get(education_url)\n",
    "    scraped_education = get_education(driver)\n",
    "    time.sleep(2 + random.random() * 6)\n",
    "\n",
    "\n",
    "    # Scrape degree of connection and mutuals if available\n",
    "    driver.get(scraped_link)\n",
    "    scraped_profile_dist, scraped_mutuals = get_degree_of_connection_and_mutuals(driver)\n",
    "    time.sleep(2 + random.random() * 6)\n",
    "\n",
    "    # Scrape description\n",
    "    scraped_description = get_description(driver)\n",
    "    time.sleep(2 + random.random() * 6)\n",
    "\n",
    "\n",
    "    # Scape profile link\n",
    "    scraped_link = driver.current_url\n",
    "    print(\"Successfully scraped profile link: \" + scraped_link)\n",
    "    time.sleep(2 + random.random() * 6)\n",
    "\n",
    "    \n",
    "    profile = ScrapedProfile(scraped_profile_name,\n",
    "                   scraped_experiences,\n",
    "                   scraped_education,\n",
    "                   scraped_profile_dist,\n",
    "                   scraped_mutuals,\n",
    "                   scraped_description,\n",
    "                   scraped_link)\n",
    "    print(\"\\nSuccess!\")\n",
    "    print(f\"Name: {profile.profile_name}\")\n",
    "    print(f\"Experiences: {profile.experiences}\")\n",
    "    print(f\"Education: {profile.education}\")\n",
    "    print(f\"Profile Distance: {profile.profile_dist}\")\n",
    "    print(f\"Mutuals: {profile.mutuals}\")\n",
    "    print(f\"Description: {profile.profile_description}\")\n",
    "    print(f\"Link: {profile.profile_link}\\n\")\n",
    "    return profile\n",
    "\n",
    "def scrape_all_profiles(driver, historyPickle, toScrapePickle, resultsPickle, failedURLsTextFile):\n",
    "    results = []\n",
    "    scraped_urls = []\n",
    "    history = []\n",
    "\n",
    "    with open(toScrapePickle, 'rb') as f:\n",
    "        try:\n",
    "            to_scrape_urls = pickle.load(f)\n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "            print(\"Error loading to scrape url pickle file\")\n",
    "        \n",
    "    totalCount = len(to_scrape_urls)\n",
    "\n",
    "    print(f'# of profiles to scrape: {totalCount}')\n",
    "    \n",
    "\n",
    "    for i in range(len(to_scrape_urls) - 1, -1, -1):\n",
    "        url = to_scrape_urls[i]\n",
    "        print(f'At index: {len(to_scrape_urls) - i} - url: {url}')\n",
    "\n",
    "        with open(historyPickle, 'rb') as f:\n",
    "            try:\n",
    "                history = pickle.load(f)\n",
    "            except Exception as e:\n",
    "                print(e)\n",
    "                print(\"Error loading history pickle file\")\n",
    "\n",
    "        try:\n",
    "            profile = scrape_profile(driver, url)\n",
    "            if profile != None:\n",
    "                print(\"saving profile info\")\n",
    "                results.append(profile)\n",
    "\n",
    "                with open(resultsPickle, 'wb') as f:\n",
    "                    pickle.dump(results, f)\n",
    "                \n",
    "                print(\"adding to history\")\n",
    "                history.append(url)\n",
    "                with open(historyPickle, 'wb') as f:\n",
    "                    pickle.dump(history, f)\n",
    "                \n",
    "                print(\"recording scraped url\")\n",
    "                scraped_urls.append(url)\n",
    "            else:\n",
    "                print(\"profile filtered out\")\n",
    "            \n",
    "            print(\"removing from to-scrape\")\n",
    "            to_scrape_urls.remove(url)\n",
    "            with open(toScrapePickle, 'wb') as f:\n",
    "                pickle.dump(to_scrape_urls, f)\n",
    "\n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "            print('Failed to scrape profile: ', url)\n",
    "\n",
    "            # TODO: CHANGED FAILED URLS\n",
    "            with open(failedURLsTextFile, 'a') as f:\n",
    "                f.write(url + '\\n')\n",
    "\n",
    "        print(((totalCount - i)/totalCount) * 100, '% Done - at index:', totalCount - i)\n",
    "        print('\\n----------------------------------------------------------------------------------------------------------\\n')\n",
    "    return results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdd18909",
   "metadata": {},
   "source": [
    "# EXPORTING HELPER FUNCTIONS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80407bba",
   "metadata": {},
   "outputs": [],
   "source": [
    "def parseProfile(profile: ScrapedProfile):\n",
    "    res = {}\n",
    "    res['url'] = profile.profile_link\n",
    "    res['name'] = profile.profile_name\n",
    "    res['dist'] = profile.profile_dist\n",
    "    res['description'] = profile.profile_description\n",
    "\n",
    "    #Experiences first\n",
    "    print(profile.experiences)\n",
    "    for i, e in enumerate(profile.experiences):\n",
    "        res[f'exp{i} title'] = e[\"title: \"]\n",
    "        res[f'exp{i} company'] = e[\"company: \"]\n",
    "        res[f'exp{i} dates'] = e[\"dates: \"]\n",
    "\n",
    "    # School second\n",
    "    for i, e in enumerate(profile.profile_school):\n",
    "        res[f'edu{i} school'] = e[\"school: \"]\n",
    "        if \"degree :\" in e:\n",
    "            res[f'edu{i} degree'] = e[\"degree: \"]\n",
    "\n",
    "    \n",
    "    return res"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "095b1a61-bfdb-4ec3-b5f5-8bc81d74a68d",
   "metadata": {},
   "source": [
    "# QUERY 1: MAIN COMPANIES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aca7a719",
   "metadata": {},
   "outputs": [],
   "source": [
    "historyPickle = 'db/main_history.pickle'\n",
    "toScrapePickle = 'db/main_to_scrape.pickle'\n",
    "resultsPickle = 'db/main_results.pickle'\n",
    "failedURLsTextFile = 'db/main_failed_urls.txt'\n",
    "\n",
    "SALES_NAV_SEARCH_URL = \"\"\"\n",
    "https://www.linkedin.com/sales/search/people?savedSearchId=1833971580&sessionId=jFoOWCqiTl2JVJ3QPZrL4g%3D%3D\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93127f7e",
   "metadata": {},
   "source": [
    "### Sales nav scraper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e0ea7d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "already_scraped_urls, to_scrape_urls = check_pickles(historyPickle, toScrapePickle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e83345e3-d166-4e30-a7d6-2634b0db34a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    updated_to_scrape_urls = scrape_profiles(driver, SALES_NAV_SEARCH_URL, already_scraped_urls, to_scrape_urls, toScrapePickle)\n",
    "    print(updated_to_scrape_urls)\n",
    "except Exception as e:\n",
    "    print(f\"An unexpected error occurred: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b1c8979-0304-4621-ae96-59108d44bada",
   "metadata": {},
   "source": [
    "### Deduplicate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ea0fe49",
   "metadata": {},
   "outputs": [],
   "source": [
    "deduplicate(toScrapePickle, to_scrape_urls, already_scraped_urls)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6da7736-3078-4e03-8be6-967f48640cdc",
   "metadata": {},
   "source": [
    "### Profile scraper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7187f14e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "results = scrape_all_profiles(driver, historyPickle, toScrapePickle, resultsPickle, failedURLsTextFile)\n",
    "print(len(results))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0df469c1",
   "metadata": {},
   "source": [
    "# QUERY 2: UNICORN COMPANIES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df63090d",
   "metadata": {},
   "outputs": [],
   "source": [
    "historyPickle = 'db/unicorn_history.pickle'\n",
    "toScrapePickle = 'db/unicorn_to_scrape.pickle'\n",
    "resultsPickle = 'db/unicorn_results.pickle'\n",
    "failedURLsTextFile = 'db/unicorn_failed_urls.txt'\n",
    "\n",
    "SALES_NAV_SEARCH_URL = \"\"\"\n",
    "https://www.linkedin.com/sales/search/people?savedSearchId=1833971588&sessionId=zVKTgf2GQk2owfFVzL9aFQ%3D%3D\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1a8a32b",
   "metadata": {},
   "source": [
    "### Sales nav scraper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc90b784",
   "metadata": {},
   "outputs": [],
   "source": [
    "already_scraped_urls, to_scrape_urls = check_pickles(historyPickle, toScrapePickle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82fb8467",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    updated_to_scrape_urls = scrape_profiles(driver, SALES_NAV_SEARCH_URL, already_scraped_urls, to_scrape_urls, toScrapePickle)\n",
    "    print(updated_to_scrape_urls)\n",
    "except Exception as e:\n",
    "    print(f\"An unexpected error occurred: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98ff5ea6",
   "metadata": {},
   "source": [
    "### Deduplicate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96c9a4c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "deduplicate(toScrapePickle, to_scrape_urls, already_scraped_urls)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29a8309e",
   "metadata": {},
   "source": [
    "### Profile scraper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77b3e90f",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = scrape_all_profiles(driver, historyPickle, toScrapePickle, resultsPickle, failedURLsTextFile)\n",
    "print(len(results))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d71f2e26",
   "metadata": {},
   "source": [
    "# QUERY 3: ACQUIRED COMPANIES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22414fd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "historyPickle = 'db/acquired_history.pickle'\n",
    "toScrapePickle = 'db/acquired_to_scrape.pickle'\n",
    "resultsPickle = 'db/acquired_results.pickle'\n",
    "failedURLsTextFile = 'db/acquired_failed_urls.txt'\n",
    "\n",
    "SALES_NAV_SEARCH_URL = \"\"\"\n",
    "https://www.linkedin.com/sales/search/people?savedSearchId=1833971612&sessionId=R7vv8AiYRXiqMseY1m3CwQ%3D%3D\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c88db61",
   "metadata": {},
   "source": [
    "### Sales nav scraper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "643faf85",
   "metadata": {},
   "outputs": [],
   "source": [
    "already_scraped_urls, to_scrape_urls = check_pickles(historyPickle, toScrapePickle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2ee22e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    updated_to_scrape_urls = scrape_profiles(driver, SALES_NAV_SEARCH_URL, already_scraped_urls, to_scrape_urls, toScrapePickle)\n",
    "    print(updated_to_scrape_urls)\n",
    "except Exception as e:\n",
    "    print(f\"An unexpected error occurred: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8eb29f7",
   "metadata": {},
   "source": [
    "### Deduplicate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d324c97",
   "metadata": {},
   "outputs": [],
   "source": [
    "deduplicate(toScrapePickle, to_scrape_urls, already_scraped_urls)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e12ff51",
   "metadata": {},
   "source": [
    "### Profile scraper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c357cf25",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = scrape_all_profiles(driver, historyPickle, toScrapePickle, resultsPickle, failedURLsTextFile)\n",
    "print(len(results))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b637904",
   "metadata": {},
   "source": [
    "# QUERY 4: VC PORTFOLIO COMPANIES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4efcd7ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "historyPickle = 'db/portfolio_history.pickle'\n",
    "toScrapePickle = 'db/portfolio_to_scrape.pickle'\n",
    "resultsPickle = 'db/portfolio_results.pickle'\n",
    "failedURLsTextFile = 'db/portfolio_failed_urls.txt'\n",
    "\n",
    "SALES_NAV_SEARCH_URL = \"\"\"\n",
    "https://www.linkedin.com/sales/search/people?savedSearchId=1833971604&sessionId=zVKTgf2GQk2owfFVzL9aFQ%3D%3D\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "457c988d",
   "metadata": {},
   "source": [
    "### Sales nav scraper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e3747a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "already_scraped_urls, to_scrape_urls = check_pickles(historyPickle, toScrapePickle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a411299f",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    updated_to_scrape_urls = scrape_profiles(driver, SALES_NAV_SEARCH_URL, already_scraped_urls, to_scrape_urls, toScrapePickle)\n",
    "    print(updated_to_scrape_urls)\n",
    "except Exception as e:\n",
    "    print(f\"An unexpected error occurred: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c4905aa",
   "metadata": {},
   "source": [
    "### Deduplicate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "baf6a7b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "deduplicate(toScrapePickle, to_scrape_urls, already_scraped_urls)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5281ff8",
   "metadata": {},
   "source": [
    "### Profile scraper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5ec701f",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = scrape_all_profiles(driver, historyPickle, toScrapePickle, resultsPickle, failedURLsTextFile)\n",
    "print(len(results))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47d73a6f",
   "metadata": {},
   "source": [
    "# QUERY 5: INFRA COMPANIES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa93ce00",
   "metadata": {},
   "outputs": [],
   "source": [
    "historyPickle = 'db/infra_history.pickle'\n",
    "toScrapePickle = 'db/infra_to_scrape.pickle'\n",
    "resultsPickle = 'db/infra_results.pickle'\n",
    "failedURLsTextFile = 'db/infra_failed_urls.txt'\n",
    "\n",
    "SALES_NAV_SEARCH_URL = \"\"\"\n",
    "https://www.linkedin.com/sales/search/people?savedSearchId=1833971620&sessionId=R7vv8AiYRXiqMseY1m3CwQ%3D%3D\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "924aae38",
   "metadata": {},
   "source": [
    "### Sales nav scraper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7961266",
   "metadata": {},
   "outputs": [],
   "source": [
    "already_scraped_urls, to_scrape_urls = check_pickles(historyPickle, toScrapePickle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2f9a6f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    updated_to_scrape_urls = scrape_profiles(driver, SALES_NAV_SEARCH_URL, already_scraped_urls, to_scrape_urls, toScrapePickle)\n",
    "    print(updated_to_scrape_urls)\n",
    "except Exception as e:\n",
    "    print(f\"An unexpected error occurred: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eec8b4b3",
   "metadata": {},
   "source": [
    "### Deduplicate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2df023a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "deduplicate(toScrapePickle, to_scrape_urls, already_scraped_urls)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f55d6e65",
   "metadata": {},
   "source": [
    "### Profile scraper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dffc900a",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = scrape_all_profiles(driver, historyPickle, toScrapePickle, resultsPickle, failedURLsTextFile)\n",
    "print(len(results))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47185763-6f2b-4f89-9acc-1651887008f6",
   "metadata": {},
   "source": [
    "# EXPORTING RESULTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a13c33b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# NEW GOOGLE SHEETS INTEGRATION\n",
    "\n",
    "def create_google_sheet(list_of_results, sheet_names):\n",
    "    # Set up the credentials\n",
    "    scope = ['https://spreadsheets.google.com/feeds', 'https://www.googleapis.com/auth/drive']\n",
    "    creds = ServiceAccountCredentials.from_json_keyfile_name('path/to/your/credentials.json', scope)\n",
    "    client = gspread.authorize(creds)\n",
    "\n",
    "    # Create a new Google Sheet\n",
    "    sheet = client.create('LinkedIn Scraped Profiles')\n",
    "    \n",
    "    for results, sheet_name in zip(list_of_results, sheet_names):\n",
    "        # Create a new worksheet\n",
    "        worksheet = sheet.add_worksheet(title=sheet_name, rows=\"1000\", cols=\"20\")\n",
    "\n",
    "        # Parse candidates and prepare rows\n",
    "        rows = []\n",
    "        for candidate in results:\n",
    "            row = parseCandidate(candidate)\n",
    "            rows.append(row)\n",
    "\n",
    "        if not rows:\n",
    "            print(f\"No data for sheet: {sheet_name}\")\n",
    "            continue\n",
    "\n",
    "        # Get column titles\n",
    "        col_titles = list(rows[0].keys())\n",
    "\n",
    "        # Prepare the data for batch update\n",
    "        cells = [col_titles]  # Start with the header row\n",
    "        for row in rows:\n",
    "            cells.append([row.get(col, '') for col in col_titles])\n",
    "\n",
    "        # Update the sheet in batch\n",
    "        cell_range = f'A1:{gspread.utils.rowcol_to_a1(len(cells), len(col_titles))}'\n",
    "        worksheet.update(cell_range, cells)\n",
    "\n",
    "        print(f\"Successfully added data to sheet: {sheet_name}\")\n",
    "\n",
    "    # Delete the default \"Sheet1\" if it exists\n",
    "    try:\n",
    "        sheet.del_worksheet(sheet.worksheet(\"Sheet1\"))\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "    print(f\"Google Sheet created successfully. URL: {sheet.url}\")\n",
    "    return sheet.url\n",
    "\n",
    "# Usage\n",
    "list_of_results = [results1, results2, results3]  # Your lists of ScrapedProfile objects\n",
    "sheet_names = [\"Results1\", \"Results2\", \"Results3\"]  # Names for each sheet\n",
    "sheet_url = create_google_sheet(list_of_results, sheet_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0906d83",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create dataframe\n",
    "rows = []\n",
    "for candidate in results:\n",
    "    row = parseCandidate(candidate)\n",
    "    rows.append(row)\n",
    "\n",
    "# Write to CSV\n",
    "col_titles = rows[0].keys()\n",
    "\n",
    "try:\n",
    "    with open('candidates.csv', 'w', newline='') as output_file:\n",
    "        dict_writer = csv.DictWriter(output_file, col_titles)\n",
    "        dict_writer.writeheader()\n",
    "        dict_writer.writerows(rows)\n",
    "        print(\"Successfully exported to csv\")\n",
    "except Exception as e:\n",
    "    print(f\"Failed to export to csv. Error: {e}\")\n",
    "\n",
    "#Export to Excel\n",
    "try:\n",
    "    df.to_excel('candidates.xlsx', index=False)\n",
    "    print(\"Exported to Excel\")\n",
    "except:\n",
    "    print(\"Failed to export to Excel\")\n",
    "\n",
    "    \n",
    "# update db/already_scraped.pickle\n",
    "with open('db/already_scraped.pickle', 'rb') as f:\n",
    "    already_scraped = pickle.load(f)\n",
    "    print(f\"Previously scraped: {len(already_scraped)}\")\n",
    "    already_scraped = already_scraped + already_scraped_urls\n",
    "    already_scraped = list(set(already_scraped))\n",
    "    print(f\"Newly scraped: {len(already_scraped)}\")\n",
    "with open('db/already_scraped.pickle', 'wb') as f:\n",
    "    pickle.dump(already_scraped, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97efd509",
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.auth.transport.requests import Request\n",
    "from google.oauth2.credentials import Credentials\n",
    "from google_auth_oauthlib.flow import InstalledAppFlow\n",
    "from googleapiclient.discovery import build\n",
    "from googleapiclient.http import MediaFileUpload\n",
    "\n",
    "# If modifying these SCOPES, delete the file token.pickle.\n",
    "SCOPES = ['https://www.googleapis.com/auth/drive.file']\n",
    "\n",
    "def authenticate():\n",
    "    creds = None\n",
    "    # The file token.pickle stores the user's access and refresh tokens, and is created automatically when the authorization flow completes for the first time.\n",
    "    if os.path.exists('token.pickle'):\n",
    "        with open('token.pickle', 'rb') as token:\n",
    "            creds = pickle.load(token)\n",
    "    # If there are no (valid) credentials available, let the user log in.\n",
    "    if not creds or not creds.valid:\n",
    "        if creds and creds.expired and creds.refresh_token:\n",
    "            creds.refresh(Request())\n",
    "        else:\n",
    "            flow = InstalledAppFlow.from_client_secrets_file(\n",
    "                'client_secrets.json', SCOPES)\n",
    "            creds = flow.run_local_server(port=0)\n",
    "        # Save the credentials for the next run\n",
    "        with open('token.pickle', 'wb') as token:\n",
    "            pickle.dump(creds, token)\n",
    "    return creds\n",
    "\n",
    "def upload_file_to_drive(file_path, file_name, mime_type):\n",
    "    creds = authenticate()\n",
    "    service = build('drive', 'v3', credentials=creds)\n",
    "\n",
    "    file_metadata = {'name': file_name}\n",
    "    media = MediaFileUpload(file_path, mimetype=mime_type)\n",
    "\n",
    "    file = service.files().create(body=file_metadata, media_body=media, fields='id').execute()\n",
    "    print('File ID: %s' % file.get('id'))\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    file_path = 'path_to_your_local_excel_file.xlsx'  # Replace with the path to your local file\n",
    "    file_name = 'your_excel_file.xlsx'  # Replace with the desired name for the file in Google Drive\n",
    "    mime_type = 'application/vnd.openxmlformats-officedocument.spreadsheetml.sheet'\n",
    "\n",
    "    upload_file_to_drive(file_path, file_name, mime_type)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ee20a9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# categories = COMPANY_CATEGORIES\n",
    "\n",
    "# # Example color mapping for categories\n",
    "# category_colors = {\n",
    "#     \"SECURITY\": 'red',\n",
    "#     \"OTHER\": 'blue',\n",
    "#     \"PUBLIC\": 'green',\n",
    "#     \"INFRA\": 'yellow',\n",
    "#     \"FINTECH\": 'orange',\n",
    "#     \"CRYPTO\": 'purple',\n",
    "#     \"FRONTIER\": 'cyan',\n",
    "#     \"AI\": 'magenta'\n",
    "# }\n",
    "\n",
    "# # Create a reverse dictionary for easier lookup: {company: category}\n",
    "# company_category = {}\n",
    "# for category, companies in categories.items():\n",
    "#     for company in companies:\n",
    "#         company_category[company] = category\n",
    "\n",
    "# # Modify the style function\n",
    "# def highlight_by_category(val):\n",
    "#     category = company_category.get(val)\n",
    "#     if category:\n",
    "#         color = category_colors.get(category, 'none')  # default to 'none' if no color is specified\n",
    "#     else:\n",
    "#         color = 'none'\n",
    "#     return f'background-color: {color}'\n",
    "\n",
    "# import re\n",
    "# ILLEGAL_CHARACTERS_RE = re.compile(r'[\\000-\\010]|[\\013-\\014]|[\\016-\\037]')\n",
    "# def find_illegal_characters(df):\n",
    "#     for column in df.columns:\n",
    "#         for idx, item in enumerate(df[column]):\n",
    "#             if isinstance(item, str) and ILLEGAL_CHARACTERS_RE.search(item):\n",
    "#                 # replace illegal characters with an empty string\n",
    "#                 df[column][idx] = ILLEGAL_CHARACTERS_RE.sub('', item)\n",
    "#     return df\n",
    "\n",
    "# styled_df = df\n",
    "# styled_df = find_illegal_characters(styled_df)\n",
    "# styled_df = df.style.applymap(highlight_by_category)\n",
    "\n",
    "# # Save the styled DataFrame to an Excel file\n",
    "# # get today's date in MM-DD-YYYY format\n",
    "# import datetime\n",
    "# now = datetime.datetime.now()\n",
    "# date = now.strftime(\"%m-%d-%Y\")\n",
    "# styled_df.to_excel(f'results/{date}_{start}-{end}.xlsx', engine='openpyxl', index=False)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
