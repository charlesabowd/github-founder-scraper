{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5b930f8e",
   "metadata": {},
   "source": [
    "#  SETUP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b9e6fd8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# if the script finishes very quickly (and generates an empty excel file), click run again\n",
    "# if the script errors on the \"Login Cell\" (added a comment to indicate which cell that is below), set IS_HEADLESS to \"False\" and run again. The scraper will automatically launch a page and attempt to login to LinkedIn. It's likely erroring because LinkedIn is asking for a captcha to verify the user is not a bot. Solve the captch/challenge and login. Once successfully logged in, set IS_HEADLESS back to \"True\" and run again.\n",
    "\n",
    "import pickle\n",
    "import csv\n",
    "import os\n",
    "import random\n",
    "import time\n",
    "from linkedin_scraper import actions\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.firefox.options import Options as FirefoxOptions\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.common.exceptions import TimeoutException, StaleElementReferenceException, NoSuchElementException, ElementClickInterceptedException\n",
    "from dotenv import load_dotenv\n",
    "import pandas as pd\n",
    "# for google docs upload\n",
    "import gspread\n",
    "from oauth2client.service_account import ServiceAccountCredentials\n",
    "from io import StringIO\n",
    "\n",
    "\n",
    "IS_HEADLESS = False\n",
    "\n",
    "# Set environment variables to email and password\n",
    "load_dotenv()\n",
    "LINKEDIN_EMAIL = os.environ.get('LINKEDIN_EMAIL')\n",
    "LINKEDIN_PASSWORD = os.environ.get('LINKEDIN_PASSWORD')\n",
    "# Check if the environment variables are set\n",
    "if not LINKEDIN_EMAIL or not LINKEDIN_PASSWORD:\n",
    "    raise ValueError(\"LinkedIn credentials not set in environment variables\")\n",
    "\n",
    "\n",
    "COMPANY_CATEGORIES = {\n",
    "    \"SECURITY\": [\n",
    "        \"Okta\",\n",
    "        \"Snyk\",\n",
    "        \"R2C/Semgrep\", \n",
    "        \"Wiz\",\n",
    "        \"Lacework\",\n",
    "        \"Crowdstrike\",\n",
    "        \"Palo Alto Networks\",\n",
    "        \"Island\",\n",
    "        \"Vanta\",\n",
    "        \"Material Security\",\n",
    "        \"Abnormal Security\",\n",
    "        \"Samsara\",\n",
    "    ],\n",
    "    \"OTHER\": [\n",
    "        \"Figma\",\n",
    "        \"Airtable\",\n",
    "        \"Notion\",\n",
    "        \"Canva\",\n",
    "        \"Webflow\",\n",
    "        \"Faire\",\n",
    "        \"Deel\",\n",
    "        \"Rippling\",\n",
    "        \"Flexport\",\n",
    "        \"Benchling\",\n",
    "        \"Solugen\"\n",
    "    ],\n",
    "    \"PUBLIC\": [\n",
    "        \"Doordash\",\n",
    "        \"Uber\",\n",
    "        \"Palantir\",\n",
    "        \"Airbnb\",\n",
    "        \"Instacart\"\n",
    "    ],\n",
    "    \"INFRA\": [\n",
    "        \"Fivetran\",\n",
    "        \"DBT\",\n",
    "        \"Temporal\",\n",
    "        \"Cockroach Labs\",\n",
    "        \"Grafana\",\n",
    "        \"Zapier\",\n",
    "        \"Starburst\",\n",
    "        \"Retool\",\n",
    "        \"Sentry\",\n",
    "        \"Sourcegraph\",\n",
    "        \"Cribl\",\n",
    "        \"Vercel\",\n",
    "        \"Clickhouse\",\n",
    "        \"Github,\"\n",
    "        \"Cisco Meraki\",\n",
    "    ],\n",
    "    \"FINTECH\": [\n",
    "        \"Robinhood\",\n",
    "        \"Square\",\n",
    "        \"Stripe\",\n",
    "        \"Ramp\",\n",
    "        \"Brex\",\n",
    "        \"Plaid\",\n",
    "        \"Modern Treasury\",\n",
    "        \"Mercury\",\n",
    "        \"Persona\",\n",
    "        \"Klarna\",\n",
    "        \"Nubank\"\n",
    "    ],\n",
    "    \"CRYPTO\": [\n",
    "        \"Coinbase\",\n",
    "        \"Uniswap\",\n",
    "        \"Chainalysis\",\n",
    "        \"Arbitrum\",\n",
    "        \"TRM\",\n",
    "        \"Fireblocks\",\n",
    "        \"Eigenlayer\"\n",
    "    ],\n",
    "    \"FRONTIER\": [\n",
    "        \"Anduril\",\n",
    "        \"SpaceX\",\n",
    "        \"Zipline\",\n",
    "        \"Varda\",\n",
    "        \"Hadrian\"\n",
    "    ],\n",
    "    \"AI\": [\n",
    "        \"Bytedance\",\n",
    "        \"Scale AI\",\n",
    "        \"Anthropic\",\n",
    "        \"Robust intelligence\",\n",
    "        \"OpenAI\",\n",
    "        \"Predibase\",\n",
    "        \"Cohere\",\n",
    "        \"Databricks\",\n",
    "        \"Hugging Face\",\n",
    "        \"RunwayML\",\n",
    "        \"Tecton\",\n",
    "        \"Weights & Biases\",\n",
    "        \"Kumo AI\",\n",
    "        \"NVIDIA\",\n",
    "        \"Adept\",\n",
    "        \"Glean\",\n",
    "        \"Character.ai\",\n",
    "        \"Midjourney\",\n",
    "        \"Facebook AI\",\n",
    "        \"FAIR\",\n",
    "        \"Google brain\"\n",
    "    ]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "adbde830",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Library/Developer/CommandLineTools/usr/bin/python3\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "print(sys.executable)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7957b23f",
   "metadata": {},
   "source": [
    "# LOGIN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3596a5f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error logging in. Please complete the captcha challenge and login manually.\n",
      "Message: \n",
      "Stacktrace:\n",
      "RemoteError@chrome://remote/content/shared/RemoteError.sys.mjs:8:8\n",
      "WebDriverError@chrome://remote/content/shared/webdriver/Errors.sys.mjs:193:5\n",
      "NoSuchElementError@chrome://remote/content/shared/webdriver/Errors.sys.mjs:511:5\n",
      "dom.find/</<@chrome://remote/content/shared/DOM.sys.mjs:136:16\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# IF THIS CELL ERRORS DUE TO CAPTCHA, do this:\n",
    "# manually complete the captcha, click on the next cell, and select Run menu, select \"run selected cell and all below\" \n",
    "\n",
    "def instantiate_driver():\n",
    "    options = FirefoxOptions()\n",
    "    if IS_HEADLESS:\n",
    "        options.add_argument(\"--headless\")\n",
    "    driver = webdriver.Firefox(options=options)\n",
    "    \n",
    "    try:\n",
    "        actions.login(driver, LINKEDIN_EMAIL, LINKEDIN_PASSWORD)\n",
    "    except Exception as e:\n",
    "        print(\"Error logging in. Please complete the captcha challenge and login manually.\")\n",
    "        print(e)\n",
    "\n",
    "    time.sleep(15)\n",
    "    return driver\n",
    "    \n",
    "driver = instantiate_driver()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "515b071f",
   "metadata": {},
   "source": [
    "# SALES NAV HELPER FUNCTIONS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "027fd1fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ScrapedProfile:\n",
    "    def __init__(self, profile_name, experiences, education, profile_dist, mutuals, profile_description, profile_link):\n",
    "        self.profile_name = profile_name\n",
    "        self.experiences = experiences\n",
    "        self.education = education\n",
    "        self.profile_dist = profile_dist\n",
    "        self.mutuals = mutuals\n",
    "        self.profile_description = profile_description\n",
    "        self.profile_link = profile_link\n",
    "\n",
    "def check_pickles(historyPickle, toScrapePickle):\n",
    "    scraped_urls = []\n",
    "    with open(historyPickle, 'rb') as f:\n",
    "        try:\n",
    "            scraped_urls = pickle.load(f)\n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "            print(\"Error loading history pickle file\")\n",
    "        print(len(scraped_urls))\n",
    "\n",
    "    to_scrape_urls = []\n",
    "    with open(toScrapePickle, 'rb') as f:\n",
    "        try:\n",
    "            to_scrape_urls = pickle.load(f)\n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "            print(\"Error loading to scrape url pickle file\")\n",
    "        print(len(to_scrape_urls))\n",
    "\n",
    "    return scraped_urls, to_scrape_urls\n",
    "\n",
    "def deduplicate(toScrapePickle, to_scrape_urls, already_scraped_urls):\n",
    "    to_scrape_urls = list(set(to_scrape_urls))\n",
    "    originalCount = len(to_scrape_urls)\n",
    "    to_scrape_urls = [url for url in to_scrape_urls if url not in already_scraped_urls]\n",
    "    print(f\"Removed {originalCount - len(to_scrape_urls)} duplicates\")\n",
    "\n",
    "    with open(toScrapePickle, 'wb') as to_scrape_file:\n",
    "        pickle.dump(to_scrape_urls, to_scrape_file)\n",
    "    \n",
    "    print(len(to_scrape_urls))\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Wait for an element to be present on the page and return it.\n",
    "\n",
    "Parameters:\n",
    "- driver: The WebDriver instance\n",
    "- by: The method to locate the element (default: By.CLASS_NAME)\n",
    "- name: The name or identifier of the element to wait for\n",
    "- base: The base element to search from (default: None, which uses the driver)\n",
    "- timeout: Maximum time to wait for the element (default: 180 seconds)\n",
    "\n",
    "Returns:\n",
    "- The WebElement if found\n",
    "- None if the element is not found within the timeout period\n",
    "\"\"\"\n",
    "def wait_for_element_to_load(driver, by=By.CLASS_NAME, name=\"pv-top-card\", base=None, timeout=100):\n",
    "    base = base or driver\n",
    "    try:\n",
    "        element = WebDriverWait(base, timeout).until(\n",
    "            EC.presence_of_element_located((by, name))\n",
    "        )\n",
    "        return element\n",
    "    except TimeoutException:\n",
    "        print(f\"Timed out waiting for element: {by}={name}\")\n",
    "        return None\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred while waiting for element {by}={name}: {str(e)}\")\n",
    "        return None\n",
    "\n",
    "# Scrolls the page to bring the specified profile element into view\n",
    "# Returns True if successful, False if an error occurs\n",
    "def scroll_to_profile(driver, profile):\n",
    "    try:\n",
    "        driver.execute_script(\"arguments[0].scrollIntoView();\", profile)\n",
    "        return True\n",
    "    except Exception as e:\n",
    "        print(f\"Error scrolling to profile: {e}\")\n",
    "        return False\n",
    "\n",
    "# Clicks on the profile element to open its details\n",
    "# Returns True if successful, False if an error occurs\n",
    "def click_profile(profile):\n",
    "    try:\n",
    "        salesNavOpenProfileButton = profile.find_element(By.CLASS_NAME, \"artdeco-entity-lockup__title\")\n",
    "        salesNavOpenProfileButton.click()\n",
    "        return True\n",
    "    except (NoSuchElementException, ElementClickInterceptedException) as e:\n",
    "        print(f\"Error clicking profile: {e}\")\n",
    "        return False\n",
    "\n",
    "# Clicks the three dots button to open the dropdown menu\n",
    "# Returns True if successful, False if an error occurs\n",
    "def click_three_dots_button(driver):\n",
    "    try:\n",
    "        actionContainer = driver.find_element(By.CLASS_NAME, \"_actions-container_1dg5u8\")\n",
    "        threeDotsButton = actionContainer.find_element(By.CLASS_NAME, \"_icon_ps32ck\")\n",
    "        threeDotsButton.click()\n",
    "        return True\n",
    "    except (NoSuchElementException, ElementClickInterceptedException) as e:\n",
    "        print(f\"Error clicking three dots button\")\n",
    "        return False\n",
    "\n",
    "# Retrieves the LinkedIn URL from the dropdown menu\n",
    "# Returns the URL if successful, None if an error occurs\n",
    "def get_linkedin_url(driver):\n",
    "    try:\n",
    "        dropdownContainer = driver.find_element(By.CLASS_NAME, \"_visible_x5gf48\")\n",
    "        normalLinkedInUrl = dropdownContainer.find_elements(By.TAG_NAME, \"a\")[1].get_attribute(\"href\")\n",
    "        return normalLinkedInUrl\n",
    "    except (NoSuchElementException, IndexError) as e:\n",
    "        print(f\"Error getting LinkedIn URL: {e}\")\n",
    "        return None\n",
    "\n",
    "# Closes the profile popout\n",
    "# Returns True if successful, False if an error occurs\n",
    "def close_popout(driver):\n",
    "    try:\n",
    "        header = driver.find_element(By.CLASS_NAME, \"_inline-sidesheet-header-actions_1cn7lg\")\n",
    "        button = header.find_elements(By.CLASS_NAME, \"_button_ps32ck\")[1]\n",
    "        button.click()\n",
    "        return True\n",
    "    except (NoSuchElementException, ElementClickInterceptedException, IndexError) as e:\n",
    "        print(f\"Error closing popout: {e}\")\n",
    "        return False\n",
    "\n",
    "# Navigates to the next page of search results\n",
    "# Returns True if successful, False if there are no more pages or an error occurs\n",
    "def navigate_to_next_page(driver):\n",
    "    try:\n",
    "        nextPageButton = driver.find_element(By.CLASS_NAME, \"artdeco-pagination__button--next\")\n",
    "        nextPageButton.click()\n",
    "        return True\n",
    "    except (NoSuchElementException, ElementClickInterceptedException) as e:\n",
    "        print(f\"No more pages or error navigating: {e}\")\n",
    "        return False\n",
    "\n",
    "# Main scraping function\n",
    "def scrape_profiles(driver, SALES_NAV_SEARCH_URL, already_scraped_urls, to_scrape_urls, toScrapePickle):\n",
    "    # reinstantiate_driver(driver)\n",
    "    driver.get(SALES_NAV_SEARCH_URL)\n",
    "    \n",
    "    while True:\n",
    "        wait_for_element_to_load(driver, By.ID, \"search-results-container\")\n",
    "        profiles = driver.find_elements(By.CLASS_NAME, \"artdeco-list__item\")\n",
    "        \n",
    "        for profile in profiles:\n",
    "            if not scroll_to_profile(driver, profile):\n",
    "                continue\n",
    "\n",
    "            # time.sleep(2 + random.random() * 6)\n",
    "\n",
    "\n",
    "            if not wait_for_element_to_load(driver, By.CLASS_NAME, \"artdeco-entity-lockup__title\"):\n",
    "                continue\n",
    "\n",
    "            if not click_profile(profile):\n",
    "                continue\n",
    "            \n",
    "            # time.sleep(2 + random.random() * 6)\n",
    "\n",
    "            if not wait_for_element_to_load(driver, By.CLASS_NAME, \"_actions-container_1dg5u8\"):\n",
    "                continue\n",
    "\n",
    "            if not click_three_dots_button(driver):\n",
    "                continue\n",
    "\n",
    "            # time.sleep(2 + random.random() * 6)\n",
    "\n",
    "\n",
    "            if not wait_for_element_to_load(driver, By.CLASS_NAME, \"_visible_x5gf48\"):\n",
    "                continue\n",
    "\n",
    "            normalLinkedInUrl = get_linkedin_url(driver)\n",
    "            if normalLinkedInUrl:\n",
    "                if normalLinkedInUrl in already_scraped_urls:\n",
    "                    print(\"Skipping (already scraped): \" + normalLinkedInUrl)\n",
    "                else:\n",
    "                    to_scrape_urls.append(normalLinkedInUrl)\n",
    "                    with open(toScrapePickle, 'wb') as f:\n",
    "                        pickle.dump(to_scrape_urls, f)\n",
    "                    print(\"Successfully scraped: \" + normalLinkedInUrl)\n",
    "\n",
    "            if not close_popout(driver):\n",
    "                continue\n",
    "\n",
    "            # time.sleep(2 + random.random() * 6)\n",
    "\n",
    "\n",
    "        next_button = wait_for_element_to_load(driver, By.CLASS_NAME, \"artdeco-pagination__button--next\")\n",
    "        if not next_button or not next_button.is_enabled():\n",
    "            break\n",
    "\n",
    "        next_button.click()\n",
    "\n",
    "        if not wait_for_element_to_load(driver, By.CLASS_NAME, \"artdeco-list__item\"):\n",
    "            break\n",
    "\n",
    "    return to_scrape_urls"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d9c83c0",
   "metadata": {},
   "source": [
    "# PROFILE SCRAPING HELPER FUNCTIONS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ce900eb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter():\n",
    "    pass     \n",
    "    # try:\n",
    "        #     WebDriverWait(driver, 240).until(lambda d: d.execute_script('return document.readyState') == 'complete')\n",
    "        # except:\n",
    "        #     driver = reinstantiate_driver()\n",
    "\n",
    "        # # FILTERING\n",
    "        \n",
    "        # likely_founder = True\n",
    "        # relevant_exp = True\n",
    "\n",
    "        # cur_exp = experiences[0]\n",
    "        # relevant_companies = [\"stealth\", \"new\"]\n",
    "        # if any(company in cur_exp.institution_name.split(\" ·\")[0].lower() for company in relevant_companies) or \"present\" not in cur_exp.to_date.lower():\n",
    "        #     likely_founder = True\n",
    "\n",
    "        # relevant_titles = [\"product\", \"engineer\", \"sales\", \"business development\", \"founder\", \"head\", \"lead\", \"senior\", \"staff\", \"chief\", \"growth\"]\n",
    "        # for experience in experiences[1:5]:\n",
    "        #     if any(title in experience.position_title.lower() for title in relevant_titles):\n",
    "        #         relevant_exp = True\n",
    "        #         break\n",
    "        # relevant_exp = True\n",
    "\n",
    "        # if not (likely_founder and relevant_exp):\n",
    "        #     print(likely_founder, relevant_exp)\n",
    "        #     return None\n",
    "\n",
    "        # person_obj = Person(profile_link, driver = driver, scrape=False, experiences = [None])\n",
    "        # try:\n",
    "        #     WebDriverWait(driver, 240).until(lambda d: d.execute_script('return document.readyState') == 'complete')\n",
    "        # except:\n",
    "        #     driver = reinstantiate_driver(driver)\n",
    "        # time.sleep(2 + random.random() * 7)\n",
    "        \n",
    "# Helper function to scrape experiences\n",
    "# Returns scraped experiences if successful, otherwise returns empty list\n",
    "def get_experiences(driver):\n",
    "    scraped_experiences = []\n",
    "\n",
    "    try:\n",
    "        wait_for_element_to_load(driver, By.CSS_SELECTOR, \"li.pvs-list__paged-list-item.artdeco-list__item.pvs-list__item--line-separated.pvs-list__item--one-column\")\n",
    "        experience_items = driver.find_elements(By.CSS_SELECTOR, \"li.pvs-list__paged-list-item.artdeco-list__item.pvs-list__item--line-separated.pvs-list__item--one-column\")\n",
    "\n",
    "        if len(experience_items) > 0:\n",
    "            for item in experience_items:\n",
    "                hidden_spans = item.find_elements(By.CSS_SELECTOR, \"span.visually-hidden\")\n",
    "                experience_texts = [span.text for span in hidden_spans]\n",
    "                experience = {\n",
    "                    \"title: \": experience_texts[0],\n",
    "                    \"company: \": experience_texts[1],\n",
    "                    \"dates: \": experience_texts[2],\n",
    "                }\n",
    "                \n",
    "                if len(experience_texts) > 3:\n",
    "                    experience[\"location: \"] = experience_texts[3]\n",
    "                if len(experience_texts) > 4:\n",
    "                    experience[\"summary: \"] = experience_texts[4]\n",
    "                if len(experience_texts) > 5:\n",
    "                    experience[\"remaining: \"] = (\", \").join(experience_texts[5:])\n",
    "                scraped_experiences.append(experience)\n",
    "\n",
    "            print(\"Successfully scraped experiences\")\n",
    "        else:\n",
    "            print(\"No experiences found\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        print(\"ERROR: No experiences found\")\n",
    "\n",
    "    return scraped_experiences\n",
    "\n",
    "\n",
    "# Helper function to scrape education\n",
    "# Returns scraped education if successful, otherwise returns empty list\n",
    "def get_education(driver):\n",
    "    scraped_education = []\n",
    "\n",
    "    try:\n",
    "        wait_for_element_to_load(driver, By.CSS_SELECTOR, \"li.pvs-list__paged-list-item.artdeco-list__item.pvs-list__item--line-separated.pvs-list__item--one-column\")\n",
    "        education_items = driver.find_elements(By.CSS_SELECTOR, \"li.pvs-list__paged-list-item.artdeco-list__item.pvs-list__item--line-separated.pvs-list__item--one-column\")\n",
    "\n",
    "        if len(education_items) > 0:\n",
    "            for item in education_items:\n",
    "                hidden_spans = item.find_elements(By.CSS_SELECTOR, \"span.visually-hidden\")\n",
    "                education_texts = [span.text for span in hidden_spans]\n",
    "                education = {\n",
    "                    \"school: \": education_texts[0],\n",
    "                }\n",
    "                \n",
    "                if len(education_texts) > 1:\n",
    "                    education[\"degree: \"] = education_texts[1]\n",
    "                if len(education_texts) > 2:\n",
    "                    education[\"dates: \"] = education_texts[2]\n",
    "                if len(education_texts) > 3:\n",
    "                    education[\"remaining: \"] = (\", \").join(education_texts[5:])\n",
    "                scraped_education.append(education)\n",
    "\n",
    "            print(\"Successfully scraped education\")\n",
    "        else:\n",
    "            print(\"No education found\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        print(\"ERROR: No education found\")\n",
    "\n",
    "    return scraped_education\n",
    "\n",
    "\n",
    "# Helper function to scrape degree of connection and mutuals\n",
    "# Returns scraped degree of connection and mutuals if successful, otherwise returns N/A\n",
    "def get_degree_of_connection_and_mutuals(driver):\n",
    "    scraped_profile_dist = \"4+\"\n",
    "    scraped_mutuals = \"N/A\"\n",
    "\n",
    "    try:\n",
    "        scraped_profile_dist = wait_for_element_to_load(driver, By.CSS_SELECTOR, \"span.dist-value\").text\n",
    "        if scraped_profile_dist == \"1st\" or scraped_profile_dist == \"2nd\":\n",
    "            try:\n",
    "                span_element = wait_for_element_to_load(driver, By.CSS_SELECTOR, \"span.t-normal.t-black--light.t-14.hoverable-link-text\")\n",
    "                scraped_mutuals = span_element.text.split('\\n')[0]\n",
    "                print(\"Successfully found mutual connections: \" + scraped_mutuals)\n",
    "            except:\n",
    "                print(\"ERROR: mutuals not found\")\n",
    "        print(\"Successfully scraped degree of connection: \" + scraped_profile_dist)\n",
    "\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        print(\"No degree of connection found\")\n",
    "\n",
    "    return scraped_profile_dist, scraped_mutuals\n",
    "\n",
    "# Helper function to scrape description\n",
    "# Returns scraped description if successful, otherwise returns N/A\n",
    "def get_description(driver):\n",
    "    scraped_description = \"N/A\"\n",
    "    \n",
    "    try:\n",
    "        scraped_description = driver.find_element(By.CLASS_NAME, \"text-body-medium.break-words\").text\n",
    "        print(\"Successfully scraped description: \" + scraped_description)\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        print(\"ERROR: description not found\")\n",
    "        \n",
    "    return scraped_description\n",
    "\n",
    "# Main function to scrape degree of connection and mutuals\n",
    "# Returns scraped degree of connection and mutuals if successful, otherwise returns N/A\n",
    "def scrape_profile(driver, scraped_link):\n",
    "\n",
    "    # Scrape Name\n",
    "    driver.get(scraped_link)\n",
    "    scraped_profile_name = wait_for_element_to_load(driver, By.CSS_SELECTOR, \"h1.text-heading-xlarge.inline.t-24.v-align-middle.break-words\").text\n",
    "    time.sleep(2 + random.random() * 6)\n",
    "\n",
    "    # Scrape Experiences\n",
    "    experiences_url = os.path.join(scraped_link, \"details/experience\")\n",
    "    driver.get(experiences_url)\n",
    "    scraped_experiences = get_experiences(driver)\n",
    "    time.sleep(2 + random.random() * 6)\n",
    "\n",
    "\n",
    "    # Scrape Education\n",
    "    education_url = os.path.join(scraped_link, \"details/education\")\n",
    "    driver.get(education_url)\n",
    "    scraped_education = get_education(driver)\n",
    "    time.sleep(2 + random.random() * 6)\n",
    "\n",
    "\n",
    "    # Scrape degree of connection and mutuals if available\n",
    "    driver.get(scraped_link)\n",
    "    scraped_profile_dist, scraped_mutuals = get_degree_of_connection_and_mutuals(driver)\n",
    "    time.sleep(2 + random.random() * 6)\n",
    "\n",
    "    # Scrape description\n",
    "    scraped_description = get_description(driver)\n",
    "    time.sleep(2 + random.random() * 6)\n",
    "\n",
    "\n",
    "    # Scape profile link\n",
    "    scraped_link = driver.current_url\n",
    "    print(\"Successfully scraped profile link: \" + scraped_link)\n",
    "    time.sleep(2 + random.random() * 6)\n",
    "\n",
    "    \n",
    "    profile = ScrapedProfile(scraped_profile_name,\n",
    "                   scraped_experiences,\n",
    "                   scraped_education,\n",
    "                   scraped_profile_dist,\n",
    "                   scraped_mutuals,\n",
    "                   scraped_description,\n",
    "                   scraped_link)\n",
    "    print(\"\\nSuccess!\")\n",
    "    print(f\"Name: {profile.profile_name}\")\n",
    "    print(f\"Experiences: {profile.experiences}\")\n",
    "    print(f\"Education: {profile.education}\")\n",
    "    print(f\"Profile Distance: {profile.profile_dist}\")\n",
    "    print(f\"Mutuals: {profile.mutuals}\")\n",
    "    print(f\"Description: {profile.profile_description}\")\n",
    "    print(f\"Link: {profile.profile_link}\\n\")\n",
    "    return profile\n",
    "\n",
    "def scrape_all_profiles(driver, historyPickle, toScrapePickle, resultsPickle):\n",
    "    results = []\n",
    "    scraped_urls = []\n",
    "    history = []\n",
    "\n",
    "    with open(toScrapePickle, 'rb') as f:\n",
    "        try:\n",
    "            to_scrape_urls = pickle.load(f)\n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "            print(\"Error loading to scrape url pickle file\")\n",
    "        \n",
    "    totalCount = len(to_scrape_urls)\n",
    "\n",
    "    print(f'# of profiles to scrape: {totalCount}')\n",
    "    \n",
    "\n",
    "    for i in range(len(to_scrape_urls) - 1, -1, -1):\n",
    "        url = to_scrape_urls[i]\n",
    "        print(f'At index: {len(to_scrape_urls) - i} - url: {url}')\n",
    "\n",
    "        with open(historyPickle, 'rb') as f:\n",
    "            try:\n",
    "                history = pickle.load(f)\n",
    "            except Exception as e:\n",
    "                print(e)\n",
    "                print(\"Error loading history pickle file\")\n",
    "\n",
    "        try:\n",
    "            profile = scrape_profile(driver, url)\n",
    "            if profile != None:\n",
    "                print(\"saving profile info\")\n",
    "                results.append(profile)\n",
    "\n",
    "                with open(resultsPickle, 'wb') as f:\n",
    "                    pickle.dump(results, f)\n",
    "                \n",
    "                print(\"adding to history\")\n",
    "                history.append(url)\n",
    "                with open(historyPickle, 'wb') as f:\n",
    "                    pickle.dump(history, f)\n",
    "                \n",
    "                print(\"recording scraped url\")\n",
    "                scraped_urls.append(url)\n",
    "            else:\n",
    "                print(\"profile filtered out\")\n",
    "            \n",
    "            print(\"removing from to-scrape\")\n",
    "            to_scrape_urls.remove(url)\n",
    "            with open(toScrapePickle, 'wb') as f:\n",
    "                pickle.dump(to_scrape_urls, f)\n",
    "\n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "            print('Failed to scrape profile: ', url)\n",
    "\n",
    "            # TODO: CHANGED FAILED URLS\n",
    "            with open('failed_urls.txt', 'a') as f:\n",
    "                f.write(url + '\\n')\n",
    "\n",
    "        print(((totalCount - i)/totalCount) * 100, '% Done - at index:', totalCount - i)\n",
    "        print('\\n----------------------------------------------------------------------------------------------------------\\n')\n",
    "    return results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "095b1a61-bfdb-4ec3-b5f5-8bc81d74a68d",
   "metadata": {},
   "source": [
    "# QUERY 1: MAIN COMPANIES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "aca7a719",
   "metadata": {},
   "outputs": [],
   "source": [
    "historyPickle = 'db/main_history.pickle'\n",
    "toScrapePickle = 'db/main_to_scrape.pickle'\n",
    "resultsPickle = 'db/main_results.pickle'\n",
    "\n",
    "SALES_NAV_SEARCH_URL = \"\"\"\n",
    "https://www.linkedin.com/sales/search/people?savedSearchId=1833971580&sessionId=jFoOWCqiTl2JVJ3QPZrL4g%3D%3D\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93127f7e",
   "metadata": {},
   "source": [
    "### Sales nav scraper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5e0ea7d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ran out of input\n",
      "Error loading history pickle file\n",
      "0\n",
      "275\n"
     ]
    }
   ],
   "source": [
    "already_scraped_urls, to_scrape_urls = check_pickles(historyPickle, toScrapePickle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e83345e3-d166-4e30-a7d6-2634b0db34a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully scraped: https://www.linkedin.com/in/anssirantanen\n",
      "Successfully scraped: https://www.linkedin.com/in/bcigdemoglu\n",
      "Successfully scraped: https://www.linkedin.com/in/wenliang-zhao\n",
      "Successfully scraped: https://www.linkedin.com/in/davidbrazdil\n",
      "Successfully scraped: https://www.linkedin.com/in/stephanaltmueller\n",
      "Successfully scraped: https://www.linkedin.com/in/rohitkelapure\n",
      "Successfully scraped: https://www.linkedin.com/in/shivangi-sharma01\n",
      "Successfully scraped: https://www.linkedin.com/in/sbocknek\n",
      "Successfully scraped: https://www.linkedin.com/in/mskalra\n",
      "Successfully scraped: https://www.linkedin.com/in/bardek\n",
      "Successfully scraped: https://www.linkedin.com/in/saumyagupta\n",
      "Successfully scraped: https://www.linkedin.com/in/dvavakian\n",
      "Successfully scraped: https://www.linkedin.com/in/imackinn\n",
      "Successfully scraped: https://www.linkedin.com/in/sshumaker\n",
      "Successfully scraped: https://www.linkedin.com/in/mattnewtonjr\n",
      "Successfully scraped: https://www.linkedin.com/in/mike-cheng-82879a12\n",
      "Successfully scraped: https://www.linkedin.com/in/ross-wolf-0523741\n",
      "Successfully scraped: https://www.linkedin.com/in/stephenrmerritt\n",
      "Successfully scraped: https://www.linkedin.com/in/iliashatzis\n",
      "Successfully scraped: https://www.linkedin.com/in/jolene-lozano\n",
      "Successfully scraped: https://www.linkedin.com/in/conrad-lee\n",
      "Successfully scraped: https://www.linkedin.com/in/hugh-ocinneide\n",
      "Successfully scraped: https://www.linkedin.com/in/albertojrigail\n",
      "Successfully scraped: https://www.linkedin.com/in/alex-g-298b8242\n",
      "Successfully scraped: https://www.linkedin.com/in/delauno-hinson\n",
      "Successfully scraped: https://www.linkedin.com/in/ferraz-gabriel\n",
      "Successfully scraped: https://www.linkedin.com/in/kaylyn-shibata-008bb23b\n",
      "Successfully scraped: https://www.linkedin.com/in/joshua-cohen-359424108\n",
      "Successfully scraped: https://www.linkedin.com/in/sagart851\n",
      "Successfully scraped: https://www.linkedin.com/in/nolanholden\n",
      "Successfully scraped: https://www.linkedin.com/in/mavismei\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m----> 2\u001b[0m     updated_to_scrape_urls \u001b[38;5;241m=\u001b[39m \u001b[43mscrape_profiles\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdriver\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mSALES_NAV_SEARCH_URL\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43malready_scraped_urls\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mto_scrape_urls\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtoScrapePickle\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      3\u001b[0m     \u001b[38;5;28mprint\u001b[39m(updated_to_scrape_urls)\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "Cell \u001b[0;32mIn[4], line 158\u001b[0m, in \u001b[0;36mscrape_profiles\u001b[0;34m(driver, SALES_NAV_SEARCH_URL, already_scraped_urls, to_scrape_urls, toScrapePickle)\u001b[0m\n\u001b[1;32m    155\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m click_profile(profile):\n\u001b[1;32m    156\u001b[0m     \u001b[38;5;28;01mcontinue\u001b[39;00m\n\u001b[0;32m--> 158\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[43mwait_for_element_to_load\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdriver\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mBy\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mCLASS_NAME\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m_actions-container_1dg5u8\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m:\n\u001b[1;32m    159\u001b[0m     \u001b[38;5;28;01mcontinue\u001b[39;00m\n\u001b[1;32m    161\u001b[0m \u001b[38;5;66;03m# time.sleep(2 + random.random() * 6)\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[4], line 61\u001b[0m, in \u001b[0;36mwait_for_element_to_load\u001b[0;34m(driver, by, name, base, timeout)\u001b[0m\n\u001b[1;32m     59\u001b[0m base \u001b[38;5;241m=\u001b[39m base \u001b[38;5;129;01mor\u001b[39;00m driver\n\u001b[1;32m     60\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 61\u001b[0m     element \u001b[38;5;241m=\u001b[39m \u001b[43mWebDriverWait\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbase\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43muntil\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     62\u001b[0m \u001b[43m        \u001b[49m\u001b[43mEC\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpresence_of_element_located\u001b[49m\u001b[43m(\u001b[49m\u001b[43m(\u001b[49m\u001b[43mby\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     63\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     64\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m element\n\u001b[1;32m     65\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m TimeoutException:\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/selenium/webdriver/support/wait.py:102\u001b[0m, in \u001b[0;36mWebDriverWait.until\u001b[0;34m(self, method, message)\u001b[0m\n\u001b[1;32m    100\u001b[0m     screen \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mgetattr\u001b[39m(exc, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mscreen\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[1;32m    101\u001b[0m     stacktrace \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mgetattr\u001b[39m(exc, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstacktrace\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[0;32m--> 102\u001b[0m \u001b[43mtime\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msleep\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_poll\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    103\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m time\u001b[38;5;241m.\u001b[39mmonotonic() \u001b[38;5;241m>\u001b[39m end_time:\n\u001b[1;32m    104\u001b[0m     \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "try:\n",
    "    updated_to_scrape_urls = scrape_profiles(driver, SALES_NAV_SEARCH_URL, already_scraped_urls, to_scrape_urls, toScrapePickle)\n",
    "    print(updated_to_scrape_urls)\n",
    "except Exception as e:\n",
    "    print(f\"An unexpected error occurred: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b1c8979-0304-4621-ae96-59108d44bada",
   "metadata": {},
   "source": [
    "### Deduplicate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0ea0fe49",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Removed 0 duplicates\n",
      "306\n"
     ]
    }
   ],
   "source": [
    "deduplicate(toScrapePickle, to_scrape_urls, already_scraped_urls)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6da7736-3078-4e03-8be6-967f48640cdc",
   "metadata": {},
   "source": [
    "### Profile scraper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c61b767",
   "metadata": {},
   "outputs": [],
   "source": [
    "# driver.close()\n",
    "# driver = instantiate_driver()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7187f14e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# of profiles to scrape: 306\n",
      "At index: 1 - url: https://www.linkedin.com/in/cierra-hylton-6952451a4\n",
      "Ran out of input\n",
      "Error loading history pickle file\n",
      "Successfully scraped experiences\n",
      "Successfully scraped education\n",
      "Successfully scraped degree of connection: 3rd\n",
      "Successfully scraped description: Operations Assistant\n",
      "Successfully scraped profile link: https://www.linkedin.com/in/cierra-hylton-6952451a4/\n",
      "\n",
      "Success!\n",
      "Name: Cierra Hylton\n",
      "Experiences: [{'title: ': 'Quinn Orthopedic Physical Therapy', 'company: ': 'Full-time · 2 yrs 5 mos', 'dates: ': 'Cupertino, California, United States', 'location: ': 'Operations Assistant', 'summary: ': 'Mar 2024 to Present · 6 mos', 'remaining: ': 'Patient Account Representative, Jul 2023 to Mar 2024 · 9 mos, Receptionist, Apr 2022 to Jul 2023 · 1 yr 4 mos, On-site'}, {'title: ': 'Manager/Driver', 'company: ': 'FedEx Ground · Full-time', 'dates: ': 'Aug 2021 to Jul 2022 · 1 yr', 'location: ': 'United States'}, {'title: ': 'Receiving Lead', 'company: ': 'Sanmina · Full-time', 'dates: ': 'Sep 2020 to Jun 2021 · 10 mos', 'location: ': 'San Jose, California, United States'}, {'title: ': 'Pizza Hut', 'company: ': '1 yr 9 mos', 'dates: ': 'Tracy, California, United States', 'location: ': 'Shift Manager', 'summary: ': 'Full-time', 'remaining: ': 'Sep 2019 to Jun 2020 · 10 mos, Delivery Driver, Part-time, Oct 2018 to Sep 2019 · 1 yr'}, {'title: ': 'Dasher', 'company: ': 'DoorDash · Self-employed', 'dates: ': 'May 2018 to Oct 2018 · 6 mos', 'location: ': 'Tracy, California, United States'}]\n",
      "Education: [{'school: ': 'Sonoma State University', 'degree: ': '2017 - 2018', 'dates: ': 'Activities and societies: Softball'}, {'school: ': 'Las Positas College', 'degree: ': 'Psychology', 'dates: ': '2019 - 2021'}, {'school: ': 'Tracy High School', 'degree: ': '2013 - 2017', 'dates: ': 'Activities and societies: Softball'}, {'school: ': 'San Joaquin Delta College', 'degree: ': '2018 - 2019'}]\n",
      "Profile Distance: 3rd\n",
      "Mutuals: N/A\n",
      "Description: Operations Assistant\n",
      "Link: https://www.linkedin.com/in/cierra-hylton-6952451a4/\n",
      "\n",
      "saving profile info\n",
      "adding to history\n",
      "recording scraped url\n",
      "removing from to-scrape\n",
      "0.32679738562091504 % Done - at index: 1\n",
      "\n",
      "----------------------------------------------------------------------------------------------------------\n",
      "\n",
      "At index: 1 - url: https://www.linkedin.com/in/graytowne\n",
      "Successfully scraped experiences\n",
      "Successfully scraped education\n",
      "Successfully scraped degree of connection: 3rd\n",
      "Successfully scraped description: Staff Software Engineer at Google Deepmind\n",
      "Successfully scraped profile link: https://www.linkedin.com/in/graytowne/\n",
      "\n",
      "Success!\n",
      "Name: Jiaxi Tang\n",
      "Experiences: [{'title: ': 'Google DeepMind', 'company: ': '2 yrs 5 mos', 'dates: ': 'Staff Software Engineer', 'location: ': 'Nov 2023 to Present · 10 mos', 'summary: ': 'Building something new about LLM x Recommender Systems', 'remaining: ': 'Senior Software Engineer, Apr 2022 to Nov 2023 · 1 yr 8 mos, (Previously Google Brain) Worked on applied machine learning (user modeling and recommender systems) for various Google products (including Ads and YouTube).'}, {'title: ': 'Software Engineer', 'company: ': 'Google Brain · Full-time', 'dates: ': 'Jun 2020 to Mar 2022 · 1 yr 10 mos'}, {'title: ': 'Research Assistant', 'company: ': 'Simon Fraser University', 'dates: ': 'Sep 2015 to Dec 2019 · 4 yrs 4 mos', 'location: ': 'Burnaby, British Columbia, Canada', 'summary: ': 'My thesis focuses on improving recommendation performance by exploiting user activity sequences.'}, {'title: ': 'Research Intern', 'company: ': 'Google AI', 'dates: ': 'Jul 2019 to Sep 2019 · 3 mos', 'location: ': 'San Francisco Bay Area', 'summary: ': 'Worked on fundamental machine learning research.'}, {'title: ': 'Software Engineering Intern', 'company: ': 'Google AI', 'dates: ': 'May 2018 to Jul 2018 · 3 mos', 'location: ': 'San Francisco Bay Area', 'summary: ': 'Worked on neural recommender system for Youtube at Google Research. The related research finding is published on WWW 2019.'}, {'title: ': 'Research Intern', 'company: ': 'Chinese Academy of Sciences', 'dates: ': 'Jul 2014 to Feb 2015 · 8 mos', 'location: ': 'Beijing City, China', 'summary: ': 'Supervised by Dr. Ping Luo from Machine Learning and Data Mining Group, I established user behavior model for a mobile application. The related work is published on IJCAI 2016.'}]\n",
      "Education: [{'school: ': 'Simon Fraser University', 'degree: ': 'Doctor of Philosophy (Ph.D.), Computer Science', 'dates: ': '2015 - 2019'}, {'school: ': 'Wuhan University', 'degree: ': \"Bachelor's degree, Software Engineering\", 'dates: ': '2011 - 2015', 'remaining: ': ''}]\n",
      "Profile Distance: 3rd\n",
      "Mutuals: N/A\n",
      "Description: Staff Software Engineer at Google Deepmind\n",
      "Link: https://www.linkedin.com/in/graytowne/\n",
      "\n",
      "saving profile info\n",
      "adding to history\n",
      "recording scraped url\n",
      "removing from to-scrape\n",
      "0.6535947712418301 % Done - at index: 2\n",
      "\n",
      "----------------------------------------------------------------------------------------------------------\n",
      "\n",
      "At index: 1 - url: https://www.linkedin.com/in/rosehogmire\n",
      "Successfully scraped experiences\n",
      "Successfully scraped education\n",
      "Successfully found mutual connections: Ben Braverman is a mutual connection\n",
      "Successfully scraped degree of connection: 2nd\n",
      "Successfully scraped description: Operations Manager at Flexport\n",
      "Successfully scraped profile link: https://www.linkedin.com/in/rosehogmire/\n",
      "\n",
      "Success!\n",
      "Name: Rose Hogmire\n",
      "Experiences: [{'title: ': 'Flexport', 'company: ': '5 yrs 5 mos', 'dates: ': 'Operations Manager - Air & Ocean', 'location: ': 'Full-time', 'summary: ': 'May 2024 to Present · 4 mos', 'remaining: ': 'Chicago, Illinois, United States · On-site, Manager of Account Management, Global Key Accounts, Full-time, Aug 2022 to Jun 2024 · 1 yr 11 mos, Chicago, Illinois, United States, Senior Client Solutions Associate, Global Key Accounts, Sep 2021 to Aug 2022 · 1 yr, Chicago, Illinois, United States, Client Solutions Associate, Enterprise Accounts, Jul 2020 to Sep 2021 · 1 yr 3 mos, Chicago, Illinois, United States, Global Operations Associate, Apr 2019 to Jul 2020 · 1 yr 4 mos, Greater Chicago Area'}, {'title: ': 'Market Analyst', 'company: ': 'StageThree', 'dates: ': 'Mar 2018 to Mar 2019 · 1 yr 1 mo', 'location: ': 'Green Bay, Wisconsin Area', 'summary: ': 'Focused research and consultative efforts developing a stealth supply chain SaaS startup. Facilitated workshops with corporate customers to inform product design and understand supply chain business processes. Served as a liaison between customers and DevOps team. Created in-depth and data-based competitive landscape analyses.'}, {'title: ': 'Legislative Intern', 'company: ': 'United States Senate - Office of Senator Al Franken', 'dates: ': 'Jan 2017 to May 2017 · 5 mos', 'location: ': 'Washington D.C. Metro Area'}, {'title: ': 'Government Relations & Regulatory Affairs Intern', 'company: ': 'ECMC Group', 'dates: ': 'Dec 2016 to Jan 2017 · 2 mos', 'location: ': 'Greater Minneapolis-St. Paul Area'}, {'title: ': 'Legislative Intern', 'company: ': 'United States Senate - Office of Senator Al Franken', 'dates: ': 'May 2016 to Aug 2016 · 4 mos', 'location: ': 'Greater Minneapolis-St. Paul Area'}, {'title: ': 'Peer Mentor & Tutor', 'company: ': 'Crossroads Academy', 'dates: ': 'Sep 2015 to May 2016 · 9 mos', 'location: ': 'Ripon, WI', 'summary: ': 'Tutored and counseled at-risk youths at an alternative education facility in the Ripon Area school district.'}, {'title: ': 'Government Affairs & Community Outreach Intern', 'company: ': 'ECMC', 'dates: ': 'Jun 2015 to Jan 2016 · 8 mos', 'location: ': 'Oakdale, MN'}, {'title: ': 'Human Resources Intern', 'company: ': 'National Recoveries, Inc.', 'dates: ': 'Jun 2014 to Aug 2014 · 3 mos', 'location: ': 'Coon Rapids, MN', 'summary: ': 'Audited files for a student loan collection agency funded through the Department of Education. Assisted with recruitment and hiring. Organized company events and activities within the office. Performed data interpretation and entry tasks.'}]\n",
      "Education: [{'school: ': 'Ripon College', 'degree: ': 'Bachelor of Arts (B.A.), Politics & Government, Economics', 'dates: ': '2015 - 2018', 'remaining: ': ''}, {'school: ': 'American University', 'degree: ': 'Washington Semester Program, Global Economics & Business', 'dates: ': '2017 - 2017', 'remaining: ': ''}]\n",
      "Profile Distance: 2nd\n",
      "Mutuals: Ben Braverman is a mutual connection\n",
      "Description: Operations Manager at Flexport\n",
      "Link: https://www.linkedin.com/in/rosehogmire/\n",
      "\n",
      "saving profile info\n",
      "adding to history\n",
      "recording scraped url\n",
      "removing from to-scrape\n",
      "0.9803921568627451 % Done - at index: 3\n",
      "\n",
      "----------------------------------------------------------------------------------------------------------\n",
      "\n",
      "At index: 1 - url: https://www.linkedin.com/in/phoenixmjay\n",
      "Successfully scraped experiences\n",
      "Successfully scraped education\n",
      "Successfully found mutual connections: Clancey Stahr, Janessa Lantz 🌻, and 3 other mutual connections\n",
      "Successfully scraped degree of connection: 2nd\n",
      "Successfully scraped description: Data & Analytics Leader | Speaker\n",
      "Successfully scraped profile link: https://www.linkedin.com/in/phoenixmjay/\n",
      "\n",
      "Success!\n",
      "Name: Phoenix Jay\n",
      "Experiences: [{'title: ': 'Global Manager, Digital Community', 'company: ': 'dbt Labs · Full-time', 'dates: ': 'Mar 2024 to Jul 2024 · 5 mos', 'location: ': 'Remote', 'summary: ': 'LinkedIn helped me get this job', 'remaining: ': '• Grew channels from 100 to 1100+ members and drove 5X improvement in engagement metrics • Supported revenue metrics by integrating strategies across sales, marketing, and community, Strategic Planning · Leadership · Team Leadership · Data Modeling · Data Literacy · Creativity and Innovation · Integrated Brand Marketing · Intercultural Communication · Financial Analysis · Data Strategies · Data Analytics & Reporting · Community Development'}, {'title: ': 'Head of Finance Analytics', 'company: ': 'K Health · Full-time', 'dates: ': 'Aug 2022 to Dec 2023 · 1 yr 5 mos', 'location: ': 'Remote', 'summary: ': 'LinkedIn helped me get this job', 'remaining: ': '• Redesigned analytics data stack to easily communicate data tools and flow to executives • Acquired cloud & API resources for my team by building relationships with DevOps, Data Models · Data Warehousing · Business Intelligence (BI) · Strategic Thinking · Looker (Software) · Team Leadership · Data Modeling · AWS Cloud Migration · Data Literacy · Business Analysis · Analytical Skills · Technical Documentation · Google BigQuery · Data Governance · SQL · Financial Analysis · Data Strategies · dbt · Platform Modernization · Data Analytics & Reporting'}, {'title: ': 'Vice President of Data & Analytics', 'company: ': 'Mentor Me Collective', 'dates: ': 'Nov 2022 to Oct 2023 · 1 yr', 'location: ': 'Remote', 'summary: ': '• Led a team from founding phase to develop and sustain ongoing data infrastructure.', 'remaining: ': 'Decision-Making · Leadership · Strategic Thinking · Team Leadership · Data Modeling · Decisiveness · Data Literacy · Business Analysis · Organizational Leadership · Organizational Development · Delegation · Time Management · Coaching & Mentoring · Financial Analysis · Data Strategies · Platform Modernization · Data Analytics & Reporting'}, {'title: ': 'Analytics Engineer, Lead', 'company: ': 'soona · Full-time', 'dates: ': 'Feb 2022 to Aug 2022 · 7 mos', 'location: ': 'Remote', 'summary: ': '• Data Platform Modernization from MS Excel to AWS Redshift and Looker • Reduced turnaround of financially valuable insights by 6 hours/request • Presented monthly innovative insights to the company & executives', 'remaining: ': 'Data Models · Data Warehousing · Project Management · ETL Tools · Data Management · Business Intelligence (BI) · Strategic Thinking · Looker (Software) · Data Modeling · GitHub · AWS Cloud Migration · Strategy · Data Literacy · Front-End Development · Business Analysis · Data Analysis · Analytical Skills · Data Science · Technical Documentation · Jira · Python · Data Governance · SQL · Data Visualization · Financial Analysis · Data Strategies · dbt · Platform Modernization · Data Analytics & Reporting'}, {'title: ': 'Data Manager', 'company: ': 'Columbia University Irving Medical Center · Full-time', 'dates: ': 'Mar 2019 to Feb 2022 · 3 yrs', 'location: ': 'New York, United States · Hybrid', 'summary: ': '• Improved data consistency and reliability to 99.9% accuracy. • Increased satisfaction rates 8% and staff processing from 2 months backlog to advance preparation ahead of data requests. • Monitored and managed database ETL processes via Excel VBA, PGAdmin, and PSQL Shell.', 'remaining: ': 'Tableau · Project Management · ETL Tools · Leadership · Data Management · Business Intelligence (BI) · Strategic Thinking · Team Leadership · Data Modeling · Machine Learning · Strategy · Statistics · Data Literacy · Data Analysis · Data Science · SQL · Data Visualization · Financial Analysis · Data Strategies · Platform Modernization · Data Analytics & Reporting'}, {'title: ': 'Associate Board Member', 'company: ': 'Bergen Volunteer Medical Initiative (BVMI)', 'dates: ': 'Jan 2022 to Jan 2022 · 1 mo', 'location: ': 'Collaborating with leadership, staff, and Board of Trustees on strategic planning, fundraising events, and volunteer recruitment to meet the medical needs of low-income, working, uninsured adults by providing free, comprehensive and patient-centered primary healthcare.', 'summary: ': 'Project Management · Leadership · Strategic Thinking · Fundraising · Strategy · Digital Marketing · Planning Budgeting & Forecasting · Board Relations · Financial Analysis · Data Strategies · Strategic Communications'}, {'title: ': 'Board Member', 'company: ': 'Arts Business Collaborative', 'dates: ': 'Jan 2022 to Jan 2022 · 1 mo', 'location: ': 'Board Level · Nonprofit Board Development · Project Management · Leadership · Strategic Thinking · Strategic Partnerships · Strategy · Data Literacy · Board Governance · Board Development · Board Relations · Board of Directors · Financial Analysis · Data Strategies · Data Analytics & Reporting'}, {'title: ': 'Mentor', 'company: ': 'AnitaB.org', 'dates: ': 'Jan 2022 to Jan 2022 · 1 mo', 'location: ': 'Leadership · Professional Mentoring · Leadership Development · Intercultural Communication · Coaching & Mentoring · Data Strategies · Empathy'}, {'title: ': '/dev/color', 'company: ': '1 yr 1 mo', 'dates: ': 'New York, United States', 'location: ': 'Board Council', 'summary: ': 'Jan 2022 to Jan 2022 · 1 mo', 'remaining: ': \"Among leadership, Board, and staff we inform direction of strategic planning and champion growth for the organization., Strategic Planning · Leadership · Interpersonal Communication · Business Strategy · Intercultural Communication · Board Relations · Financial Analysis · Data Analytics & Reporting · Community Development, A* Facilitator, Jan 2021 to Jan 2022 · 1 yr 1 mo, As Facilitator, I lead monthly meetings, by guiding and nurturing the health of the cohort's relationship while providing feedback to inspire and mobilize each member's career and personal growth., Project Management · Leadership · Strategic Thinking · Team Leadership · Strategy · Financial Analysis\"}, {'title: ': 'Columbia University School of Professional Studies', 'company: ': '2 yrs 10 mos', 'dates: ': 'Finance Operations, Specialist', 'location: ': 'Full-time', 'summary: ': 'Oct 2016 to Mar 2019 · 2 yrs 6 mos', 'remaining: ': 'Greater New York City Area, Systematically used data-driven practices to reduce backlog from 6 months down to 2 weeks. • Mitigated process gaps and managed all operational functions • Zendesk management of accounts payable and procurement requests • Monthly reconciliation and annual fiscal year-end closing procedures, Project Management · Leadership · Strategic Thinking · Strategy · Data Analysis · Financial Analysis · Data Strategies · Data Analytics & Reporting, Data & Finance Coordinator, Jun 2016 to Oct 2016 · 5 mos, New York, United States · On-site, Data Analytics & Reporting'}, {'title: ': 'Columbia University', 'company: ': '4 yrs 1 mo', 'dates: ': 'Head Intern, Finance', 'location: ': '2015 to 2016 · 1 yr', 'summary: ': 'On-site', 'remaining: ': '5 Senior Intern reports, ~12 Intern reports/Senior, Team Leadership, Senior Intern, Media, 2014 to 2015 · 1 yr, 2014 - Senior Intern, Media (12 Intern reports), Team Leadership, Intern, 2012 to 2014 · 2 yrs, On-site'}, {'title: ': 'Business Operations Coordinator', 'company: ': 'Say Yes Buffalo', 'dates: ': '2015 to 2016 · 1 yr', 'location: ': 'New York, United States · On-site'}]\n",
      "Education: [{'school: ': 'Columbia University', 'degree: ': 'Master of Science (M.S.), Applied Analytics', 'dates: ': '2018 - 2020', 'remaining: ': ''}, {'school: ': 'Columbia University', 'degree: ': 'Bachelor of Arts (B.A.), East Asian Languages and Culture', 'dates: ': 'Activities and societies: Alpha Omicron Pi', 'remaining: ': ''}, {'school: ': 'Phillips Exeter Academy', 'degree: ': 'High School Diploma', 'dates: ': 'Python'}]\n",
      "Profile Distance: 2nd\n",
      "Mutuals: Clancey Stahr, Janessa Lantz 🌻, and 3 other mutual connections\n",
      "Description: Data & Analytics Leader | Speaker\n",
      "Link: https://www.linkedin.com/in/phoenixmjay/\n",
      "\n",
      "saving profile info\n",
      "adding to history\n",
      "recording scraped url\n",
      "removing from to-scrape\n",
      "1.3071895424836601 % Done - at index: 4\n",
      "\n",
      "----------------------------------------------------------------------------------------------------------\n",
      "\n",
      "At index: 1 - url: https://www.linkedin.com/in/xi-sun-161a5b54\n",
      "Successfully scraped experiences\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[10], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m results \u001b[38;5;241m=\u001b[39m \u001b[43mscrape_all_profiles\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdriver\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhistoryPickle\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtoScrapePickle\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mresultsPickle\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;28mlen\u001b[39m(results))\n",
      "Cell \u001b[0;32mIn[5], line 232\u001b[0m, in \u001b[0;36mscrape_all_profiles\u001b[0;34m(driver, historyPickle, toScrapePickle, resultsPickle)\u001b[0m\n\u001b[1;32m    229\u001b[0m         \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mError loading history pickle file\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    231\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 232\u001b[0m     profile \u001b[38;5;241m=\u001b[39m \u001b[43mscrape_profile\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdriver\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43murl\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    233\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m profile \u001b[38;5;241m!=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    234\u001b[0m         \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msaving profile info\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[0;32mIn[5], line 166\u001b[0m, in \u001b[0;36mscrape_profile\u001b[0;34m(driver, scraped_link)\u001b[0m\n\u001b[1;32m    164\u001b[0m education_url \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(scraped_link, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdetails/education\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    165\u001b[0m driver\u001b[38;5;241m.\u001b[39mget(education_url)\n\u001b[0;32m--> 166\u001b[0m scraped_education \u001b[38;5;241m=\u001b[39m \u001b[43mget_education\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdriver\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    167\u001b[0m time\u001b[38;5;241m.\u001b[39msleep(\u001b[38;5;241m2\u001b[39m \u001b[38;5;241m+\u001b[39m random\u001b[38;5;241m.\u001b[39mrandom() \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m6\u001b[39m)\n\u001b[1;32m    170\u001b[0m \u001b[38;5;66;03m# Scrape degree of connection and mutuals if available\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[5], line 80\u001b[0m, in \u001b[0;36mget_education\u001b[0;34m(driver)\u001b[0m\n\u001b[1;32m     77\u001b[0m scraped_education \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m     79\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 80\u001b[0m     \u001b[43mwait_for_element_to_load\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdriver\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mBy\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mCSS_SELECTOR\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mli.pvs-list__paged-list-item.artdeco-list__item.pvs-list__item--line-separated.pvs-list__item--one-column\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     81\u001b[0m     education_items \u001b[38;5;241m=\u001b[39m driver\u001b[38;5;241m.\u001b[39mfind_elements(By\u001b[38;5;241m.\u001b[39mCSS_SELECTOR, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mli.pvs-list__paged-list-item.artdeco-list__item.pvs-list__item--line-separated.pvs-list__item--one-column\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     83\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(education_items) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
      "Cell \u001b[0;32mIn[4], line 61\u001b[0m, in \u001b[0;36mwait_for_element_to_load\u001b[0;34m(driver, by, name, base, timeout)\u001b[0m\n\u001b[1;32m     59\u001b[0m base \u001b[38;5;241m=\u001b[39m base \u001b[38;5;129;01mor\u001b[39;00m driver\n\u001b[1;32m     60\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 61\u001b[0m     element \u001b[38;5;241m=\u001b[39m \u001b[43mWebDriverWait\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbase\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43muntil\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     62\u001b[0m \u001b[43m        \u001b[49m\u001b[43mEC\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpresence_of_element_located\u001b[49m\u001b[43m(\u001b[49m\u001b[43m(\u001b[49m\u001b[43mby\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     63\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     64\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m element\n\u001b[1;32m     65\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m TimeoutException:\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/selenium/webdriver/support/wait.py:102\u001b[0m, in \u001b[0;36mWebDriverWait.until\u001b[0;34m(self, method, message)\u001b[0m\n\u001b[1;32m    100\u001b[0m     screen \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mgetattr\u001b[39m(exc, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mscreen\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[1;32m    101\u001b[0m     stacktrace \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mgetattr\u001b[39m(exc, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstacktrace\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[0;32m--> 102\u001b[0m \u001b[43mtime\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msleep\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_poll\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    103\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m time\u001b[38;5;241m.\u001b[39mmonotonic() \u001b[38;5;241m>\u001b[39m end_time:\n\u001b[1;32m    104\u001b[0m     \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "results = scrape_all_profiles(driver, historyPickle, toScrapePickle, resultsPickle)\n",
    "print(len(results))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0df469c1",
   "metadata": {},
   "source": [
    "# QUERY 2: UNICORN COMPANIES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df63090d",
   "metadata": {},
   "outputs": [],
   "source": [
    "historyPickle = 'db/unicorn_history.pickle'\n",
    "toScrapePickle = 'db/unicorn_to_scrape.pickle'\n",
    "resultsPickle = 'db/unicorn_results.pickle'\n",
    "\n",
    "SALES_NAV_SEARCH_URL = \"\"\"\n",
    "https://www.linkedin.com/sales/search/people?savedSearchId=1833971588&sessionId=zVKTgf2GQk2owfFVzL9aFQ%3D%3D\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1a8a32b",
   "metadata": {},
   "source": [
    "### Sales nav scraper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc90b784",
   "metadata": {},
   "outputs": [],
   "source": [
    "already_scraped_urls, to_scrape_urls = check_pickles(historyPickle, toScrapePickle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82fb8467",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    updated_to_scrape_urls = scrape_profiles(driver, SALES_NAV_SEARCH_URL, already_scraped_urls, to_scrape_urls, toScrapePickle)\n",
    "    print(updated_to_scrape_urls)\n",
    "except Exception as e:\n",
    "    print(f\"An unexpected error occurred: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98ff5ea6",
   "metadata": {},
   "source": [
    "### Deduplicate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96c9a4c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "deduplicate(toScrapePickle, to_scrape_urls, already_scraped_urls)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29a8309e",
   "metadata": {},
   "source": [
    "### Profile scraper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7c7ad15",
   "metadata": {},
   "outputs": [],
   "source": [
    "# driver.close()\n",
    "# driver = instantiate_driver()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77b3e90f",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = scrape_all_profiles(driver, historyPickle, toScrapePickle, resultsPickle)\n",
    "print(len(results))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d71f2e26",
   "metadata": {},
   "source": [
    "# QUERY 3: ACQUIRED COMPANIES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22414fd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "historyPickle = 'db/acquired_history.pickle'\n",
    "toScrapePickle = 'db/acquired_to_scrape.pickle'\n",
    "resultsPickle = 'db/acquired_results.pickle'\n",
    "\n",
    "SALES_NAV_SEARCH_URL = \"\"\"\n",
    "https://www.linkedin.com/sales/search/people?savedSearchId=1833971612&sessionId=R7vv8AiYRXiqMseY1m3CwQ%3D%3D\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c88db61",
   "metadata": {},
   "source": [
    "### Sales nav scraper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "643faf85",
   "metadata": {},
   "outputs": [],
   "source": [
    "already_scraped_urls, to_scrape_urls = check_pickles(historyPickle, toScrapePickle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2ee22e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    updated_to_scrape_urls = scrape_profiles(driver, SALES_NAV_SEARCH_URL, already_scraped_urls, to_scrape_urls, toScrapePickle)\n",
    "    print(updated_to_scrape_urls)\n",
    "except Exception as e:\n",
    "    print(f\"An unexpected error occurred: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8eb29f7",
   "metadata": {},
   "source": [
    "### Deduplicate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d324c97",
   "metadata": {},
   "outputs": [],
   "source": [
    "deduplicate(toScrapePickle, to_scrape_urls, already_scraped_urls)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e12ff51",
   "metadata": {},
   "source": [
    "### Profile scraper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d25220f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# driver.close()\n",
    "# driver = instantiate_driver()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c357cf25",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = scrape_all_profiles(driver, historyPickle, toScrapePickle, resultsPickle)\n",
    "print(len(results))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b637904",
   "metadata": {},
   "source": [
    "# QUERY 4: VC PORTFOLIO COMPANIES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4efcd7ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "historyPickle = 'db/portfolio_history.pickle'\n",
    "toScrapePickle = 'db/portfolio_to_scrape.pickle'\n",
    "resultsPickle = 'db/portfolio_results.pickle'\n",
    "\n",
    "SALES_NAV_SEARCH_URL = \"\"\"\n",
    "https://www.linkedin.com/sales/search/people?savedSearchId=1833971604&sessionId=zVKTgf2GQk2owfFVzL9aFQ%3D%3D\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "457c988d",
   "metadata": {},
   "source": [
    "### Sales nav scraper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e3747a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "already_scraped_urls, to_scrape_urls = check_pickles(historyPickle, toScrapePickle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a411299f",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    updated_to_scrape_urls = scrape_profiles(driver, SALES_NAV_SEARCH_URL, already_scraped_urls, to_scrape_urls, toScrapePickle)\n",
    "    print(updated_to_scrape_urls)\n",
    "except Exception as e:\n",
    "    print(f\"An unexpected error occurred: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c4905aa",
   "metadata": {},
   "source": [
    "### Deduplicate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "baf6a7b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "deduplicate(toScrapePickle, to_scrape_urls, already_scraped_urls)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5281ff8",
   "metadata": {},
   "source": [
    "### Profile scraper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a190ff43",
   "metadata": {},
   "outputs": [],
   "source": [
    "# driver.close()\n",
    "# driver = instantiate_driver()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5ec701f",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = scrape_all_profiles(driver, historyPickle, toScrapePickle, resultsPickle)\n",
    "print(len(results))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47d73a6f",
   "metadata": {},
   "source": [
    "# QUERY 5: INFRA COMPANIES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa93ce00",
   "metadata": {},
   "outputs": [],
   "source": [
    "historyPickle = 'db/infra_history.pickle'\n",
    "toScrapePickle = 'db/infra_to_scrape.pickle'\n",
    "resultsPickle = 'db/infra_results.pickle'\n",
    "\n",
    "SALES_NAV_SEARCH_URL = \"\"\"\n",
    "https://www.linkedin.com/sales/search/people?savedSearchId=1833971620&sessionId=R7vv8AiYRXiqMseY1m3CwQ%3D%3D\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "924aae38",
   "metadata": {},
   "source": [
    "### Sales nav scraper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7961266",
   "metadata": {},
   "outputs": [],
   "source": [
    "already_scraped_urls, to_scrape_urls = check_pickles(historyPickle, toScrapePickle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2f9a6f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    updated_to_scrape_urls = scrape_profiles(driver, SALES_NAV_SEARCH_URL, already_scraped_urls, to_scrape_urls, toScrapePickle)\n",
    "    print(updated_to_scrape_urls)\n",
    "except Exception as e:\n",
    "    print(f\"An unexpected error occurred: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eec8b4b3",
   "metadata": {},
   "source": [
    "### Deduplicate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2df023a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "deduplicate(toScrapePickle, to_scrape_urls, already_scraped_urls)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f55d6e65",
   "metadata": {},
   "source": [
    "### Profile scraper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73492e48",
   "metadata": {},
   "outputs": [],
   "source": [
    "# driver.close()\n",
    "# driver = instantiate_driver()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dffc900a",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = scrape_all_profiles(driver, historyPickle, toScrapePickle, resultsPickle)\n",
    "print(len(results))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47185763-6f2b-4f89-9acc-1651887008f6",
   "metadata": {},
   "source": [
    "# EXPORTING RESULTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0a13c33b",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'results1' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[11], line 50\u001b[0m\n\u001b[1;32m     47\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m sheet\u001b[38;5;241m.\u001b[39murl\n\u001b[1;32m     49\u001b[0m \u001b[38;5;66;03m# Usage\u001b[39;00m\n\u001b[0;32m---> 50\u001b[0m list_of_results \u001b[38;5;241m=\u001b[39m [\u001b[43mresults1\u001b[49m, results2, results3]  \u001b[38;5;66;03m# Your lists of ScrapedProfile objects\u001b[39;00m\n\u001b[1;32m     51\u001b[0m sheet_names \u001b[38;5;241m=\u001b[39m [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mResults1\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mResults2\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mResults3\u001b[39m\u001b[38;5;124m\"\u001b[39m]  \u001b[38;5;66;03m# Names for each sheet\u001b[39;00m\n\u001b[1;32m     52\u001b[0m sheet_url \u001b[38;5;241m=\u001b[39m create_google_sheet(list_of_results, sheet_names)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'results1' is not defined"
     ]
    }
   ],
   "source": [
    "# NEW GOOGLE SHEETS INTEGRATION\n",
    "\n",
    "def create_google_sheet(list_of_results, sheet_names):\n",
    "    # Set up the credentials\n",
    "    scope = ['https://spreadsheets.google.com/feeds', 'https://www.googleapis.com/auth/drive']\n",
    "    creds = ServiceAccountCredentials.from_json_keyfile_name('path/to/your/credentials.json', scope)\n",
    "    client = gspread.authorize(creds)\n",
    "\n",
    "    # Create a new Google Sheet\n",
    "    sheet = client.create('LinkedIn Scraped Profiles')\n",
    "    \n",
    "    for results, sheet_name in zip(list_of_results, sheet_names):\n",
    "        # Create a new worksheet\n",
    "        worksheet = sheet.add_worksheet(title=sheet_name, rows=\"1000\", cols=\"20\")\n",
    "\n",
    "        # Parse candidates and prepare rows\n",
    "        rows = []\n",
    "        for candidate in results:\n",
    "            row = parseCandidate(candidate)\n",
    "            rows.append(row)\n",
    "\n",
    "        if not rows:\n",
    "            print(f\"No data for sheet: {sheet_name}\")\n",
    "            continue\n",
    "\n",
    "        # Get column titles\n",
    "        col_titles = list(rows[0].keys())\n",
    "\n",
    "        # Prepare the data for batch update\n",
    "        cells = [col_titles]  # Start with the header row\n",
    "        for row in rows:\n",
    "            cells.append([row.get(col, '') for col in col_titles])\n",
    "\n",
    "        # Update the sheet in batch\n",
    "        cell_range = f'A1:{gspread.utils.rowcol_to_a1(len(cells), len(col_titles))}'\n",
    "        worksheet.update(cell_range, cells)\n",
    "\n",
    "        print(f\"Successfully added data to sheet: {sheet_name}\")\n",
    "\n",
    "    # Delete the default \"Sheet1\" if it exists\n",
    "    try:\n",
    "        sheet.del_worksheet(sheet.worksheet(\"Sheet1\"))\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "    print(f\"Google Sheet created successfully. URL: {sheet.url}\")\n",
    "    return sheet.url\n",
    "\n",
    "# Usage\n",
    "list_of_results = [results1, results2, results3]  # Your lists of ScrapedProfile objects\n",
    "sheet_names = [\"Results1\", \"Results2\", \"Results3\"]  # Names for each sheet\n",
    "sheet_url = create_google_sheet(list_of_results, sheet_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0906d83",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: FIX EXPORTS\n",
    "def parseCandidate(x):\n",
    "    res = {}\n",
    "    res['url'] = x.profile_link\n",
    "    res['name'] = x.profile_name\n",
    "    res['dist'] = x.profile_dist\n",
    "    res['description'] = x.profile_description\n",
    "\n",
    "    #Experiences first\n",
    "    print(x.experiences)\n",
    "    for i, e in enumerate(x.experiences):\n",
    "        res[f'exp{i} title'] = e[\"title: \"]\n",
    "        res[f'exp{i} company'] = e[\"company: \"]\n",
    "        res[f'exp{i} dates'] = e[\"dates: \"]\n",
    "\n",
    "    # School second\n",
    "    for i, e in enumerate(x.profile_school):\n",
    "        res[f'edu{i} school'] = e[\"school: \"]\n",
    "        if \"degree :\" in e:\n",
    "            res[f'edu{i} degree'] = e[\"degree: \"]\n",
    "\n",
    "    \n",
    "    return res\n",
    "\n",
    "# Create dataframe\n",
    "rows = []\n",
    "for candidate in results:\n",
    "    row = parseCandidate(candidate)\n",
    "    rows.append(row)\n",
    "\n",
    "# Write to CSV\n",
    "col_titles = rows[0].keys()\n",
    "\n",
    "try:\n",
    "    with open('candidates.csv', 'w', newline='') as output_file:\n",
    "        dict_writer = csv.DictWriter(output_file, col_titles)\n",
    "        dict_writer.writeheader()\n",
    "        dict_writer.writerows(rows)\n",
    "        print(\"Successfully exported to csv\")\n",
    "except Exception as e:\n",
    "    print(f\"Failed to export to csv. Error: {e}\")\n",
    "\n",
    "#Export to Excel\n",
    "try:\n",
    "    df.to_excel('candidates.xlsx', index=False)\n",
    "    print(\"Exported to Excel\")\n",
    "except:\n",
    "    print(\"Failed to export to Excel\")\n",
    "\n",
    "    \n",
    "# update db/already_scraped.pickle\n",
    "with open('db/already_scraped.pickle', 'rb') as f:\n",
    "    already_scraped = pickle.load(f)\n",
    "    print(f\"Previously scraped: {len(already_scraped)}\")\n",
    "    already_scraped = already_scraped + already_scraped_urls\n",
    "    already_scraped = list(set(already_scraped))\n",
    "    print(f\"Newly scraped: {len(already_scraped)}\")\n",
    "with open('db/already_scraped.pickle', 'wb') as f:\n",
    "    pickle.dump(already_scraped, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97efd509",
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.auth.transport.requests import Request\n",
    "from google.oauth2.credentials import Credentials\n",
    "from google_auth_oauthlib.flow import InstalledAppFlow\n",
    "from googleapiclient.discovery import build\n",
    "from googleapiclient.http import MediaFileUpload\n",
    "\n",
    "# If modifying these SCOPES, delete the file token.pickle.\n",
    "SCOPES = ['https://www.googleapis.com/auth/drive.file']\n",
    "\n",
    "def authenticate():\n",
    "    creds = None\n",
    "    # The file token.pickle stores the user's access and refresh tokens, and is created automatically when the authorization flow completes for the first time.\n",
    "    if os.path.exists('token.pickle'):\n",
    "        with open('token.pickle', 'rb') as token:\n",
    "            creds = pickle.load(token)\n",
    "    # If there are no (valid) credentials available, let the user log in.\n",
    "    if not creds or not creds.valid:\n",
    "        if creds and creds.expired and creds.refresh_token:\n",
    "            creds.refresh(Request())\n",
    "        else:\n",
    "            flow = InstalledAppFlow.from_client_secrets_file(\n",
    "                'client_secrets.json', SCOPES)\n",
    "            creds = flow.run_local_server(port=0)\n",
    "        # Save the credentials for the next run\n",
    "        with open('token.pickle', 'wb') as token:\n",
    "            pickle.dump(creds, token)\n",
    "    return creds\n",
    "\n",
    "def upload_file_to_drive(file_path, file_name, mime_type):\n",
    "    creds = authenticate()\n",
    "    service = build('drive', 'v3', credentials=creds)\n",
    "\n",
    "    file_metadata = {'name': file_name}\n",
    "    media = MediaFileUpload(file_path, mimetype=mime_type)\n",
    "\n",
    "    file = service.files().create(body=file_metadata, media_body=media, fields='id').execute()\n",
    "    print('File ID: %s' % file.get('id'))\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    file_path = 'path_to_your_local_excel_file.xlsx'  # Replace with the path to your local file\n",
    "    file_name = 'your_excel_file.xlsx'  # Replace with the desired name for the file in Google Drive\n",
    "    mime_type = 'application/vnd.openxmlformats-officedocument.spreadsheetml.sheet'\n",
    "\n",
    "    upload_file_to_drive(file_path, file_name, mime_type)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ee20a9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "categories = COMPANY_CATEGORIES\n",
    "\n",
    "# Example color mapping for categories\n",
    "category_colors = {\n",
    "    \"SECURITY\": 'red',\n",
    "    \"OTHER\": 'blue',\n",
    "    \"PUBLIC\": 'green',\n",
    "    \"INFRA\": 'yellow',\n",
    "    \"FINTECH\": 'orange',\n",
    "    \"CRYPTO\": 'purple',\n",
    "    \"FRONTIER\": 'cyan',\n",
    "    \"AI\": 'magenta'\n",
    "}\n",
    "\n",
    "# Create a reverse dictionary for easier lookup: {company: category}\n",
    "company_category = {}\n",
    "for category, companies in categories.items():\n",
    "    for company in companies:\n",
    "        company_category[company] = category\n",
    "\n",
    "# Modify the style function\n",
    "def highlight_by_category(val):\n",
    "    category = company_category.get(val)\n",
    "    if category:\n",
    "        color = category_colors.get(category, 'none')  # default to 'none' if no color is specified\n",
    "    else:\n",
    "        color = 'none'\n",
    "    return f'background-color: {color}'\n",
    "\n",
    "import re\n",
    "ILLEGAL_CHARACTERS_RE = re.compile(r'[\\000-\\010]|[\\013-\\014]|[\\016-\\037]')\n",
    "def find_illegal_characters(df):\n",
    "    for column in df.columns:\n",
    "        for idx, item in enumerate(df[column]):\n",
    "            if isinstance(item, str) and ILLEGAL_CHARACTERS_RE.search(item):\n",
    "                # replace illegal characters with an empty string\n",
    "                df[column][idx] = ILLEGAL_CHARACTERS_RE.sub('', item)\n",
    "    return df\n",
    "\n",
    "styled_df = df\n",
    "styled_df = find_illegal_characters(styled_df)\n",
    "styled_df = df.style.applymap(highlight_by_category)\n",
    "\n",
    "# Save the styled DataFrame to an Excel file\n",
    "# get today's date in MM-DD-YYYY format\n",
    "import datetime\n",
    "now = datetime.datetime.now()\n",
    "date = now.strftime(\"%m-%d-%Y\")\n",
    "styled_df.to_excel(f'results/{date}_{start}-{end}.xlsx', engine='openpyxl', index=False)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
