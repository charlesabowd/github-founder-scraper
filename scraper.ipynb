{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "b9e6fd8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "IS_HEADLESS = True\n",
    "LINKEDIN_EMAIL = \"jchao2001@gmail.com\"\n",
    "LINKEDIN_PASSWORD = \"Spoiler.Neurology.Primarily.Sandstorm.Laziness\"\n",
    "SALES_NAV_SEARCH_URL = \"\"\"\n",
    "https://www.linkedin.com/sales/search/people#query=(spellCorrectionEnabled%3Atrue%2CrecentSearchParam%3A(id%3A2703706810%2CdoLogHistory%3Atrue)%2Cfilters%3AList((type%3APAST_COMPANY%2Cvalues%3AList((id%3Aurn%253Ali%253Aorganization%253A1815218%2Ctext%3AUber%2CselectionType%3AINCLUDED%2Cparent%3A(id%3A0))%2C(id%3Aurn%253Ali%253Aorganization%253A309694%2Ctext%3AAirbnb%2CselectionType%3AINCLUDED%2Cparent%3A(id%3A0))%2C(id%3Aurn%253Ali%253Aorganization%253A2857634%2Ctext%3ACoinbase%2CselectionType%3AINCLUDED%2Cparent%3A(id%3A0))%2C(id%3Aurn%253Ali%253Aorganization%253A2135371%2Ctext%3AStripe%2CselectionType%3AINCLUDED%2Cparent%3A(id%3A0))%2C(id%3Aurn%253Ali%253Aorganization%253A20708%2Ctext%3APalantir%2520Technologies%2CselectionType%3AINCLUDED%2Cparent%3A(id%3A0))%2C(id%3Aurn%253Ali%253Aorganization%253A3608%2Ctext%3ANVIDIA%2CselectionType%3AINCLUDED%2Cparent%3A(id%3A0))%2C(id%3Aurn%253Ali%253Aorganization%253A3205573%2Ctext%3ADoorDash%2CselectionType%3AINCLUDED%2Cparent%3A(id%3A0))%2C(id%3Aurn%253Ali%253Aorganization%253A675562%2Ctext%3ASquare%2CselectionType%3AINCLUDED%2Cparent%3A(id%3A0))%2C(id%3Aurn%253Ali%253Aorganization%253A30846%2Ctext%3ASpaceX%2CselectionType%3AINCLUDED%2Cparent%3A(id%3A0))%2C(id%3Aurn%253Ali%253Aorganization%253A30086%2Ctext%3APalo%2520Alto%2520Networks%2CselectionType%3AINCLUDED%2Cparent%3A(id%3A0))%2C(id%3Aurn%253Ali%253Aorganization%253A3131483%2Ctext%3AFlexport%2CselectionType%3AINCLUDED%2Cparent%3A(id%3A0))%2C(id%3Aurn%253Ali%253Aorganization%253A3477522%2Ctext%3ADatabricks%2CselectionType%3AINCLUDED%2Cparent%3A(id%3A0))%2C(id%3Aurn%253Ali%253Aorganization%253A748731%2Ctext%3AKlarna%2CselectionType%3AINCLUDED%2Cparent%3A(id%3A0))%2C(id%3Aurn%253Ali%253Aorganization%253A3254263%2Ctext%3ARobinhood%2CselectionType%3AINCLUDED%2Cparent%3A(id%3A0))%2C(id%3Aurn%253Ali%253Aorganization%253A6575553%2Ctext%3AByteDance%2CselectionType%3AINCLUDED%2Cparent%3A(id%3A0))%2C(id%3Aurn%253Ali%253Aorganization%253A18505670%2Ctext%3ABrex%2CselectionType%3AINCLUDED%2Cparent%3A(id%3A0))%2C(id%3Aurn%253Ali%253Aorganization%253A2732417%2Ctext%3AInstacart%2CselectionType%3AINCLUDED%2Cparent%3A(id%3A0))%2C(id%3Aurn%253Ali%253Aorganization%253A17998520%2Ctext%3AScale%2520AI%2CselectionType%3AINCLUDED%2Cparent%3A(id%3A0))%2C(id%3Aurn%253Ali%253Aorganization%253A2684737%2Ctext%3APlaid%2CselectionType%3AINCLUDED%2Cparent%3A(id%3A0))%2C(id%3Aurn%253Ali%253Aorganization%253A3767529%2Ctext%3ANubank%2CselectionType%3AINCLUDED%2Cparent%3A(id%3A0))%2C(id%3Aurn%253Ali%253Aorganization%253A3991822%2Ctext%3AAirtable%2CselectionType%3AINCLUDED%2Cparent%3A(id%3A0))%2C(id%3Aurn%253Ali%253Aorganization%253A10043614%2Ctext%3ASnyk%2CselectionType%3AINCLUDED%2Cparent%3A(id%3A0))%2C(id%3Aurn%253Ali%253Aorganization%253A10607336%2Ctext%3AChainalysis%2CselectionType%3AINCLUDED%2Cparent%3A(id%3A0))%2C(id%3Aurn%253Ali%253Aorganization%253A10893210%2Ctext%3Adbt%2520Labs%2CselectionType%3AINCLUDED%2Cparent%3A(id%3A0))%2C(id%3Aurn%253Ali%253Aorganization%253A11062162%2Ctext%3AGrafana%2520Labs%2CselectionType%3AINCLUDED%2Cparent%3A(id%3A0))%2C(id%3Aurn%253Ali%253Aorganization%253A11130470%2Ctext%3AOpenAI%2CselectionType%3AINCLUDED%2Cparent%3A(id%3A0))%2C(id%3Aurn%253Ali%253Aorganization%253A11193683%2Ctext%3AHugging%2520Face%2CselectionType%3AINCLUDED%2Cparent%3A(id%3A0))%2C(id%3Aurn%253Ali%253Aorganization%253A11247457%2Ctext%3ASolugen%2CselectionType%3AINCLUDED%2Cparent%3A(id%3A0))%2C(id%3Aurn%253Ali%253Aorganization%253A11741116%2Ctext%3ARunway%2CselectionType%3AINCLUDED%2Cparent%3A(id%3A0))%2C(id%3Aurn%253Ali%253Aorganization%253A11869260%2Ctext%3ARetool%2CselectionType%3AINCLUDED%2Cparent%3A(id%3A0))%2C(id%3Aurn%253Ali%253Aorganization%253A1406226%2Ctext%3ARamp%2CselectionType%3AINCLUDED%2Cparent%3A(id%3A0))%2C(id%3Aurn%253Ali%253Aorganization%253A14824547%2Ctext%3AFireblocks%2CselectionType%3AINCLUDED%2Cparent%3A(id%3A0))%2C(id%3Aurn%253Ali%253Aorganization%253A1594050%2Ctext%3AGoogle%2520DeepMind%2CselectionType%3AINCLUDED%2Cparent%3A(id%3A0))%2C(id%3Aurn%253Ali%253Aorganization%253A16181286%2Ctext%3AVercel%2CselectionType%3AINCLUDED%2Cparent%3A(id%3A0))%2C(id%3Aurn%253Ali%253Aorganization%253A17932068%2Ctext%3ALacework%2CselectionType%3AINCLUDED%2Cparent%3A(id%3A0))%2C(id%3Aurn%253Ali%253Aorganization%253A17988315%2Ctext%3ARippling%2CselectionType%3AINCLUDED%2Cparent%3A(id%3A0))%2C(id%3Aurn%253Ali%253Aorganization%253A18013280%2Ctext%3AFaire%2CselectionType%3AINCLUDED%2Cparent%3A(id%3A0))%2C(id%3Aurn%253Ali%253Aorganization%253A18293159%2Ctext%3AAnduril%2520Industries%2CselectionType%3AINCLUDED%2Cparent%3A(id%3A0))%2C(id%3Aurn%253Ali%253Aorganization%253A18309569%2Ctext%3ASemgrep%2CselectionType%3AINCLUDED%2Cparent%3A(id%3A0))%2C(id%3Aurn%253Ali%253Aorganization%253A18586257%2Ctext%3AAbnormal%2520Security%2CselectionType%3AINCLUDED%2Cparent%3A(id%3A0))%2C(id%3Aurn%253Ali%253Aorganization%253A18593641%2Ctext%3AWeights%2520%2526%2520Biases%2CselectionType%3AINCLUDED%2Cparent%3A(id%3A0))%2C(id%3Aurn%253Ali%253Aorganization%253A18742807%2Ctext%3ATRM%2520Labs%2CselectionType%3AINCLUDED%2Cparent%3A(id%3A0))%2C(id%3Aurn%253Ali%253Aorganization%253A18769344%2Ctext%3AModern%2520Treasury%2CselectionType%3AINCLUDED%2Cparent%3A(id%3A0))%2C(id%3Aurn%253Ali%253Aorganization%253A18777798%2Ctext%3ACribl%2CselectionType%3AINCLUDED%2Cparent%3A(id%3A0))%2C(id%3Aurn%253Ali%253Aorganization%253A18922914%2Ctext%3ADeel%2CselectionType%3AINCLUDED%2Cparent%3A(id%3A0))%2C(id%3Aurn%253Ali%253Aorganization%253A19107985%2Ctext%3AMercury%2CselectionType%3AINCLUDED%2Cparent%3A(id%3A0))%2C(id%3Aurn%253Ali%253Aorganization%253A24024765%2Ctext%3ACohere%2CselectionType%3AINCLUDED%2Cparent%3A(id%3A0))%2C(id%3Aurn%253Ali%253Aorganization%253A2418251%2Ctext%3AZapier%2CselectionType%3AINCLUDED%2Cparent%3A(id%3A0))%2C(id%3Aurn%253Ali%253Aorganization%253A2497653%2Ctext%3ACrowdStrike%2CselectionType%3AINCLUDED%2Cparent%3A(id%3A0))%2C(id%3Aurn%253Ali%253Aorganization%253A27159855%2Ctext%3AStarburst%2CselectionType%3AINCLUDED%2Cparent%3A(id%3A0))%2C(id%3Aurn%253Ali%253Aorganization%253A2850862%2Ctext%3ACanva%2CselectionType%3AINCLUDED%2Cparent%3A(id%3A0))%2C(id%3Aurn%253Ali%253Aorganization%253A30898036%2Ctext%3ANotion%2CselectionType%3AINCLUDED%2Cparent%3A(id%3A0))%2C(id%3Aurn%253Ali%253Aorganization%253A3502352%2Ctext%3AWebflow%2CselectionType%3AINCLUDED%2Cparent%3A(id%3A0))%2C(id%3Aurn%253Ali%253Aorganization%253A35462987%2Ctext%3AVanta%2CselectionType%3AINCLUDED%2Cparent%3A(id%3A0))%2C(id%3Aurn%253Ali%253Aorganization%253A3650502%2Ctext%3AFigma%2CselectionType%3AINCLUDED%2Cparent%3A(id%3A0))%2C(id%3Aurn%253Ali%253Aorganization%253A37564254%2Ctext%3APersona%2CselectionType%3AINCLUDED%2Cparent%3A(id%3A0))%2C(id%3Aurn%253Ali%253Aorganization%253A3769390%2Ctext%3ABenchling%2CselectionType%3AINCLUDED%2Cparent%3A(id%3A0))%2C(id%3Aurn%253Ali%253Aorganization%253A3954657%2Ctext%3AFivetran%2CselectionType%3AINCLUDED%2Cparent%3A(id%3A0))%2C(id%3Aurn%253Ali%253Aorganization%253A40671813%2Ctext%3ARobust%2520Intelligence%2CselectionType%3AINCLUDED%2Cparent%3A(id%3A0))%2C(id%3Aurn%253Ali%253Aorganization%253A4803356%2Ctext%3ASourcegraph%2CselectionType%3AINCLUDED%2Cparent%3A(id%3A0))%2C(id%3Aurn%253Ali%253Aorganization%253A6424460%2Ctext%3ASentry%2520%2528sentry.io%2529%2CselectionType%3AINCLUDED%2Cparent%3A(id%3A0))%2C(id%3Aurn%253Ali%253Aorganization%253A64890982%2Ctext%3AWiz%2CselectionType%3AINCLUDED%2Cparent%3A(id%3A0))%2C(id%3Aurn%253Ali%253Aorganization%253A65281968%2Ctext%3ATecton%2CselectionType%3AINCLUDED%2Cparent%3A(id%3A0))%2C(id%3Aurn%253Ali%253Aorganization%253A65638805%2Ctext%3AMaterial%2520Security%2CselectionType%3AINCLUDED%2Cparent%3A(id%3A0))%2C(id%3Aurn%253Ali%253Aorganization%253A67081245%2Ctext%3ATemporal%2520Technologies%2CselectionType%3AINCLUDED%2Cparent%3A(id%3A0))%2C(id%3Aurn%253Ali%253Aorganization%253A68023390%2Ctext%3AIsland%2CselectionType%3AINCLUDED%2Cparent%3A(id%3A0))%2C(id%3Aurn%253Ali%253Aorganization%253A68047275%2Ctext%3AUniswap%2520Labs%2CselectionType%3AINCLUDED%2Cparent%3A(id%3A0))%2C(id%3Aurn%253Ali%253Aorganization%253A70975817%2Ctext%3AVarda%2520Space%2520Industries%2CselectionType%3AINCLUDED%2Cparent%3A(id%3A0))%2C(id%3Aurn%253Ali%253Aorganization%253A71668100%2Ctext%3AHadrian%2CselectionType%3AINCLUDED%2Cparent%3A(id%3A0))%2C(id%3Aurn%253Ali%253Aorganization%253A74126343%2Ctext%3AAnthropic%2CselectionType%3AINCLUDED%2Cparent%3A(id%3A0))%2C(id%3Aurn%253Ali%253Aorganization%253A74882602%2Ctext%3AGlean%2CselectionType%3AINCLUDED%2Cparent%3A(id%3A0))%2C(id%3Aurn%253Ali%253Aorganization%253A7602863%2Ctext%3AZipline%2CselectionType%3AINCLUDED%2Cparent%3A(id%3A0))%2C(id%3Aurn%253Ali%253Aorganization%253A76262108%2Ctext%3AKumo.AI%2CselectionType%3AINCLUDED%2Cparent%3A(id%3A0))%2C(id%3Aurn%253Ali%253Aorganization%253A80114151%2Ctext%3AClickHouse%2CselectionType%3AINCLUDED%2Cparent%3A(id%3A0))%2C(id%3Aurn%253Ali%253Aorganization%253A81330326%2Ctext%3AAdept%2CselectionType%3AINCLUDED%2Cparent%3A(id%3A0))%2C(id%3Aurn%253Ali%253Aorganization%253A81491861%2Ctext%3APredibase%2CselectionType%3AINCLUDED%2Cparent%3A(id%3A0))%2C(id%3Aurn%253Ali%253Aorganization%253A82318617%2Ctext%3AMidjourney%2CselectionType%3AINCLUDED%2Cparent%3A(id%3A0))%2C(id%3Aurn%253Ali%253Aorganization%253A83019124%2Ctext%3AEigenLayer%2CselectionType%3AINCLUDED%2Cparent%3A(id%3A0))%2C(id%3Aurn%253Ali%253Aorganization%253A89486558%2Ctext%3ACharacter.AI%2CselectionType%3AINCLUDED%2Cparent%3A(id%3A0))%2C(id%3Aurn%253Ali%253Aorganization%253A89962189%2Ctext%3AThe%2520Arbitrum%2520Foundation%2CselectionType%3AINCLUDED%2Cparent%3A(id%3A0))%2C(id%3Aurn%253Ali%253Aorganization%253A926041%2Ctext%3AOkta%2CselectionType%3AINCLUDED%2Cparent%3A(id%3A0))%2C(id%3Aurn%253Ali%253Aorganization%253A9309408%2Ctext%3ACockroach%2520Labs%2CselectionType%3AINCLUDED%2Cparent%3A(id%3A0))))%2C(type%3ALEAD_INTERACTIONS%2Cvalues%3AList((id%3ALIVP%2Ctext%3AViewed%2520profile%2CselectionType%3AEXCLUDED)%2C(id%3ALIMP%2Ctext%3AMessaged%2CselectionType%3AEXCLUDED)))%2C(type%3AFUNCTION%2Cvalues%3AList((id%3A12%2Ctext%3AHuman%2520Resources%2CselectionType%3AEXCLUDED)%2C(id%3A26%2Ctext%3ACustomer%2520Success%2520and%2520Support%2CselectionType%3AEXCLUDED)%2C(id%3A15%2Ctext%3AMarketing%2CselectionType%3AEXCLUDED)%2C(id%3A3%2Ctext%3AArts%2520and%2520Design%2CselectionType%3AEXCLUDED)%2C(id%3A1%2Ctext%3AAccounting%2CselectionType%3AEXCLUDED)%2C(id%3A2%2Ctext%3AAdministrative%2CselectionType%3AEXCLUDED)))%2C(type%3AYEARS_IN_CURRENT_POSITION%2Cvalues%3AList((id%3A1%2Ctext%3ALess%2520than%25201%2520year%2CselectionType%3AINCLUDED)))%2C(type%3APROFILE_LANGUAGE%2Cvalues%3AList((id%3Aen%2Ctext%3AEnglish%2CselectionType%3AINCLUDED))))%2Ckeywords%3A%2522something%2520new%2522%2520OR%2520%2522stealth%2522)&sessionId=stajGZpuROWIdGr%2BfkPTtA%3D%3D\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "063ef315",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "\n",
    "import os\n",
    "import random\n",
    "import time\n",
    "from linkedin_scraper import Person, actions\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.firefox.options import Options as FirefoxOptions\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "2cdabfa0",
   "metadata": {},
   "outputs": [],
   "source": [
    "options = FirefoxOptions()\n",
    "if IS_HEADLESS:\n",
    "    options.add_argument(\"--headless\")\n",
    "driver = webdriver.Firefox(options=options)\n",
    "driver.get(\"https://dev.to\")\n",
    "actions.login(driver, LINKEDIN_EMAIL, LINKEDIN_PASSWORD)\n",
    "driver.get(\"https://www.linkedin.com/sales/home\")\n",
    "\n",
    "def reinstantiate_driver(driver):\n",
    "    print(\"Reinstantiating driver...\")\n",
    "    options = FirefoxOptions()\n",
    "    options.add_argument(\"--headless\")\n",
    "    driver = webdriver.Firefox(options=options)\n",
    "    driver.get(\"https://dev.to\")\n",
    "    actions.login(driver, LINKEDIN_EMAIL, LINKEDIN_PASSWORD)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "43859799",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1293\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "\n",
    "with open('db/already_scraped.pickle', 'rb') as f:\n",
    "  scraped_urls = pickle.load(f)\n",
    "\n",
    "print(len(scraped_urls))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "027fd1fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class Experience():\n",
    "    position_title: str = None\n",
    "    from_date: str = None\n",
    "    to_date: str = None\n",
    "    description: str = None\n",
    "    position_title: str = None\n",
    "    duration: str = None\n",
    "    location: str = None\n",
    "    institution_name: str = None\n",
    "    linkedin_url: str = None\n",
    "\n",
    "class ScrapedProfile:\n",
    "    def __init__(self, profile_name, experiences, profile_school, profile_dist, profile_description, profile_link):\n",
    "        self.profile_name = profile_name\n",
    "        self.experiences = experiences\n",
    "        self.profile_school = profile_school\n",
    "        self.profile_dist = profile_dist\n",
    "        self.profile_description = profile_description\n",
    "        self.profile_link = profile_link\n",
    "\n",
    "def wait_for_element_to_load(by=By.CLASS_NAME, name=\"pv-top-card\", base=None):\n",
    "    base = base or driver\n",
    "    return WebDriverWait(base, 180).until(\n",
    "        EC.presence_of_element_located(\n",
    "            (\n",
    "                by,\n",
    "                name\n",
    "            )\n",
    "        )\n",
    "    )\n",
    "\n",
    "def wait_for_all_elements_to_load(by=By.CLASS_NAME, name=\"pv-top-card\", base=None):\n",
    "    base = base or driver\n",
    "    return WebDriverWait(base, 180).until(\n",
    "        EC.presence_of_all_elements_located(\n",
    "            (\n",
    "                by,\n",
    "                name\n",
    "            )\n",
    "        )\n",
    "    )\n",
    "\n",
    "def get_pvs_list_element(position_summary_text):\n",
    "    if not position_summary_text:\n",
    "        return None\n",
    "    \n",
    "    try:\n",
    "        return position_summary_text.find_element(By.CLASS_NAME,\"pvs-list\").find_element(By.CLASS_NAME,\"pvs-list\")\n",
    "    except:\n",
    "        return position_summary_text.find_element(By.CLASS_NAME,\"pvs-list\")\n",
    "    \n",
    "    return position_summary_text.find_element(By.CLASS_NAME,\"pvs-list\").find_element(By.CLASS_NAME,\"pvs-list\")\n",
    "\n",
    "def get_experiences(driver):\n",
    "    driver.execute_script('alert(\"Focus window\")')\n",
    "    driver.switch_to.alert.accept()\n",
    "    try:\n",
    "        WebDriverWait(driver, 240).until(lambda d: d.execute_script('return document.readyState') == 'complete')\n",
    "        main = wait_for_element_to_load(by=By.TAG_NAME, name=\"main\")\n",
    "    except:\n",
    "        driver = reinstantiate_driver(driver)\n",
    "    \n",
    "    driver.execute_script(\n",
    "                \"window.scrollTo(0, Math.ceil(document.body.scrollHeight/2));\"\n",
    "            )\n",
    "    driver.execute_script(\n",
    "                \"window.scrollTo(0, document.body.scrollHeight);\"\n",
    "            )\n",
    "\n",
    "    main_list = wait_for_element_to_load(name=\"pvs-list\", base=main)\n",
    "    experiences = []\n",
    "\n",
    "    for position in main_list.find_elements(By.XPATH,\"li\"):\n",
    "        # print(\"position\", position)\n",
    "        position = position.find_element(By.CLASS_NAME,\"pvs-entity--padded\")\n",
    "        # print(\"post-entity\")\n",
    "        company_logo_elem, position_details = position.find_elements(By.XPATH,\"*\")\n",
    "\n",
    "        # company elem\n",
    "        company_linkedin_url = company_logo_elem.find_element(By.XPATH,\"*\").get_attribute(\"href\")\n",
    "\n",
    "        # position details\n",
    "        position_details_list = position_details.find_elements(By.XPATH,\"*\")\n",
    "        position_summary_details = position_details_list[0] if len(position_details_list) > 0 else None\n",
    "        position_summary_text = position_details_list[1] if len(position_details_list) > 1 else None\n",
    "        outer_positions = position_summary_details.find_element(By.XPATH,\"*\").find_elements(By.XPATH,\"*\")\n",
    "        work_times = ''\n",
    "        \n",
    "        if len(outer_positions) == 4:\n",
    "            # position_title = outer_positions[0].find_element(By.TAG_NAME,\"span\").find_element(By.TAG_NAME,\"span\").text\n",
    "            position_title = outer_positions[0].find_element(By.TAG_NAME,\"span\").text\n",
    "            company = outer_positions[1].find_element(By.TAG_NAME,\"span\").text\n",
    "            work_times = outer_positions[2].find_element(By.TAG_NAME,\"span\").text\n",
    "            location = outer_positions[3].find_element(By.TAG_NAME,\"span\").text\n",
    "        elif len(outer_positions) == 3:\n",
    "            if \"·\" in outer_positions[2].text:\n",
    "                # position_title = outer_positions[0].find_element(By.TAG_NAME,\"span\").find_element(By.TAG_NAME,\"span\").text\n",
    "                position_title = outer_positions[0].find_element(By.TAG_NAME,\"span\").text                \n",
    "                company = outer_positions[1].find_element(By.TAG_NAME,\"span\").text\n",
    "                work_times = outer_positions[2].find_element(By.TAG_NAME,\"span\").text\n",
    "                location = \"\"\n",
    "            else:\n",
    "                position_title = \"\"\n",
    "                # company = outer_positions[0].find_element(By.TAG_NAME,\"span\").find_element(By.TAG_NAME,\"span\").text\n",
    "                company = outer_positions[0].find_element(By.TAG_NAME,\"span\").text\n",
    "                work_times = outer_positions[1].find_element(By.TAG_NAME,\"span\").text\n",
    "                location = outer_positions[2].find_element(By.TAG_NAME,\"span\").text\n",
    "        elif len(outer_positions) == 2:\n",
    "            company = outer_positions[0].text.split('\\n')[0]\n",
    "            # duration = outer_positions[1].text.split('\\n')[0].split(\"·\")[1].strip()\n",
    "\n",
    "        company = company.split(' · ')[0] # 6/14/23 added this line to handle showing \"full-time\" in company name\n",
    "        times = work_times.split(\"·\")[0].strip() if work_times else \"\"\n",
    "        duration = work_times.split(\"·\")[1].strip() if len(work_times.split(\"·\")) > 1 else None\n",
    "\n",
    "        from_date = \" \".join(times.split(\" \")[:2]) if times else \"\"\n",
    "        to_date = \" \".join(times.split(\" \")[3:]) if times else \"\"\n",
    "        \n",
    "        pvs_list_element = get_pvs_list_element(position_summary_text)\n",
    "\n",
    "        if position_summary_text and len(pvs_list_element.find_elements(By.XPATH,\"li\")) > 1:\n",
    "            descriptions = pvs_list_element.find_elements(By.XPATH,\"li\")\n",
    "            for description in descriptions:\n",
    "                res = description.find_element(By.TAG_NAME,\"a\").find_elements(By.XPATH,\"*\")\n",
    "                position_title_elem = res[0] if len(res) > 0 else None\n",
    "                work_times_elem = res[1] if len(res) > 1 else None\n",
    "                location_elem = res[2] if len(res) > 2 else None\n",
    "\n",
    "                location = location_elem.find_element(By.XPATH,\"*\").text if location_elem else None\n",
    "                position_title = position_title_elem.find_element(By.XPATH,\"*\").find_element(By.TAG_NAME,\"*\").text if position_title_elem else \"\"\n",
    "                work_times = work_times_elem.find_element(By.XPATH,\"*\").text if work_times_elem else \"\"\n",
    "                times = work_times.split(\"·\")[0].strip() if work_times else \"\"\n",
    "                duration = work_times.split(\"·\")[1].strip() if len(work_times.split(\"·\")) > 1 else None\n",
    "                from_date = \" \".join(times.split(\" \")[:2]) if times else \"\"\n",
    "                to_date = \" \".join(times.split(\" \")[3:]) if times else \"\"\n",
    "\n",
    "                experience = Experience(\n",
    "                    position_title=position_title,\n",
    "                    from_date=from_date,\n",
    "                    to_date=to_date,\n",
    "                    duration=duration,\n",
    "                    location=location,\n",
    "                    description=description,\n",
    "                    institution_name=company,\n",
    "                    linkedin_url=company_linkedin_url\n",
    "                )\n",
    "                experiences.append(experience)\n",
    "        else:\n",
    "            description = position_summary_text.text if position_summary_text else \"\"\n",
    "\n",
    "            experience = Experience(\n",
    "                position_title=position_title,\n",
    "                from_date=from_date,\n",
    "                to_date=to_date,\n",
    "                duration=duration,\n",
    "                location=location,\n",
    "                description=description,\n",
    "                institution_name=company,\n",
    "                linkedin_url=company_linkedin_url\n",
    "            )\n",
    "            experiences.append(experience)\n",
    "    return experiences\n",
    "\n",
    "def scrape_profile_live_filtering(driver, profile_link):\n",
    "    \n",
    "    experiences_url = os.path.join(profile_link, \"details/experience\")\n",
    "    print(experiences_url)\n",
    "    driver.get(experiences_url)\n",
    "    try:\n",
    "        WebDriverWait(driver, 240).until(lambda d: d.execute_script('return document.readyState') == 'complete')\n",
    "    except:\n",
    "        driver = reinstantiate_driver(driver)\n",
    "    time.sleep(5 + random.random() * 10)\n",
    "    experiences = get_experiences(driver)\n",
    "    \n",
    "    # FILTERING\n",
    "    likely_founder = False\n",
    "    relevant_exp = False\n",
    "    \n",
    "    cur_exp = experiences[0]\n",
    "    relevant_companies = [\"stealth\", \"new\"]\n",
    "    if any(company in cur_exp.institution_name.split(\" ·\")[0].lower() for company in relevant_companies) or \"present\" not in cur_exp.to_date.lower():\n",
    "        likely_founder = True\n",
    "\n",
    "#     relevant_titles = [\"product\", \"engineer\", \"sales\", \"business development\", \"founder\", \"head\", \"lead\", \"senior\", \"staff\", \"chief\", \"growth\"]\n",
    "#     for experience in experiences[1:5]:\n",
    "#         if any(title in experience.position_title.lower() for title in relevant_titles):\n",
    "#             relevant_exp = True\n",
    "#             break\n",
    "    relevant_exp = True\n",
    "\n",
    "    if not (likely_founder and relevant_exp):\n",
    "        print(likely_founder, relevant_exp)\n",
    "        return None\n",
    "    \n",
    "    person_obj = Person(profile_link, driver = driver, scrape=False, experiences = [None])\n",
    "    try:\n",
    "        WebDriverWait(driver, 240).until(lambda d: d.execute_script('return document.readyState') == 'complete')\n",
    "    except:\n",
    "        driver = reinstantiate_driver(driver)\n",
    "    time.sleep(2 + random.random() * 7)\n",
    "    \n",
    "    # name\n",
    "    profile_name = driver.find_element(By.CLASS_NAME, \"text-heading-xlarge\").text\n",
    "    time.sleep(1 + random.random())\n",
    "\n",
    "    # education\n",
    "    education = []\n",
    "    try:\n",
    "        edu_section = driver.find_element(By.ID, \"education\")\n",
    "        parent_element = edu_section.find_element(By.XPATH, \"./..\")\n",
    "        entries = parent_element.find_elements(By.CLASS_NAME, \"pvs-entity\")\n",
    "        for entry in entries:\n",
    "            elem = entry.find_elements(By.CLASS_NAME, \"visually-hidden\")\n",
    "            education.append({\"school\": elem[0].text, \"degree\": elem[1].text})\n",
    "        time.sleep(1 + random.random())\n",
    "    except:\n",
    "        print(\"education not found\")\n",
    "\n",
    "    # degree of connection\n",
    "    profile_dist = driver.find_element(By.CLASS_NAME, \"dist-value\").text\n",
    "    time.sleep(1 + random.random())\n",
    "\n",
    "    # description\n",
    "    profile_description = driver.find_element(By.CLASS_NAME, \"text-body-medium\").text\n",
    "    time.sleep(1 + random.random())\n",
    "\n",
    "    # profile link\n",
    "    profile_link = driver.current_url\n",
    "    time.sleep(1 + random.random())\n",
    "    \n",
    "    profile = ScrapedProfile(profile_name,\n",
    "                   experiences,\n",
    "                   education,\n",
    "                   profile_dist,\n",
    "                   profile_description,\n",
    "                   profile_link)\n",
    "    return profile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "5e0ea7d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    }
   ],
   "source": [
    "with open('db/to_scrape.pickle', 'rb') as f:\n",
    "  to_scrape_urls = pickle.load(f)\n",
    "print(len(to_scrape_urls))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "7c0da654",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "scrolling to profile\n",
      "clicking profile\n",
      "clicking three dots button\n",
      "getting dropdown menu\n",
      "https://www.linkedin.com/in/marcapicella\n",
      "closing popout\n",
      "scrolling to profile\n",
      "clicking profile\n",
      "clicking three dots button\n",
      "getting dropdown menu\n",
      "https://www.linkedin.com/in/michaeljwoo\n",
      "closing popout\n",
      "scrolling to profile\n",
      "clicking profile\n",
      "clicking three dots button\n",
      "getting dropdown menu\n",
      "https://www.linkedin.com/in/john-xing\n",
      "closing popout\n",
      "scrolling to profile\n",
      "clicking profile\n",
      "clicking three dots button\n",
      "getting dropdown menu\n",
      "https://www.linkedin.com/in/svennaef\n",
      "closing popout\n",
      "scrolling to profile\n",
      "clicking profile\n",
      "clicking three dots button\n",
      "getting dropdown menu\n",
      "https://www.linkedin.com/in/yash-maheshwari-cse08\n",
      "closing popout\n",
      "scrolling to profile\n",
      "clicking profile\n",
      "clicking three dots button\n",
      "getting dropdown menu\n",
      "https://www.linkedin.com/in/lancindyzhang\n",
      "closing popout\n",
      "scrolling to profile\n",
      "clicking profile\n",
      "clicking three dots button\n",
      "getting dropdown menu\n",
      "https://www.linkedin.com/in/rony-brailovsky\n",
      "closing popout\n",
      "scrolling to profile\n",
      "clicking profile\n",
      "clicking three dots button\n",
      "getting dropdown menu\n",
      "https://www.linkedin.com/in/eric-wu-29001364\n",
      "closing popout\n",
      "scrolling to profile\n",
      "clicking profile\n",
      "clicking three dots button\n",
      "getting dropdown menu\n",
      "https://www.linkedin.com/in/drakewong\n",
      "closing popout\n",
      "scrolling to profile\n",
      "clicking profile\n",
      "clicking three dots button\n",
      "getting dropdown menu\n",
      "https://www.linkedin.com/in/christianorta\n",
      "closing popout\n",
      "scrolling to profile\n",
      "clicking profile\n",
      "clicking three dots button\n",
      "getting dropdown menu\n",
      "https://www.linkedin.com/in/ula-rustamova\n",
      "closing popout\n",
      "scrolling to profile\n",
      "clicking profile\n",
      "clicking three dots button\n",
      "getting dropdown menu\n",
      "https://www.linkedin.com/in/sarveshrao\n",
      "closing popout\n",
      "scrolling to profile\n",
      "clicking profile\n",
      "clicking three dots button\n",
      "getting dropdown menu\n",
      "https://www.linkedin.com/in/donghoon-lee-0aa732111\n",
      "closing popout\n",
      "scrolling to profile\n",
      "clicking profile\n",
      "clicking three dots button\n",
      "getting dropdown menu\n",
      "https://www.linkedin.com/in/annette-obika-43259793\n",
      "closing popout\n",
      "scrolling to profile\n",
      "clicking profile\n",
      "clicking three dots button\n",
      "getting dropdown menu\n",
      "https://www.linkedin.com/in/tomshea\n",
      "closing popout\n",
      "scrolling to profile\n",
      "clicking profile\n",
      "clicking three dots button\n",
      "getting dropdown menu\n",
      "https://www.linkedin.com/in/anjali-chikkula\n",
      "closing popout\n",
      "scrolling to profile\n",
      "clicking profile\n",
      "clicking three dots button\n",
      "getting dropdown menu\n",
      "https://www.linkedin.com/in/graebe\n",
      "closing popout\n",
      "scrolling to profile\n",
      "clicking profile\n",
      "clicking three dots button\n"
     ]
    }
   ],
   "source": [
    "driver.get(SALES_NAV_SEARCH_URL)\n",
    "\n",
    "try:\n",
    "    WebDriverWait(driver, 240).until(lambda d: d.execute_script(\n",
    "        'return document.readyState') == 'complete')\n",
    "except:\n",
    "    print(\"webdriver error\")\n",
    "time.sleep(2 + random.random() * 6)\n",
    "done = False\n",
    "\n",
    "for i in range(0, 2):\n",
    "    try:\n",
    "        while True:\n",
    "            profiles = driver.find_elements(By.CLASS_NAME, \"artdeco-list__item\")\n",
    "\n",
    "            for profile in profiles:\n",
    "                # scroll to the profile\n",
    "                print(\"scrolling to profile\")\n",
    "                driver.execute_script(\"arguments[0].scrollIntoView();\", profile)\n",
    "                wait_for_element_to_load(name=\"artdeco-entity-lockup__title\")\n",
    "\n",
    "                # click the profile\n",
    "                print(\"clicking profile\")\n",
    "                salesNavOpenProfileButton = profile.find_element(By.CLASS_NAME, \"artdeco-entity-lockup__title\")\n",
    "                salesNavOpenProfileButton.click()\n",
    "                wait_for_element_to_load(name=\"_actions-container_1dg5u8\")\n",
    "                time.sleep(2 + random.random() * 6)\n",
    "                \n",
    "                try:\n",
    "                    # click the three dots button on the salesnav popout\n",
    "                    print(\"clicking three dots button\")\n",
    "                    actionContainer = driver.find_element(By.CLASS_NAME, \"_actions-container_1dg5u8\")\n",
    "                    threeDotsButton = actionContainer.find_element(By.CLASS_NAME, \"_icon_ps32ck\")\n",
    "                    threeDotsButton.click()\n",
    "                    wait_for_element_to_load(name=\"_visible_x5gf48\")\n",
    "                    time.sleep(2 + random.random() * 6)\n",
    "\n",
    "                    # get an <a> tag which is a child of dropdown menu\n",
    "                    print(\"getting dropdown menu\")\n",
    "                    dropdownContainer = driver.find_element(By.CLASS_NAME, \"_visible_x5gf48\")\n",
    "                    normalLinkedInUrl = dropdownContainer.find_elements(By.TAG_NAME, \"a\")[1].get_attribute(\"href\")\n",
    "                    wait_for_element_to_load(name=\"artdeco-pagination__button--next\")\n",
    "\n",
    "                    if (normalLinkedInUrl in scraped_urls):\n",
    "                        print(\"skipping (already scraped) \" + normalLinkedInUrl)\n",
    "                    else:\n",
    "                        to_scrape_urls.append(normalLinkedInUrl)\n",
    "                        with open('db/to_scrape.pickle', 'wb') as f:\n",
    "                            pickle.dump(to_scrape_urls, f)\n",
    "                        print(normalLinkedInUrl)\n",
    "\n",
    "                    # close the popout\n",
    "                    print(\"closing popout\")\n",
    "                    header = driver.find_element(By.CLASS_NAME, \"_inline-sidesheet-header-actions_1cn7lg\")\n",
    "                    button = header.find_elements(By.CLASS_NAME, \"_button_ps32ck\")[1]\n",
    "                    button.click()\n",
    "                except:\n",
    "                    print(\"skipping (error)\")\n",
    "\n",
    "            # navigate to next page\n",
    "            try:\n",
    "                print(\"next page\")\n",
    "                nextPageButton = driver.find_element(By.CLASS_NAME, \"artdeco-pagination__button--next\")\n",
    "                nextPageButton.click()\n",
    "                wait_for_element_to_load(name=\"artdeco-list__item\")\n",
    "            except:\n",
    "                print(\"no more pages\")\n",
    "                done = True\n",
    "                break\n",
    "            time.sleep(2 + random.random() * 6)\n",
    "        if done:\n",
    "            break\n",
    "    except:\n",
    "        print(\"looping...\")\n",
    "        time.sleep(2 + random.random() * 6)\n",
    "\n",
    "print(to_scrape_urls)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ea0fe49",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    }
   ],
   "source": [
    "# deduplicate to_scrape_urls\n",
    "with open('db/to_scrape.pickle', 'wb') as f:\n",
    "    to_scrape_urls = list(set(to_scrape_urls))\n",
    "    with open('db/already_scraped.pickle', 'rb') as f2:\n",
    "        already_scraped_urls = pickle.load(f2)\n",
    "        to_scrape_urls = [url for url in to_scrape_urls if url not in already_scraped_urls]\n",
    "        pickle.dump(to_scrape_urls, f)\n",
    "        print(len(to_scrape_urls))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92e50d5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "candidates = []\n",
    "scraped_urls = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7187f14e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "To-Scrape Urls: len:0\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import pickle\n",
    "\n",
    "with open('db/to_scrape.pickle', 'rb') as f:\n",
    "    to_scrape_urls = pickle.load(f)\n",
    "\n",
    "print(f'To-Scrape Urls: len:{len(to_scrape_urls)}')\n",
    "\n",
    "start = 0\n",
    "end = len(to_scrape_urls)\n",
    "\n",
    "for idx, url in enumerate(to_scrape_urls.copy()):\n",
    "  if start > idx:\n",
    "    continue\n",
    "  if idx >= end:\n",
    "    break\n",
    "  print(f'At index: {idx} - url: {url}') \n",
    "  \n",
    "  # scrape profiles, and write results to a file\n",
    "  try:\n",
    "    profile = scrape_profile_live_filtering(driver, url)\n",
    "    \n",
    "    if profile != None:\n",
    "        print(\"saving profile info\", end=\"\")\n",
    "        candidates.append(profile)\n",
    "\n",
    "        print(\"; recording scraped url\", end=\"\")\n",
    "        url = url.strip().strip('/')\n",
    "        scraped_urls.append(url)\n",
    "    else:\n",
    "        print(\"profile filtered out\", end=\"\")\n",
    "    \n",
    "    print(\"; removing from to-scrape\", end=\"\")\n",
    "    to_scrape_urls.remove(url)\n",
    "    with open('db/to_scrape.pickle', 'wb') as f:\n",
    "      pickle.dump(to_scrape_urls, f)\n",
    "\n",
    "    print(\"; success!\")\n",
    "    print(((idx+1)/end) * 100, '% Done - at index:', idx)\n",
    "  except Exception as e:\n",
    "    print(e)\n",
    "    print('Failed to scrape profile: ', url)\n",
    "    with open('failed_urls.txt', 'a') as f:\n",
    "      f.write(url + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11ef0fdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "for x in candidates:\n",
    "    e = x.experiences[0]\n",
    "    print(e.to_date, e.position_title)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bd44fc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.DataFrame(columns=[\"url\", \"name\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0906d83",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Previously scraped: 1293\n",
      "Newly scraped: 1293\n"
     ]
    }
   ],
   "source": [
    "def parseCandidate(x):\n",
    "    res = {}\n",
    "    res['url'] = x.profile_link\n",
    "    res['name'] = x.profile_name\n",
    "    res['dist'] = x.profile_dist\n",
    "    res['description'] = x.profile_description\n",
    "    schoolIndex = 0\n",
    "    for i, e in enumerate(x.profile_school):\n",
    "        res[f'edu{i} school'] = e[\"school\"]\n",
    "        res[f'edu{i} degree'] = e[\"degree\"]\n",
    "        schoolIndex += 1\n",
    "    exp = 0\n",
    "    for i, e in enumerate(x.experiences):\n",
    "        res[f'exp{i} title'] = e.position_title\n",
    "        res[f'exp{i} company'] = e.institution_name.split(\" ·\")[0]\n",
    "        res[f'exp{i} duration'] = e.duration\n",
    "        res[f'exp{i} start'] = e.from_date\n",
    "        exp += 1\n",
    "    return res\n",
    "\n",
    "for candidate in candidates:\n",
    "    row = parseCandidate(candidate)\n",
    "    df = pd.concat([df, pd.DataFrame([row])], ignore_index=True)\n",
    "\n",
    "# update db/already_scraped.pickle\n",
    "with open('db/already_scraped.pickle', 'rb') as f:\n",
    "    already_scraped = pickle.load(f)\n",
    "    print(f\"Previously scraped: {len(already_scraped)}\")\n",
    "    already_scraped = already_scraped + scraped_urls\n",
    "    already_scraped = list(set(already_scraped))\n",
    "    print(f\"Newly scraped: {len(already_scraped)}\")\n",
    "with open('db/already_scraped.pickle', 'wb') as f:\n",
    "    pickle.dump(already_scraped, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ee20a9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "categories = {\n",
    "    \"SECURITY\": [\n",
    "        \"Okta\",\n",
    "        \"Snyk\",\n",
    "        \"R2C/Semgrep\",\n",
    "        \"Wiz\",\n",
    "        \"Lacework\",\n",
    "        \"Crowdstrike\",\n",
    "        \"Palo Alto Networks\",\n",
    "        \"Island\",\n",
    "        \"Vanta\",\n",
    "        \"Material Security\",\n",
    "        \"Abnormal Security\"\n",
    "    ],\n",
    "    \"OTHER\": [\n",
    "        \"Figma\",\n",
    "        \"Airtable\",\n",
    "        \"Notion\",\n",
    "        \"Canva\",\n",
    "        \"Webflow\",\n",
    "        \"Faire\",\n",
    "        \"Deel\",\n",
    "        \"Rippling\",\n",
    "        \"Flexport\",\n",
    "        \"Benchling\",\n",
    "        \"Solugen\"\n",
    "    ],\n",
    "    \"PUBLIC\": [\n",
    "        \"Doordash\",\n",
    "        \"Uber\",\n",
    "        \"Palantir\",\n",
    "        \"Airbnb\",\n",
    "        \"Instacart\"\n",
    "    ],\n",
    "    \"INFRA\": [\n",
    "        \"Fivetran\",\n",
    "        \"DBT\",\n",
    "        \"Temporal\",\n",
    "        \"Cockroach Labs\",\n",
    "        \"Grafana\",\n",
    "        \"Zapier\",\n",
    "        \"Starburst\",\n",
    "        \"Retool\",\n",
    "        \"Sentry\",\n",
    "        \"Sourcegraph\",\n",
    "        \"Cribl\",\n",
    "        \"Vercel\",\n",
    "        \"Clickhouse\",\n",
    "        \"Github,\"\n",
    "    ],\n",
    "    \"FINTECH\": [\n",
    "        \"Robinhood\",\n",
    "        \"Square\",\n",
    "        \"Stripe\",\n",
    "        \"Ramp\",\n",
    "        \"Brex\",\n",
    "        \"Plaid\",\n",
    "        \"Modern Treasury\",\n",
    "        \"Mercury\",\n",
    "        \"Persona\",\n",
    "        \"Klarna\",\n",
    "        \"Nubank\"\n",
    "    ],\n",
    "    \"CRYPTO\": [\n",
    "        \"Coinbase\",\n",
    "        \"Uniswap\",\n",
    "        \"Chainalysis\",\n",
    "        \"Arbitrum\",\n",
    "        \"TRM\",\n",
    "        \"Fireblocks\",\n",
    "        \"Eigenlayer\"\n",
    "    ],\n",
    "    \"FRONTIER\": [\n",
    "        \"Anduril\",\n",
    "        \"SpaceX\",\n",
    "        \"Zipline\",\n",
    "        \"Varda\",\n",
    "        \"Hadrian\"\n",
    "    ],\n",
    "    \"AI\": [\n",
    "        \"Bytedance\",\n",
    "        \"Scale AI\",\n",
    "        \"Anthropic\",\n",
    "        \"Robust intelligence\",\n",
    "        \"OpenAI\",\n",
    "        \"Predibase\",\n",
    "        \"Cohere\",\n",
    "        \"Databricks\",\n",
    "        \"Hugging Face\",\n",
    "        \"RunwayML\",\n",
    "        \"Tecton\",\n",
    "        \"Weights & Biases\",\n",
    "        \"Kumo AI\",\n",
    "        \"NVIDIA\",\n",
    "        \"Adept\",\n",
    "        \"Glean\",\n",
    "        \"Character.ai\",\n",
    "        \"Midjourney\",\n",
    "        \"Facebook AI\",\n",
    "        \"FAIR\",\n",
    "        \"Google brain\"\n",
    "    ]\n",
    "}\n",
    "\n",
    "# Example color mapping for categories\n",
    "category_colors = {\n",
    "    \"SECURITY\": 'red',\n",
    "    \"OTHER\": 'blue',\n",
    "    \"PUBLIC\": 'green',\n",
    "    \"INFRA\": 'yellow',\n",
    "    \"FINTECH\": 'orange',\n",
    "    \"CRYPTO\": 'purple',\n",
    "    \"FRONTIER\": 'cyan',\n",
    "    \"AI\": 'magenta'\n",
    "}\n",
    "\n",
    "# Create a reverse dictionary for easier lookup: {company: category}\n",
    "company_category = {}\n",
    "for category, companies in categories.items():\n",
    "    for company in companies:\n",
    "        company_category[company] = category\n",
    "\n",
    "# Modify the style function\n",
    "def highlight_by_category(val):\n",
    "    category = company_category.get(val)\n",
    "    if category:\n",
    "        color = category_colors.get(category, 'none')  # default to 'none' if no color is specified\n",
    "    else:\n",
    "        color = 'none'\n",
    "    return f'background-color: {color}'\n",
    "\n",
    "import re\n",
    "ILLEGAL_CHARACTERS_RE = re.compile(r'[\\000-\\010]|[\\013-\\014]|[\\016-\\037]')\n",
    "def find_illegal_characters(df):\n",
    "    for column in df.columns:\n",
    "        for idx, item in enumerate(df[column]):\n",
    "            if isinstance(item, str) and ILLEGAL_CHARACTERS_RE.search(item):\n",
    "                # replace illegal characters with an empty string\n",
    "                df[column][idx] = ILLEGAL_CHARACTERS_RE.sub('', item)\n",
    "    return df\n",
    "\n",
    "styled_df = df\n",
    "styled_df = find_illegal_characters(styled_df)\n",
    "styled_df = df.style.applymap(highlight_by_category)\n",
    "\n",
    "# Save the styled DataFrame to an Excel file\n",
    "# get today's date in MM-DD-YYYY format\n",
    "import datetime\n",
    "now = datetime.datetime.now()\n",
    "date = now.strftime(\"%m-%d-%Y\")\n",
    "styled_df.to_excel(f'results/{date}_{start}-{end}.xlsx', engine='openpyxl', index=False)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
