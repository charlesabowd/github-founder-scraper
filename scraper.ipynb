{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5b930f8e",
   "metadata": {},
   "source": [
    "#  SETUP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9e6fd8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# if the script finishes very quickly (and generates an empty excel file), click run again\n",
    "# if the script errors on the \"Login Cell\" (added a comment to indicate which cell that is below), set IS_HEADLESS to \"False\" and run again. The scraper will automatically launch a page and attempt to login to LinkedIn. It's likely erroring because LinkedIn is asking for a captcha to verify the user is not a bot. Solve the captch/challenge and login. Once successfully logged in, set IS_HEADLESS back to \"True\" and run again.\n",
    "\n",
    "IS_HEADLESS = False\n",
    "# CHANGE THIS TO YOUR LINKEDIN ACCOUNT INFO\n",
    "LINKEDIN_EMAIL = \"XXXXXX\"\n",
    "LINKEDIN_PASSWORD = \"XXXXXXX\"\n",
    "COMPANY_CATEGORIES = {\n",
    "    \"SECURITY\": [\n",
    "        \"Okta\",\n",
    "        \"Snyk\",\n",
    "        \"R2C/Semgrep\", \n",
    "        \"Wiz\",\n",
    "        \"Lacework\",\n",
    "        \"Crowdstrike\",\n",
    "        \"Palo Alto Networks\",\n",
    "        \"Island\",\n",
    "        \"Vanta\",\n",
    "        \"Material Security\",\n",
    "        \"Abnormal Security\",\n",
    "        \"Samsara\",\n",
    "    ],\n",
    "    \"OTHER\": [\n",
    "        \"Figma\",\n",
    "        \"Airtable\",\n",
    "        \"Notion\",\n",
    "        \"Canva\",\n",
    "        \"Webflow\",\n",
    "        \"Faire\",\n",
    "        \"Deel\",\n",
    "        \"Rippling\",\n",
    "        \"Flexport\",\n",
    "        \"Benchling\",\n",
    "        \"Solugen\"\n",
    "    ],\n",
    "    \"PUBLIC\": [\n",
    "        \"Doordash\",\n",
    "        \"Uber\",\n",
    "        \"Palantir\",\n",
    "        \"Airbnb\",\n",
    "        \"Instacart\"\n",
    "    ],\n",
    "    \"INFRA\": [\n",
    "        \"Fivetran\",\n",
    "        \"DBT\",\n",
    "        \"Temporal\",\n",
    "        \"Cockroach Labs\",\n",
    "        \"Grafana\",\n",
    "        \"Zapier\",\n",
    "        \"Starburst\",\n",
    "        \"Retool\",\n",
    "        \"Sentry\",\n",
    "        \"Sourcegraph\",\n",
    "        \"Cribl\",\n",
    "        \"Vercel\",\n",
    "        \"Clickhouse\",\n",
    "        \"Github,\"\n",
    "        \"Cisco Meraki\",\n",
    "    ],\n",
    "    \"FINTECH\": [\n",
    "        \"Robinhood\",\n",
    "        \"Square\",\n",
    "        \"Stripe\",\n",
    "        \"Ramp\",\n",
    "        \"Brex\",\n",
    "        \"Plaid\",\n",
    "        \"Modern Treasury\",\n",
    "        \"Mercury\",\n",
    "        \"Persona\",\n",
    "        \"Klarna\",\n",
    "        \"Nubank\"\n",
    "    ],\n",
    "    \"CRYPTO\": [\n",
    "        \"Coinbase\",\n",
    "        \"Uniswap\",\n",
    "        \"Chainalysis\",\n",
    "        \"Arbitrum\",\n",
    "        \"TRM\",\n",
    "        \"Fireblocks\",\n",
    "        \"Eigenlayer\"\n",
    "    ],\n",
    "    \"FRONTIER\": [\n",
    "        \"Anduril\",\n",
    "        \"SpaceX\",\n",
    "        \"Zipline\",\n",
    "        \"Varda\",\n",
    "        \"Hadrian\"\n",
    "    ],\n",
    "    \"AI\": [\n",
    "        \"Bytedance\",\n",
    "        \"Scale AI\",\n",
    "        \"Anthropic\",\n",
    "        \"Robust intelligence\",\n",
    "        \"OpenAI\",\n",
    "        \"Predibase\",\n",
    "        \"Cohere\",\n",
    "        \"Databricks\",\n",
    "        \"Hugging Face\",\n",
    "        \"RunwayML\",\n",
    "        \"Tecton\",\n",
    "        \"Weights & Biases\",\n",
    "        \"Kumo AI\",\n",
    "        \"NVIDIA\",\n",
    "        \"Adept\",\n",
    "        \"Glean\",\n",
    "        \"Character.ai\",\n",
    "        \"Midjourney\",\n",
    "        \"Facebook AI\",\n",
    "        \"FAIR\",\n",
    "        \"Google brain\"\n",
    "    ]\n",
    "}\n",
    "SALES_NAV_SEARCH_URL = \"\"\"\n",
    "https://www.linkedin.com/sales/search/people#coach=false&query=(spellCorrectionEnabled%3Atrue%2CrecentSearchParam%3A(doLogHistory%3Afalse)%2Cfilters%3AList((type%3APAST_COMPANY%2Cvalues%3AList((id%3Aurn%253Ali%253Aorganization%253A1815218%2Ctext%3AUber%2CselectionType%3AINCLUDED%2Cparent%3A(id%3A0))%2C(id%3Aurn%253Ali%253Aorganization%253A309694%2Ctext%3AAirbnb%2CselectionType%3AINCLUDED%2Cparent%3A(id%3A0))%2C(id%3Aurn%253Ali%253Aorganization%253A2857634%2Ctext%3ACoinbase%2CselectionType%3AINCLUDED%2Cparent%3A(id%3A0))%2C(id%3Aurn%253Ali%253Aorganization%253A2135371%2Ctext%3AStripe%2CselectionType%3AINCLUDED%2Cparent%3A(id%3A0))%2C(id%3Aurn%253Ali%253Aorganization%253A20708%2Ctext%3APalantir%2520Technologies%2CselectionType%3AINCLUDED%2Cparent%3A(id%3A0))%2C(id%3Aurn%253Ali%253Aorganization%253A3608%2Ctext%3ANVIDIA%2CselectionType%3AINCLUDED%2Cparent%3A(id%3A0))%2C(id%3Aurn%253Ali%253Aorganization%253A3205573%2Ctext%3ADoorDash%2CselectionType%3AINCLUDED%2Cparent%3A(id%3A0))%2C(id%3Aurn%253Ali%253Aorganization%253A675562%2Ctext%3ASquare%2CselectionType%3AINCLUDED%2Cparent%3A(id%3A0))%2C(id%3Aurn%253Ali%253Aorganization%253A30846%2Ctext%3ASpaceX%2CselectionType%3AINCLUDED%2Cparent%3A(id%3A0))%2C(id%3Aurn%253Ali%253Aorganization%253A30086%2Ctext%3APalo%2520Alto%2520Networks%2CselectionType%3AINCLUDED%2Cparent%3A(id%3A0))%2C(id%3Aurn%253Ali%253Aorganization%253A3131483%2Ctext%3AFlexport%2CselectionType%3AINCLUDED%2Cparent%3A(id%3A0))%2C(id%3Aurn%253Ali%253Aorganization%253A3477522%2Ctext%3ADatabricks%2CselectionType%3AINCLUDED%2Cparent%3A(id%3A0))%2C(id%3Aurn%253Ali%253Aorganization%253A748731%2Ctext%3AKlarna%2CselectionType%3AINCLUDED%2Cparent%3A(id%3A0))%2C(id%3Aurn%253Ali%253Aorganization%253A3254263%2Ctext%3ARobinhood%2CselectionType%3AINCLUDED%2Cparent%3A(id%3A0))%2C(id%3Aurn%253Ali%253Aorganization%253A6575553%2Ctext%3AByteDance%2CselectionType%3AINCLUDED%2Cparent%3A(id%3A0))%2C(id%3Aurn%253Ali%253Aorganization%253A18505670%2Ctext%3ABrex%2CselectionType%3AINCLUDED%2Cparent%3A(id%3A0))%2C(id%3Aurn%253Ali%253Aorganization%253A2732417%2Ctext%3AInstacart%2CselectionType%3AINCLUDED%2Cparent%3A(id%3A0))%2C(id%3Aurn%253Ali%253Aorganization%253A17998520%2Ctext%3AScale%2520AI%2CselectionType%3AINCLUDED%2Cparent%3A(id%3A0))%2C(id%3Aurn%253Ali%253Aorganization%253A2684737%2Ctext%3APlaid%2CselectionType%3AINCLUDED%2Cparent%3A(id%3A0))%2C(id%3Aurn%253Ali%253Aorganization%253A3767529%2Ctext%3ANubank%2CselectionType%3AINCLUDED%2Cparent%3A(id%3A0))%2C(id%3Aurn%253Ali%253Aorganization%253A3991822%2Ctext%3AAirtable%2CselectionType%3AINCLUDED%2Cparent%3A(id%3A0))%2C(id%3Aurn%253Ali%253Aorganization%253A10043614%2Ctext%3ASnyk%2CselectionType%3AINCLUDED%2Cparent%3A(id%3A0))%2C(id%3Aurn%253Ali%253Aorganization%253A10607336%2Ctext%3AChainalysis%2CselectionType%3AINCLUDED%2Cparent%3A(id%3A0))%2C(id%3Aurn%253Ali%253Aorganization%253A10893210%2Ctext%3Adbt%2520Labs%2CselectionType%3AINCLUDED%2Cparent%3A(id%3A0))%2C(id%3Aurn%253Ali%253Aorganization%253A11062162%2Ctext%3AGrafana%2520Labs%2CselectionType%3AINCLUDED%2Cparent%3A(id%3A0))%2C(id%3Aurn%253Ali%253Aorganization%253A11130470%2Ctext%3AOpenAI%2CselectionType%3AINCLUDED%2Cparent%3A(id%3A0))%2C(id%3Aurn%253Ali%253Aorganization%253A11193683%2Ctext%3AHugging%2520Face%2CselectionType%3AINCLUDED%2Cparent%3A(id%3A0))%2C(id%3Aurn%253Ali%253Aorganization%253A11247457%2Ctext%3ASolugen%2CselectionType%3AINCLUDED%2Cparent%3A(id%3A0))%2C(id%3Aurn%253Ali%253Aorganization%253A11741116%2Ctext%3ARunway%2CselectionType%3AINCLUDED%2Cparent%3A(id%3A0))%2C(id%3Aurn%253Ali%253Aorganization%253A11869260%2Ctext%3ARetool%2CselectionType%3AINCLUDED%2Cparent%3A(id%3A0))%2C(id%3Aurn%253Ali%253Aorganization%253A1406226%2Ctext%3ARamp%2CselectionType%3AINCLUDED%2Cparent%3A(id%3A0))%2C(id%3Aurn%253Ali%253Aorganization%253A14824547%2Ctext%3AFireblocks%2CselectionType%3AINCLUDED%2Cparent%3A(id%3A0))%2C(id%3Aurn%253Ali%253Aorganization%253A1594050%2Ctext%3AGoogle%2520DeepMind%2CselectionType%3AINCLUDED%2Cparent%3A(id%3A0))%2C(id%3Aurn%253Ali%253Aorganization%253A16181286%2Ctext%3AVercel%2CselectionType%3AINCLUDED%2Cparent%3A(id%3A0))%2C(id%3Aurn%253Ali%253Aorganization%253A17932068%2Ctext%3ALacework%2CselectionType%3AINCLUDED%2Cparent%3A(id%3A0))%2C(id%3Aurn%253Ali%253Aorganization%253A17988315%2Ctext%3ARippling%2CselectionType%3AINCLUDED%2Cparent%3A(id%3A0))%2C(id%3Aurn%253Ali%253Aorganization%253A18013280%2Ctext%3AFaire%2CselectionType%3AINCLUDED%2Cparent%3A(id%3A0))%2C(id%3Aurn%253Ali%253Aorganization%253A18293159%2Ctext%3AAnduril%2520Industries%2CselectionType%3AINCLUDED%2Cparent%3A(id%3A0))%2C(id%3Aurn%253Ali%253Aorganization%253A18309569%2Ctext%3ASemgrep%2CselectionType%3AINCLUDED%2Cparent%3A(id%3A0))%2C(id%3Aurn%253Ali%253Aorganization%253A18586257%2Ctext%3AAbnormal%2520Security%2CselectionType%3AINCLUDED%2Cparent%3A(id%3A0))%2C(id%3Aurn%253Ali%253Aorganization%253A18593641%2Ctext%3AWeights%2520%2526%2520Biases%2CselectionType%3AINCLUDED%2Cparent%3A(id%3A0))%2C(id%3Aurn%253Ali%253Aorganization%253A18742807%2Ctext%3ATRM%2520Labs%2CselectionType%3AINCLUDED%2Cparent%3A(id%3A0))%2C(id%3Aurn%253Ali%253Aorganization%253A18769344%2Ctext%3AModern%2520Treasury%2CselectionType%3AINCLUDED%2Cparent%3A(id%3A0))%2C(id%3Aurn%253Ali%253Aorganization%253A18777798%2Ctext%3ACribl%2CselectionType%3AINCLUDED%2Cparent%3A(id%3A0))%2C(id%3Aurn%253Ali%253Aorganization%253A18922914%2Ctext%3ADeel%2CselectionType%3AINCLUDED%2Cparent%3A(id%3A0))%2C(id%3Aurn%253Ali%253Aorganization%253A19107985%2Ctext%3AMercury%2CselectionType%3AINCLUDED%2Cparent%3A(id%3A0))%2C(id%3Aurn%253Ali%253Aorganization%253A24024765%2Ctext%3ACohere%2CselectionType%3AINCLUDED%2Cparent%3A(id%3A0))%2C(id%3Aurn%253Ali%253Aorganization%253A2418251%2Ctext%3AZapier%2CselectionType%3AINCLUDED%2Cparent%3A(id%3A0))%2C(id%3Aurn%253Ali%253Aorganization%253A2497653%2Ctext%3ACrowdStrike%2CselectionType%3AINCLUDED%2Cparent%3A(id%3A0))%2C(id%3Aurn%253Ali%253Aorganization%253A27159855%2Ctext%3AStarburst%2CselectionType%3AINCLUDED%2Cparent%3A(id%3A0))%2C(id%3Aurn%253Ali%253Aorganization%253A2850862%2Ctext%3ACanva%2CselectionType%3AINCLUDED%2Cparent%3A(id%3A0))%2C(id%3Aurn%253Ali%253Aorganization%253A30898036%2Ctext%3ANotion%2CselectionType%3AINCLUDED%2Cparent%3A(id%3A0))%2C(id%3Aurn%253Ali%253Aorganization%253A3502352%2Ctext%3AWebflow%2CselectionType%3AINCLUDED%2Cparent%3A(id%3A0))%2C(id%3Aurn%253Ali%253Aorganization%253A35462987%2Ctext%3AVanta%2CselectionType%3AINCLUDED%2Cparent%3A(id%3A0))%2C(id%3Aurn%253Ali%253Aorganization%253A3650502%2Ctext%3AFigma%2CselectionType%3AINCLUDED%2Cparent%3A(id%3A0))%2C(id%3Aurn%253Ali%253Aorganization%253A37564254%2Ctext%3APersona%2CselectionType%3AINCLUDED%2Cparent%3A(id%3A0))%2C(id%3Aurn%253Ali%253Aorganization%253A3769390%2Ctext%3ABenchling%2CselectionType%3AINCLUDED%2Cparent%3A(id%3A0))%2C(id%3Aurn%253Ali%253Aorganization%253A3954657%2Ctext%3AFivetran%2CselectionType%3AINCLUDED%2Cparent%3A(id%3A0))%2C(id%3Aurn%253Ali%253Aorganization%253A40671813%2Ctext%3ARobust%2520Intelligence%2CselectionType%3AINCLUDED%2Cparent%3A(id%3A0))%2C(id%3Aurn%253Ali%253Aorganization%253A4803356%2Ctext%3ASourcegraph%2CselectionType%3AINCLUDED%2Cparent%3A(id%3A0))%2C(id%3Aurn%253Ali%253Aorganization%253A6424460%2Ctext%3ASentry%2520%2528sentry.io%2529%2CselectionType%3AINCLUDED%2Cparent%3A(id%3A0))%2C(id%3Aurn%253Ali%253Aorganization%253A64890982%2Ctext%3AWiz%2CselectionType%3AINCLUDED%2Cparent%3A(id%3A0))%2C(id%3Aurn%253Ali%253Aorganization%253A65281968%2Ctext%3ATecton%2CselectionType%3AINCLUDED%2Cparent%3A(id%3A0))%2C(id%3Aurn%253Ali%253Aorganization%253A65638805%2Ctext%3AMaterial%2520Security%2CselectionType%3AINCLUDED%2Cparent%3A(id%3A0))%2C(id%3Aurn%253Ali%253Aorganization%253A67081245%2Ctext%3ATemporal%2520Technologies%2CselectionType%3AINCLUDED%2Cparent%3A(id%3A0))%2C(id%3Aurn%253Ali%253Aorganization%253A68023390%2Ctext%3AIsland%2CselectionType%3AINCLUDED%2Cparent%3A(id%3A0))%2C(id%3Aurn%253Ali%253Aorganization%253A68047275%2Ctext%3AUniswap%2520Labs%2CselectionType%3AINCLUDED%2Cparent%3A(id%3A0))%2C(id%3Aurn%253Ali%253Aorganization%253A70975817%2Ctext%3AVarda%2520Space%2520Industries%2CselectionType%3AINCLUDED%2Cparent%3A(id%3A0))%2C(id%3Aurn%253Ali%253Aorganization%253A71668100%2Ctext%3AHadrian%2CselectionType%3AINCLUDED%2Cparent%3A(id%3A0))%2C(id%3Aurn%253Ali%253Aorganization%253A74126343%2Ctext%3AAnthropic%2CselectionType%3AINCLUDED%2Cparent%3A(id%3A0))%2C(id%3Aurn%253Ali%253Aorganization%253A74882602%2Ctext%3AGlean%2CselectionType%3AINCLUDED%2Cparent%3A(id%3A0))%2C(id%3Aurn%253Ali%253Aorganization%253A7602863%2Ctext%3AZipline%2CselectionType%3AINCLUDED%2Cparent%3A(id%3A0))%2C(id%3Aurn%253Ali%253Aorganization%253A76262108%2Ctext%3AKumo.AI%2CselectionType%3AINCLUDED%2Cparent%3A(id%3A0))%2C(id%3Aurn%253Ali%253Aorganization%253A80114151%2Ctext%3AClickHouse%2CselectionType%3AINCLUDED%2Cparent%3A(id%3A0))%2C(id%3Aurn%253Ali%253Aorganization%253A81330326%2Ctext%3AAdept%2CselectionType%3AINCLUDED%2Cparent%3A(id%3A0))%2C(id%3Aurn%253Ali%253Aorganization%253A81491861%2Ctext%3APredibase%2CselectionType%3AINCLUDED%2Cparent%3A(id%3A0))%2C(id%3Aurn%253Ali%253Aorganization%253A82318617%2Ctext%3AMidjourney%2CselectionType%3AINCLUDED%2Cparent%3A(id%3A0))%2C(id%3Aurn%253Ali%253Aorganization%253A83019124%2Ctext%3AEigen%2520Labs%2CselectionType%3AINCLUDED%2Cparent%3A(id%3A0))%2C(id%3Aurn%253Ali%253Aorganization%253A89486558%2Ctext%3ACharacter.AI%2CselectionType%3AINCLUDED%2Cparent%3A(id%3A0))%2C(id%3Aurn%253Ali%253Aorganization%253A89962189%2Ctext%3AThe%2520Arbitrum%2520Foundation%2CselectionType%3AINCLUDED%2Cparent%3A(id%3A0))%2C(id%3Aurn%253Ali%253Aorganization%253A926041%2Ctext%3AOkta%2CselectionType%3AINCLUDED%2Cparent%3A(id%3A0))%2C(id%3Aurn%253Ali%253Aorganization%253A9309408%2Ctext%3ACockroach%2520Labs%2CselectionType%3AINCLUDED%2Cparent%3A(id%3A0))%2C(id%3Aurn%253Ali%253Aorganization%253A6453825%2Ctext%3ASamsara%2CselectionType%3AINCLUDED%2Cparent%3A(id%3A0))%2C(id%3Aurn%253Ali%253Aorganization%253A92950%2Ctext%3ACisco%2520Meraki%2CselectionType%3AINCLUDED%2Cparent%3A(id%3A0))%2C(id%3Aurn%253Ali%253Aorganization%253A101156365%2Ctext%3AManta%2520Network%2520%2528MANTA%2529%2CselectionType%3AINCLUDED%2Cparent%3A(id%3A0))%2C(id%3Aurn%253Ali%253Aorganization%253A19104188%2Ctext%3AAvalanche%2CselectionType%3AINCLUDED%2Cparent%3A(id%3A0))%2C(id%3Aurn%253Ali%253Aorganization%253A81901372%2Ctext%3AOptimism%2520Foundation%2CselectionType%3AINCLUDED%2Cparent%3A(id%3A0))%2C(id%3Aurn%253Ali%253Aorganization%253A13449964%2Ctext%3APolygon%2520Labs%2CselectionType%3AINCLUDED%2Cparent%3A(id%3A0))%2C(id%3Aurn%253Ali%253Aorganization%253A72057622%2Ctext%3ASolana%2520Foundation%2CselectionType%3AINCLUDED%2Cparent%3A(id%3A0))%2C(id%3Aurn%253Ali%253Aorganization%253A18453134%2Ctext%3ASolana%2520Labs%2CselectionType%3AINCLUDED%2Cparent%3A(id%3A0))%2C(id%3Aurn%253Ali%253Aorganization%253A40769465%2Ctext%3AEthereum%2520Foundation%2CselectionType%3AINCLUDED%2Cparent%3A(id%3A0))%2C(id%3Aurn%253Ali%253Aorganization%253A89970028%2Ctext%3ASui%2520Foundation%2CselectionType%3AINCLUDED%2Cparent%3A(id%3A0))%2C(id%3Aurn%253Ali%253Aorganization%253A27137803%2Ctext%3AStarkWare%2CselectionType%3AINCLUDED%2Cparent%3A(id%3A0))%2C(id%3Aurn%253Ali%253Aorganization%253A74341323%2Ctext%3AOsmosis%2520Labs%2CselectionType%3AINCLUDED%2Cparent%3A(id%3A0))%2C(id%3Aurn%253Ali%253Aorganization%253A79119792%2Ctext%3AAptos%2520Labs%2CselectionType%3AINCLUDED%2Cparent%3A(id%3A0))%2C(id%3Aurn%253Ali%253Aorganization%253A71528119%2Ctext%3ANEAR%2520Protocol%2CselectionType%3AINCLUDED%2Cparent%3A(id%3A0))%2C(id%3Aurn%253Ali%253Aorganization%253A40708618%2Ctext%3AInjective%2520Labs%2CselectionType%3AINCLUDED%2Cparent%3A(id%3A0))%2C(id%3Aurn%253Ali%253Aorganization%253A76174015%2Ctext%3AMysten%2520Labs%2CselectionType%3AINCLUDED%2Cparent%3A(id%3A0))%2C(id%3Aurn%253Ali%253Aorganization%253A94854888%2Ctext%3ASei%2520Network%2CselectionType%3AINCLUDED%2Cparent%3A(id%3A0))%2C(id%3Aurn%253Ali%253Aorganization%253A81462746%2Ctext%3ABabylonChain%2520Inc.%2CselectionType%3AINCLUDED%2Cparent%3A(id%3A0))%2C(id%3Aurn%253Ali%253Aorganization%253A84802787%2Ctext%3AMonad%2520Labs%2CselectionType%3AINCLUDED%2Cparent%3A(id%3A0))%2C(id%3Aurn%253Ali%253Aorganization%253A100996178%2Ctext%3ARitual%2CselectionType%3AINCLUDED%2Cparent%3A(id%3A0))%2C(id%3Aurn%253Ali%253Aorganization%253A28405403%2Ctext%3ACelestia%2520Labs%2CselectionType%3AINCLUDED%2Cparent%3A(id%3A0))%2C(id%3Aurn%253Ali%253Aorganization%253A82157982%2Ctext%3AAltLayer%2CselectionType%3AINCLUDED%2Cparent%3A(id%3A0))%2C(id%3Aurn%253Ali%253Aorganization%253A75574122%2Ctext%3AFlashbots%2CselectionType%3AINCLUDED%2Cparent%3A(id%3A0))%2C(id%3Aurn%253Ali%253Aorganization%253A69266957%2Ctext%3AAxelar%2520Network%2CselectionType%3AINCLUDED%2Cparent%3A(id%3A0))%2C(id%3Aurn%253Ali%253Aorganization%253A80940997%2Ctext%3AAxelar%2520Foundation%2CselectionType%3AINCLUDED%2Cparent%3A(id%3A0))%2C(id%3Aurn%253Ali%253Aorganization%253A16181958%2Ctext%3AProtocol%2520Labs%2CselectionType%3AINCLUDED%2Cparent%3A(id%3A0))%2C(id%3Aurn%253Ali%253Aorganization%253A89707979%2Ctext%3AEclipse%2520Labs%2CselectionType%3AINCLUDED%2Cparent%3A(id%3A0))%2C(id%3Aurn%253Ali%253Aorganization%253A79159372%2Ctext%3AEspresso%2520Systems%2CselectionType%3AINCLUDED%2Cparent%3A(id%3A0))%2C(id%3Aurn%253Ali%253Aorganization%253A42785598%2Ctext%3AAleo%2CselectionType%3AINCLUDED%2Cparent%3A(id%3A0))%2C(id%3Aurn%253Ali%253Aorganization%253A86632358%2Ctext%3ALido%2520Finance%2CselectionType%3AINCLUDED%2Cparent%3A(id%3A0))%2C(id%3Aurn%253Ali%253Aorganization%253A75654012%2Ctext%3ALayerZero%2520Labs%2CselectionType%3AINCLUDED%2Cparent%3A(id%3A0))%2C(id%3Aurn%253Ali%253Aorganization%253A79296193%2Ctext%3ACompound%2520Labs%2CselectionType%3AINCLUDED%2Cparent%3A(id%3A0))%2C(id%3Aurn%253Ali%253Aorganization%253A18264732%2Ctext%3AdYdX%2CselectionType%3AINCLUDED%2Cparent%3A(id%3A0))%2C(id%3Aurn%253Ali%253Aorganization%253A79948926%2Ctext%3AdYdX%2520Foundation%2CselectionType%3AINCLUDED)))%2C(type%3ALEAD_INTERACTIONS%2Cvalues%3AList((id%3ALIVP%2Ctext%3AViewed%2520profile%2CselectionType%3AEXCLUDED)%2C(id%3ALIMP%2Ctext%3AMessaged%2CselectionType%3AEXCLUDED)))%2C(type%3AFUNCTION%2Cvalues%3AList((id%3A12%2Ctext%3AHuman%2520Resources%2CselectionType%3AEXCLUDED)%2C(id%3A26%2Ctext%3ACustomer%2520Success%2520and%2520Support%2CselectionType%3AEXCLUDED)%2C(id%3A15%2Ctext%3AMarketing%2CselectionType%3AEXCLUDED)%2C(id%3A3%2Ctext%3AArts%2520and%2520Design%2CselectionType%3AEXCLUDED)%2C(id%3A1%2Ctext%3AAccounting%2CselectionType%3AEXCLUDED)%2C(id%3A2%2Ctext%3AAdministrative%2CselectionType%3AEXCLUDED)))%2C(type%3AYEARS_IN_CURRENT_POSITION%2Cvalues%3AList((id%3A1%2Ctext%3ALess%2520than%25201%2520year%2CselectionType%3AINCLUDED)))%2C(type%3APROFILE_LANGUAGE%2Cvalues%3AList((id%3Aen%2Ctext%3AEnglish%2CselectionType%3AINCLUDED))))%2Ckeywords%3A%2522something%2520new%2522%2520OR%2520%2522stealth%2522)&sessionId=stajGZpuROWIdGr%2BfkPTtA%3D%3D\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "063ef315",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "\n",
    "import os\n",
    "import random\n",
    "import time\n",
    "from linkedin_scraper import actions\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.firefox.options import Options as FirefoxOptions\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.common.exceptions import TimeoutException, StaleElementReferenceException, NoSuchElementException, ElementClickInterceptedException"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7957b23f",
   "metadata": {},
   "source": [
    "# LOGIN CELL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cdabfa0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# IF THIS CELL ERRORS DUE TO CAPTCHA, do this:\n",
    "# manually complete the captcha, click on the next cell, and select Run menu, select \"run selected cell and all below\" \n",
    "\n",
    "options = FirefoxOptions()\n",
    "if IS_HEADLESS:\n",
    "    options.add_argument(\"--headless\")\n",
    "driver = webdriver.Firefox(options=options)\n",
    "actions.login(driver, LINKEDIN_EMAIL, LINKEDIN_PASSWORD)\n",
    "driver.get(\"https://www.linkedin.com/sales/home\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3596a5f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def reinstantiate_driver():\n",
    "    print(\"Reinstantiating driver...\")\n",
    "    driver = webdriver.Firefox(options=options)\n",
    "    actions.login(driver, LINKEDIN_EMAIL, LINKEDIN_PASSWORD)\n",
    "    driver.get(\"https://www.linkedin.com/sales/home\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "314afe60-1439-4d2d-9af6-f60c08c79f5a",
   "metadata": {},
   "source": [
    "# open pickle, store already scraped urls in scraped_urls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43859799",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "with open('db/already_scraped.pickle', 'rb') as f:\n",
    "  scraped_urls = pickle.load(f)\n",
    "\n",
    "print(len(scraped_urls))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9bfa16f-7709-4631-858d-ab70a2e3782a",
   "metadata": {},
   "source": [
    "# DEFINE HELPER FUNCTIONS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "027fd1fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class Experience():\n",
    "    position_title: str = None\n",
    "    from_date: str = None\n",
    "    to_date: str = None\n",
    "    description: str = None\n",
    "    duration: str = None\n",
    "    location: str = None\n",
    "    institution_name: str = None\n",
    "    linkedin_url: str = None\n",
    "\n",
    "class ScrapedProfile:\n",
    "    def __init__(self, profile_name, experiences, profile_school, profile_dist, mutuals, profile_description, profile_link):\n",
    "        self.profile_name = profile_name\n",
    "        self.experiences = experiences\n",
    "        self.profile_school = profile_school\n",
    "        self.profile_dist = profile_dist\n",
    "        self.mututals = mutuals\n",
    "        self.profile_description = profile_description\n",
    "        self.profile_link = profile_link\n",
    "\n",
    "\"\"\"\n",
    "Wait for an element to be present on the page and return it.\n",
    "\n",
    "Parameters:\n",
    "- driver: The WebDriver instance\n",
    "- by: The method to locate the element (default: By.CLASS_NAME)\n",
    "- name: The name or identifier of the element to wait for\n",
    "- base: The base element to search from (default: None, which uses the driver)\n",
    "- timeout: Maximum time to wait for the element (default: 180 seconds)\n",
    "\n",
    "Returns:\n",
    "- The WebElement if found\n",
    "- None if the element is not found within the timeout period\n",
    "\"\"\"\n",
    "def wait_for_element_to_load(driver, by=By.CLASS_NAME, name=\"pv-top-card\", base=None, timeout=180):\n",
    "    base = base or driver\n",
    "    try:\n",
    "        element = WebDriverWait(base, timeout).until(\n",
    "            EC.presence_of_element_located((by, name))\n",
    "        )\n",
    "        return element\n",
    "    except TimeoutException:\n",
    "        print(f\"Timed out waiting for element: {by}={name}\")\n",
    "        return None\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred while waiting for element {by}={name}: {str(e)}\")\n",
    "        return None\n",
    "\n",
    "# Helper function to scrape experiences\n",
    "# Returns scraped experiences if successful, otherwise returns empty list\n",
    "def get_experiences(driver):\n",
    "    try:\n",
    "        scraped_experiences = []\n",
    "        WebDriverWait(driver, 10).until(EC.presence_of_element_located((By.CSS_SELECTOR, \"li.pvs-list__paged-list-item.artdeco-list__item.pvs-list__item--line-separated.pvs-list__item--one-column\")))\n",
    "        experience_items = driver.find_elements(By.CSS_SELECTOR, \"li.pvs-list__paged-list-item.artdeco-list__item.pvs-list__item--line-separated.pvs-list__item--one-column\")    \n",
    "        if len(experience_items) > 0:\n",
    "            print(f\"Found {len(experience_items)} experience items\")\n",
    "            for item in experience_items:\n",
    "                hidden_spans = item.find_elements(By.CSS_SELECTOR, \"span.visually-hidden\")\n",
    "                experience_texts = [span.text for span in hidden_spans]\n",
    "                experience = {\n",
    "                    \"title: \": experience_texts[0],\n",
    "                    \"company: \": experience_texts[1],\n",
    "                    \"dates: \": experience_texts[2],\n",
    "                }\n",
    "                \n",
    "                if len(experience_texts) > 3:\n",
    "                    experience[\"location: \"] = experience_texts[3]\n",
    "                if len(experience_texts) > 4:\n",
    "                    experience[\"summary: \"] = experience_texts[4]\n",
    "                if len(experience_texts) > 5:\n",
    "                    experience[\"remaining: \"] = (\", \").join(experience_texts[5:])\n",
    "\n",
    "                scraped_experiences.append(experience)\n",
    "\n",
    "            for experience in scraped_experiences:\n",
    "                for k, v in experience.items():\n",
    "                    print(f\"{k}: {v}\")\n",
    "            print(\"Successfully scraped experiences\")\n",
    "        else:\n",
    "            print(\"No experiences found\")\n",
    "    except:\n",
    "        print(\"ERROR: No experiences found\")\n",
    "    return scraped_experiences\n",
    "\n",
    "\n",
    "# Helper function to scrape education\n",
    "# Returns scraped education if successful, otherwise returns empty list\n",
    "def get_education(driver):\n",
    "    try:\n",
    "        scraped_education = []\n",
    "        WebDriverWait(driver, 10).until(EC.presence_of_element_located((By.CSS_SELECTOR, \"li.pvs-list__paged-list-item.artdeco-list__item.pvs-list__item--line-separated.pvs-list__item--one-column\")))\n",
    "        education_items = driver.find_elements(By.CSS_SELECTOR, \"li.pvs-list__paged-list-item.artdeco-list__item.pvs-list__item--line-separated.pvs-list__item--one-column\")    \n",
    "        if len(education_items) > 0:\n",
    "            for item in education_items:\n",
    "                hidden_spans = item.find_elements(By.CSS_SELECTOR, \"span.visually-hidden\")\n",
    "                education_texts = [span.text for span in hidden_spans]\n",
    "                education = {\n",
    "                    \"school: \": education_texts[0],\n",
    "                }\n",
    "                \n",
    "                if len(education_texts) > 1:\n",
    "                    education[\"degree: \"] = education_texts[1]\n",
    "                if len(education_texts) > 2:\n",
    "                    education[\"dates: \"] = education_texts[2]\n",
    "                if len(education_texts) > 3:\n",
    "                    education[\"remaining: \"] = (\", \").join(education_texts[5:])\n",
    "\n",
    "                scraped_education.append(education)\n",
    "\n",
    "            time.sleep(1 + random.random())\n",
    "            for education in scraped_education:\n",
    "                for k, v in education.items():\n",
    "                    print(f\"{k}: {v}\")\n",
    "            print(\"Successfully scraped education\")\n",
    "        else:\n",
    "            print(\"No education found\")\n",
    "    except Exception as e:\n",
    "        print(\"ERROR: No education found\")\n",
    "    return scraped_education\n",
    "\n",
    "# Helper function to scrape degree of connection and mutuals\n",
    "# Returns scraped degree of connection and mutuals if successful, otherwise returns N/A\n",
    "def scrape_degree_of_connection_and_mutuals(driver):\n",
    "    scraped_profile_dist = \"4+\"\n",
    "    scraped_mutuals = \"N/A\"\n",
    "    try:\n",
    "        scraped_profile_dist = WebDriverWait(driver, 10).until(EC.presence_of_element_located((By.CSS_SELECTOR, \"span.dist-value\")))\n",
    "        if scraped_profile_dist == \"1st\" or scraped_profile_dist == \"2nd\":\n",
    "            WebDriverWait(driver, 10).until(EC.presence_of_element_located((By.CSS_SELECTOR, \"h1.text-heading-xlarge.inline.t-24.v-align-middle.break-words\")))\n",
    "            try:\n",
    "                span_element = WebDriverWait(driver, 10).until(\n",
    "                            EC.presence_of_element_located((By.CSS_SELECTOR, \"span.t-normal.t-black--light.t-14.hoverable-link-text\"))\n",
    "                        )\n",
    "                scraped_mutuals = span_element.text.split('\\n')[0]\n",
    "                print(\"successfully found mutual connections: \" + scraped_mutuals)\n",
    "            except:\n",
    "                print(\"ERROR: mutuals not found\")\n",
    "        print(\"Successfully scraped degree of connection: \" + scraped_profile_dist)\n",
    "    except:\n",
    "        print(\"No degree of connection found\")\n",
    "    return scraped_profile_dist, scraped_mutuals\n",
    "\n",
    "# Helper function to scrape description\n",
    "# Returns scraped description if successful, otherwise returns N/A\n",
    "def scrape_description(driver):\n",
    "    scraped_description = \"N/A\"\n",
    "    try:\n",
    "        scraped_description = driver.find_element(By.CLASS_NAME, \"text-body-medium.break-words\").text\n",
    "        print(\"Successfully scraped description: \" + scraped_description)\n",
    "    except:\n",
    "        print(\"ERROR: description not found\")\n",
    "\n",
    "        \n",
    "def filter():\n",
    "    pass     \n",
    "    # try:\n",
    "        #     WebDriverWait(driver, 240).until(lambda d: d.execute_script('return document.readyState') == 'complete')\n",
    "        # except:\n",
    "        #     driver = reinstantiate_driver()\n",
    "\n",
    "        # # FILTERING\n",
    "        \n",
    "        # likely_founder = True\n",
    "        # relevant_exp = True\n",
    "\n",
    "        # cur_exp = experiences[0]\n",
    "        # relevant_companies = [\"stealth\", \"new\"]\n",
    "        # if any(company in cur_exp.institution_name.split(\" ·\")[0].lower() for company in relevant_companies) or \"present\" not in cur_exp.to_date.lower():\n",
    "        #     likely_founder = True\n",
    "\n",
    "        # relevant_titles = [\"product\", \"engineer\", \"sales\", \"business development\", \"founder\", \"head\", \"lead\", \"senior\", \"staff\", \"chief\", \"growth\"]\n",
    "        # for experience in experiences[1:5]:\n",
    "        #     if any(title in experience.position_title.lower() for title in relevant_titles):\n",
    "        #         relevant_exp = True\n",
    "        #         break\n",
    "        # relevant_exp = True\n",
    "\n",
    "        # if not (likely_founder and relevant_exp):\n",
    "        #     print(likely_founder, relevant_exp)\n",
    "        #     return None\n",
    "\n",
    "        # person_obj = Person(profile_link, driver = driver, scrape=False, experiences = [None])\n",
    "        # try:\n",
    "        #     WebDriverWait(driver, 240).until(lambda d: d.execute_script('return document.readyState') == 'complete')\n",
    "        # except:\n",
    "        #     driver = reinstantiate_driver(driver)\n",
    "        # time.sleep(2 + random.random() * 7)\n",
    "\n",
    "\n",
    "\n",
    "# Main function to scrape degree of connection and mutuals\n",
    "# Returns scraped degree of connection and mutuals if successful, otherwise returns N/A\n",
    "def scrape_profile_live_filtering(driver, scraped_link):\n",
    "\n",
    "    # Scrape Name\n",
    "    driver.get(scraped_link)\n",
    "    scraped_profile_name = WebDriverWait(driver, 10).until(EC.presence_of_element_located((By.CSS_SELECTOR, \"h1.text-heading-xlarge.inline.t-24.v-align-middle.break-words\")))\n",
    "    time.sleep(1 + random.random())\n",
    "\n",
    "    # Scrape Experiences\n",
    "    experiences_url = os.path.join(scraped_link, \"details/experience\")\n",
    "    print(experiences_url)\n",
    "    driver.get(experiences_url)\n",
    "    scraped_experiences = get_experiences(driver)\n",
    "    print(\"Finished scraping experiences\")\n",
    "\n",
    "    # Scrape Education\n",
    "    education_url = os.path.join(scraped_link, \"details/education\")\n",
    "    print(education_url)\n",
    "    driver.get(education_url)\n",
    "    scraped_education = get_education(driver)\n",
    "    print(\"Finished scraping education\")\n",
    "\n",
    "    # Scrape degree of connection and mutuals if available\n",
    "    driver.get(scraped_link)\n",
    "    scraped_profile_dist, scraped_mutuals = scrape_degree_of_connection_and_mutuals(driver)\n",
    "    # Scrape description\n",
    "    scraped_description = scrape_description(driver)\n",
    "\n",
    "    # Scape profile link\n",
    "    scraped_link = driver.current_url\n",
    "    print(\"successfully scraped profile link: \" + scraped_link)\n",
    "    \n",
    "    profile = ScrapedProfile(scraped_profile_name,\n",
    "                   scraped_experiences,\n",
    "                   scraped_education,\n",
    "                   scraped_profile_dist,\n",
    "                   scraped_mutuals,\n",
    "                   scraped_description,\n",
    "                   scraped_link)\n",
    "    return profile"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "095b1a61-bfdb-4ec3-b5f5-8bc81d74a68d",
   "metadata": {},
   "source": [
    "# load to_scrape_urls from pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e0ea7d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('db/to_scrape.pickle', 'rb') as f:\n",
    "  to_scrape_urls = pickle.load(f)\n",
    "print(len(to_scrape_urls))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7e3118f-25a6-4de7-936d-5e009afdb22d",
   "metadata": {},
   "source": [
    "# SALES NAV PAGE SCRAPER:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e83345e3-d166-4e30-a7d6-2634b0db34a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# rebuilding old\n",
    "\n",
    "# Scrolls the page to bring the specified profile element into view\n",
    "# Returns True if successful, False if an error occurs\n",
    "def scroll_to_profile(driver, profile):\n",
    "    try:\n",
    "        driver.execute_script(\"arguments[0].scrollIntoView();\", profile)\n",
    "        return True\n",
    "    except Exception as e:\n",
    "        print(f\"Error scrolling to profile: {e}\")\n",
    "        return False\n",
    "\n",
    "# Clicks on the profile element to open its details\n",
    "# Returns True if successful, False if an error occurs\n",
    "def click_profile(profile):\n",
    "    try:\n",
    "        salesNavOpenProfileButton = profile.find_element(By.CLASS_NAME, \"artdeco-entity-lockup__title\")\n",
    "        salesNavOpenProfileButton.click()\n",
    "        return True\n",
    "    except (NoSuchElementException, ElementClickInterceptedException) as e:\n",
    "        print(f\"Error clicking profile: {e}\")\n",
    "        return False\n",
    "\n",
    "# Clicks the three dots button to open the dropdown menu\n",
    "# Returns True if successful, False if an error occurs\n",
    "def click_three_dots_button(driver):\n",
    "    try:\n",
    "        actionContainer = driver.find_element(By.CLASS_NAME, \"_actions-container_1dg5u8\")\n",
    "        threeDotsButton = actionContainer.find_element(By.CLASS_NAME, \"_icon_ps32ck\")\n",
    "        threeDotsButton.click()\n",
    "        return True\n",
    "    except (NoSuchElementException, ElementClickInterceptedException) as e:\n",
    "        print(f\"Error clicking three dots button\")\n",
    "        return False\n",
    "\n",
    "# Retrieves the LinkedIn URL from the dropdown menu\n",
    "# Returns the URL if successful, None if an error occurs\n",
    "def get_linkedin_url(driver):\n",
    "    try:\n",
    "        dropdownContainer = driver.find_element(By.CLASS_NAME, \"_visible_x5gf48\")\n",
    "        normalLinkedInUrl = dropdownContainer.find_elements(By.TAG_NAME, \"a\")[1].get_attribute(\"href\")\n",
    "        return normalLinkedInUrl\n",
    "    except (NoSuchElementException, IndexError) as e:\n",
    "        print(f\"Error getting LinkedIn URL: {e}\")\n",
    "        return None\n",
    "\n",
    "# Closes the profile popout\n",
    "# Returns True if successful, False if an error occurs\n",
    "def close_popout(driver):\n",
    "    try:\n",
    "        header = driver.find_element(By.CLASS_NAME, \"_inline-sidesheet-header-actions_1cn7lg\")\n",
    "        button = header.find_elements(By.CLASS_NAME, \"_button_ps32ck\")[1]\n",
    "        button.click()\n",
    "        return True\n",
    "    except (NoSuchElementException, ElementClickInterceptedException, IndexError) as e:\n",
    "        print(f\"Error closing popout: {e}\")\n",
    "        return False\n",
    "\n",
    "# Navigates to the next page of search results\n",
    "# Returns True if successful, False if there are no more pages or an error occurs\n",
    "def navigate_to_next_page(driver):\n",
    "    try:\n",
    "        nextPageButton = driver.find_element(By.CLASS_NAME, \"artdeco-pagination__button--next\")\n",
    "        nextPageButton.click()\n",
    "        return True\n",
    "    except (NoSuchElementException, ElementClickInterceptedException) as e:\n",
    "        print(f\"No more pages or error navigating: {e}\")\n",
    "        return False\n",
    "\n",
    "# Main scraping function\n",
    "def scrape_profiles(driver, SALES_NAV_SEARCH_URL, scraped_urls, to_scrape_urls):\n",
    "    # reinstantiate_driver()\n",
    "    driver.get(SALES_NAV_SEARCH_URL)\n",
    "    if not wait_for_element_to_load(driver, By.TAG_NAME, \"body\", timeout=240):\n",
    "        print(\"Timeout waiting for page to load\")\n",
    "        return to_scrape_urls\n",
    "\n",
    "    time.sleep(2 + random.random() * 6)\n",
    "    \n",
    "    while True:\n",
    "        profiles = driver.find_elements(By.CLASS_NAME, \"artdeco-list__item\")\n",
    "        \n",
    "        for profile in profiles:\n",
    "            if not scroll_to_profile(driver, profile):\n",
    "                continue\n",
    "\n",
    "            if not wait_for_element_to_load(driver, By.CLASS_NAME, \"artdeco-entity-lockup__title\"):\n",
    "                continue\n",
    "\n",
    "            if not click_profile(profile):\n",
    "                continue\n",
    "\n",
    "            if not wait_for_element_to_load(driver, By.CLASS_NAME, \"_actions-container_1dg5u8\"):\n",
    "                continue\n",
    "\n",
    "            time.sleep(2 + random.random() * 6)\n",
    "\n",
    "            if not click_three_dots_button(driver):\n",
    "                continue\n",
    "\n",
    "            if not wait_for_element_to_load(driver, By.CLASS_NAME, \"_visible_x5gf48\"):\n",
    "                continue\n",
    "\n",
    "            time.sleep(2 + random.random() * 6)\n",
    "\n",
    "            normalLinkedInUrl = get_linkedin_url(driver)\n",
    "            if normalLinkedInUrl:\n",
    "                if normalLinkedInUrl in scraped_urls:\n",
    "                    print(f\"Skipping (already scraped) {normalLinkedInUrl}\")\n",
    "                else:\n",
    "                    to_scrape_urls.append(normalLinkedInUrl)\n",
    "                    with open('db/to_scrape.pickle', 'wb') as f:\n",
    "                        pickle.dump(to_scrape_urls, f)\n",
    "                    print(normalLinkedInUrl)\n",
    "\n",
    "            if not close_popout(driver):\n",
    "                continue\n",
    "\n",
    "        next_button = wait_for_element_to_load(driver, By.CLASS_NAME, \"artdeco-pagination__button--next\")\n",
    "        if not next_button or not next_button.is_enabled():\n",
    "            break\n",
    "\n",
    "        next_button.click()\n",
    "\n",
    "        if not wait_for_element_to_load(driver, By.CLASS_NAME, \"artdeco-list__item\"):\n",
    "            break\n",
    "\n",
    "        time.sleep(2 + random.random() * 6)\n",
    "\n",
    "    return to_scrape_urls\n",
    "try:\n",
    "    updated_to_scrape_urls = scrape_profiles(driver, SALES_NAV_SEARCH_URL, scraped_urls, to_scrape_urls)\n",
    "    print(updated_to_scrape_urls)\n",
    "except Exception as e:\n",
    "    print(f\"An unexpected error occurred: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d4f0599-f3d3-4423-9088-15da03a7dc52",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(to_scrape_urls)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b1c8979-0304-4621-ae96-59108d44bada",
   "metadata": {},
   "source": [
    "# deduplicate to_scrape_urls, declaring candidates and scraped_url lists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ea0fe49",
   "metadata": {},
   "outputs": [],
   "source": [
    "# deduplicate to_scrape_urls\n",
    "with open('db/to_scrape.pickle', 'wb') as f:\n",
    "    to_scrape_urls = list(set(to_scrape_urls))\n",
    "    with open('db/already_scraped.pickle', 'rb') as f2:\n",
    "        already_scraped_urls = pickle.load(f2)\n",
    "        to_scrape_urls = [url for url in to_scrape_urls if url not in already_scraped_urls]\n",
    "        pickle.dump(to_scrape_urls, f)\n",
    "        print(len(to_scrape_urls))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92e50d5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "reinstantiate_driver()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6da7736-3078-4e03-8be6-967f48640cdc",
   "metadata": {},
   "source": [
    "# ACTUAL PROFILE SCRAPING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7187f14e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# iterates through to_scrape_urls, scrapes and adds to list: candidates\n",
    "\n",
    "import pickle\n",
    "candidates = []\n",
    "scraped_urls = []\n",
    "\n",
    "with open('db/to_scrape.pickle', 'rb') as f:\n",
    "    to_scrape_urls = pickle.load(f)\n",
    "\n",
    "print(f'To-Scrape Urls: len:{len(to_scrape_urls)}')\n",
    "\n",
    "start = 0\n",
    "end = len(to_scrape_urls)\n",
    "\n",
    "for idx, url in enumerate(to_scrape_urls.copy()):\n",
    "  if start > idx:\n",
    "    continue\n",
    "  if idx >= end:\n",
    "    break\n",
    "  print(f'At index: {idx} - url: {url}') \n",
    "  \n",
    "  # scrape profiles, and write results to a file\n",
    "  try:\n",
    "    profile = scrape_profile_live_filtering(driver, url)\n",
    "    \n",
    "    if profile != None:\n",
    "        print(\"saving profile info\", end=\"\")\n",
    "        candidates.append(profile)\n",
    "\n",
    "        print(\"; recording scraped url\", end=\"\")\n",
    "        url = url.strip().strip('/')\n",
    "        scraped_urls.append(url)\n",
    "    else:\n",
    "        print(\"profile filtered out\", end=\"\")\n",
    "    \n",
    "    print(\"; removing from to-scrape\", end=\"\")\n",
    "    to_scrape_urls.remove(url)\n",
    "    with open('db/to_scrape.pickle', 'wb') as f:\n",
    "      pickle.dump(to_scrape_urls, f)\n",
    "\n",
    "    print(\"; success!\")\n",
    "    print(((idx+1)/end) * 100, '% Done - at index:', idx)\n",
    "  except Exception as e:\n",
    "    print(e)\n",
    "    print('Failed to scrape profile: ', url)\n",
    "    with open('failed_urls.txt', 'a') as f:\n",
    "      f.write(url + '\\n')\n",
    "  print('\\n-----------------------------------\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47185763-6f2b-4f89-9acc-1651887008f6",
   "metadata": {},
   "source": [
    "# EXPORTING RESULTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0906d83",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def parseCandidate(x):\n",
    "    res = {}\n",
    "    res['url'] = x.profile_link\n",
    "    res['name'] = x.profile_name\n",
    "    res['dist'] = x.profile_dist\n",
    "    res['description'] = x.profile_description\n",
    "\n",
    "    #Experiences first\n",
    "    print(x.experiences)\n",
    "    for i, e in enumerate(x.experiences):\n",
    "        res[f'exp{i} title'] = e[\"title: \"]\n",
    "        res[f'exp{i} company'] = e[\"company: \"]\n",
    "        res[f'exp{i} dates'] = e[\"dates: \"]\n",
    "\n",
    "    # School second\n",
    "    for i, e in enumerate(x.profile_school):\n",
    "        res[f'edu{i} school'] = e[\"school: \"]\n",
    "        if \"degree :\" in e:\n",
    "            res[f'edu{i} degree'] = e[\"degree: \"]\n",
    "\n",
    "    \n",
    "    return res\n",
    "\n",
    "# New Method for Creating df\n",
    "rows = []\n",
    "for candidate in candidates:\n",
    "    row = parseCandidate(candidate)\n",
    "    rows.append(row)\n",
    "\n",
    "df = pd.DataFrame(rows)\n",
    "\n",
    "#Export to csv\n",
    "try:\n",
    "    df.to_csv('candidates.csv', index=False)\n",
    "    print(\"Exported to csv\")\n",
    "except:\n",
    "    print(\"Failed to export to csv\")\n",
    "\n",
    "#Export to Excel\n",
    "try:\n",
    "    df.to_excel('candidates.xlsx', index=False)\n",
    "    print(\"Exported to Excel\")\n",
    "except:\n",
    "    print(\"Failed to export to Excel\")\n",
    "\n",
    "    \n",
    "# update db/already_scraped.pickle\n",
    "with open('db/already_scraped.pickle', 'rb') as f:\n",
    "    already_scraped = pickle.load(f)\n",
    "    print(f\"Previously scraped: {len(already_scraped)}\")\n",
    "    already_scraped = already_scraped + scraped_urls\n",
    "    already_scraped = list(set(already_scraped))\n",
    "    print(f\"Newly scraped: {len(already_scraped)}\")\n",
    "with open('db/already_scraped.pickle', 'wb') as f:\n",
    "    pickle.dump(already_scraped, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97efd509",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pickle\n",
    "import io\n",
    "from google.auth.transport.requests import Request\n",
    "from google.oauth2.credentials import Credentials\n",
    "from google_auth_oauthlib.flow import InstalledAppFlow\n",
    "from googleapiclient.discovery import build\n",
    "from googleapiclient.http import MediaFileUpload\n",
    "\n",
    "# If modifying these SCOPES, delete the file token.pickle.\n",
    "SCOPES = ['https://www.googleapis.com/auth/drive.file']\n",
    "\n",
    "def authenticate():\n",
    "    creds = None\n",
    "    # The file token.pickle stores the user's access and refresh tokens, and is created automatically when the authorization flow completes for the first time.\n",
    "    if os.path.exists('token.pickle'):\n",
    "        with open('token.pickle', 'rb') as token:\n",
    "            creds = pickle.load(token)\n",
    "    # If there are no (valid) credentials available, let the user log in.\n",
    "    if not creds or not creds.valid:\n",
    "        if creds and creds.expired and creds.refresh_token:\n",
    "            creds.refresh(Request())\n",
    "        else:\n",
    "            flow = InstalledAppFlow.from_client_secrets_file(\n",
    "                'client_secrets.json', SCOPES)\n",
    "            creds = flow.run_local_server(port=0)\n",
    "        # Save the credentials for the next run\n",
    "        with open('token.pickle', 'wb') as token:\n",
    "            pickle.dump(creds, token)\n",
    "    return creds\n",
    "\n",
    "def upload_file_to_drive(file_path, file_name, mime_type):\n",
    "    creds = authenticate()\n",
    "    service = build('drive', 'v3', credentials=creds)\n",
    "\n",
    "    file_metadata = {'name': file_name}\n",
    "    media = MediaFileUpload(file_path, mimetype=mime_type)\n",
    "\n",
    "    file = service.files().create(body=file_metadata, media_body=media, fields='id').execute()\n",
    "    print('File ID: %s' % file.get('id'))\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    file_path = 'path_to_your_local_excel_file.xlsx'  # Replace with the path to your local file\n",
    "    file_name = 'your_excel_file.xlsx'  # Replace with the desired name for the file in Google Drive\n",
    "    mime_type = 'application/vnd.openxmlformats-officedocument.spreadsheetml.sheet'\n",
    "\n",
    "    upload_file_to_drive(file_path, file_name, mime_type)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ee20a9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "categories = COMPANY_CATEGORIES\n",
    "\n",
    "# Example color mapping for categories\n",
    "category_colors = {\n",
    "    \"SECURITY\": 'red',\n",
    "    \"OTHER\": 'blue',\n",
    "    \"PUBLIC\": 'green',\n",
    "    \"INFRA\": 'yellow',\n",
    "    \"FINTECH\": 'orange',\n",
    "    \"CRYPTO\": 'purple',\n",
    "    \"FRONTIER\": 'cyan',\n",
    "    \"AI\": 'magenta'\n",
    "}\n",
    "\n",
    "# Create a reverse dictionary for easier lookup: {company: category}\n",
    "company_category = {}\n",
    "for category, companies in categories.items():\n",
    "    for company in companies:\n",
    "        company_category[company] = category\n",
    "\n",
    "# Modify the style function\n",
    "def highlight_by_category(val):\n",
    "    category = company_category.get(val)\n",
    "    if category:\n",
    "        color = category_colors.get(category, 'none')  # default to 'none' if no color is specified\n",
    "    else:\n",
    "        color = 'none'\n",
    "    return f'background-color: {color}'\n",
    "\n",
    "import re\n",
    "ILLEGAL_CHARACTERS_RE = re.compile(r'[\\000-\\010]|[\\013-\\014]|[\\016-\\037]')\n",
    "def find_illegal_characters(df):\n",
    "    for column in df.columns:\n",
    "        for idx, item in enumerate(df[column]):\n",
    "            if isinstance(item, str) and ILLEGAL_CHARACTERS_RE.search(item):\n",
    "                # replace illegal characters with an empty string\n",
    "                df[column][idx] = ILLEGAL_CHARACTERS_RE.sub('', item)\n",
    "    return df\n",
    "\n",
    "styled_df = df\n",
    "styled_df = find_illegal_characters(styled_df)\n",
    "styled_df = df.style.applymap(highlight_by_category)\n",
    "\n",
    "# Save the styled DataFrame to an Excel file\n",
    "# get today's date in MM-DD-YYYY format\n",
    "import datetime\n",
    "now = datetime.datetime.now()\n",
    "date = now.strftime(\"%m-%d-%Y\")\n",
    "styled_df.to_excel(f'results/{date}_{start}-{end}.xlsx', engine='openpyxl', index=False)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
