{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5b930f8e",
   "metadata": {},
   "source": [
    "#  SETUP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b9e6fd8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# if the script finishes very quickly (and generates an empty excel file), click run again\n",
    "# if the script errors on the \"Login Cell\" (added a comment to indicate which cell that is below), set IS_HEADLESS to \"False\" and run again. The scraper will automatically launch a page and attempt to login to LinkedIn. It's likely erroring because LinkedIn is asking for a captcha to verify the user is not a bot. Solve the captch/challenge and login. Once successfully logged in, set IS_HEADLESS back to \"True\" and run again.\n",
    "\n",
    "import pickle\n",
    "import csv\n",
    "import os\n",
    "import random\n",
    "import time\n",
    "from linkedin_scraper import actions\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.firefox.options import Options as FirefoxOptions\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.common.exceptions import TimeoutException, StaleElementReferenceException, NoSuchElementException, ElementClickInterceptedException\n",
    "from dotenv import load_dotenv\n",
    "import pandas as pd\n",
    "# for google docs upload\n",
    "import gspread\n",
    "from oauth2client.service_account import ServiceAccountCredentials\n",
    "from io import StringIO\n",
    "\n",
    "\n",
    "IS_HEADLESS = False\n",
    "\n",
    "# Set environment variables to email and password\n",
    "load_dotenv()\n",
    "LINKEDIN_EMAIL = \"ljiangfbla@gmail.com\"\n",
    "LINKEDIN_PASSWORD = \"L!nked!nS0urc!ng\"\n",
    "\n",
    "# Check if the environment variables are set\n",
    "if not LINKEDIN_EMAIL or not LINKEDIN_PASSWORD:\n",
    "    raise ValueError(\"LinkedIn credentials not set in environment variables\")\n",
    "\n",
    "\n",
    "COMPANY_CATEGORIES = {\n",
    "    \"SECURITY\": [\n",
    "        \"Okta\",\n",
    "        \"Snyk\",\n",
    "        \"R2C/Semgrep\", \n",
    "        \"Wiz\",\n",
    "        \"Lacework\",\n",
    "        \"Crowdstrike\",\n",
    "        \"Palo Alto Networks\",\n",
    "        \"Island\",\n",
    "        \"Vanta\",\n",
    "        \"Material Security\",\n",
    "        \"Abnormal Security\",\n",
    "        \"Samsara\",\n",
    "    ],\n",
    "    \"OTHER\": [\n",
    "        \"Figma\",\n",
    "        \"Airtable\",\n",
    "        \"Notion\",\n",
    "        \"Canva\",\n",
    "        \"Webflow\",\n",
    "        \"Faire\",\n",
    "        \"Deel\",\n",
    "        \"Rippling\",\n",
    "        \"Flexport\",\n",
    "        \"Benchling\",\n",
    "        \"Solugen\"\n",
    "    ],\n",
    "    \"PUBLIC\": [\n",
    "        \"Doordash\",\n",
    "        \"Uber\",\n",
    "        \"Palantir\",\n",
    "        \"Airbnb\",\n",
    "        \"Instacart\"\n",
    "    ],\n",
    "    \"INFRA\": [\n",
    "        \"Fivetran\",\n",
    "        \"DBT\",\n",
    "        \"Temporal\",\n",
    "        \"Cockroach Labs\",\n",
    "        \"Grafana\",\n",
    "        \"Zapier\",\n",
    "        \"Starburst\",\n",
    "        \"Retool\",\n",
    "        \"Sentry\",\n",
    "        \"Sourcegraph\",\n",
    "        \"Cribl\",\n",
    "        \"Vercel\",\n",
    "        \"Clickhouse\",\n",
    "        \"Github,\"\n",
    "        \"Cisco Meraki\",\n",
    "    ],\n",
    "    \"FINTECH\": [\n",
    "        \"Robinhood\",\n",
    "        \"Square\",\n",
    "        \"Stripe\",\n",
    "        \"Ramp\",\n",
    "        \"Brex\",\n",
    "        \"Plaid\",\n",
    "        \"Modern Treasury\",\n",
    "        \"Mercury\",\n",
    "        \"Persona\",\n",
    "        \"Klarna\",\n",
    "        \"Nubank\"\n",
    "    ],\n",
    "    \"CRYPTO\": [\n",
    "        \"Coinbase\",\n",
    "        \"Uniswap\",\n",
    "        \"Chainalysis\",\n",
    "        \"Arbitrum\",\n",
    "        \"TRM\",\n",
    "        \"Fireblocks\",\n",
    "        \"Eigenlayer\"\n",
    "    ],\n",
    "    \"FRONTIER\": [\n",
    "        \"Anduril\",\n",
    "        \"SpaceX\",\n",
    "        \"Zipline\",\n",
    "        \"Varda\",\n",
    "        \"Hadrian\"\n",
    "    ],\n",
    "    \"AI\": [\n",
    "        \"Bytedance\",\n",
    "        \"Scale AI\",\n",
    "        \"Anthropic\",\n",
    "        \"Robust intelligence\",\n",
    "        \"OpenAI\",\n",
    "        \"Predibase\",\n",
    "        \"Cohere\",\n",
    "        \"Databricks\",\n",
    "        \"Hugging Face\",\n",
    "        \"RunwayML\",\n",
    "        \"Tecton\",\n",
    "        \"Weights & Biases\",\n",
    "        \"Kumo AI\",\n",
    "        \"NVIDIA\",\n",
    "        \"Adept\",\n",
    "        \"Glean\",\n",
    "        \"Character.ai\",\n",
    "        \"Midjourney\",\n",
    "        \"Facebook AI\",\n",
    "        \"FAIR\",\n",
    "        \"Google brain\"\n",
    "    ]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "adbde830",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Library/Developer/CommandLineTools/usr/bin/python3\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "print(sys.executable)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7957b23f",
   "metadata": {},
   "source": [
    "# LOGIN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3596a5f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# IF THIS CELL ERRORS DUE TO CAPTCHA, do this:\n",
    "# manually complete the captcha, click on the next cell, and select Run menu, select \"run selected cell and all below\" \n",
    "\n",
    "def instantiate_driver():\n",
    "    options = FirefoxOptions()\n",
    "    if IS_HEADLESS:\n",
    "        options.add_argument(\"--headless\")\n",
    "    driver = webdriver.Firefox(options=options)\n",
    "    \n",
    "    try:\n",
    "        actions.login(driver, LINKEDIN_EMAIL, LINKEDIN_PASSWORD)\n",
    "    except Exception as e:\n",
    "        print(\"Error logging in. Please complete the captcha challenge and login manually.\")\n",
    "        print(e)\n",
    "\n",
    "    time.sleep(15)\n",
    "    return driver\n",
    "    \n",
    "driver = instantiate_driver()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "515b071f",
   "metadata": {},
   "source": [
    "# SALES NAV HELPER FUNCTIONS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "027fd1fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ScrapedProfile:\n",
    "    def __init__(self, profile_name, experiences, education, profile_dist, mutuals, profile_description, profile_link):\n",
    "        self.profile_name = profile_name\n",
    "        self.experiences = experiences\n",
    "        self.education = education\n",
    "        self.profile_dist = profile_dist\n",
    "        self.mutuals = mutuals\n",
    "        self.profile_description = profile_description\n",
    "        self.profile_link = profile_link\n",
    "\n",
    "def check_pickles(historyPickle, toScrapePickle):\n",
    "    scraped_urls = []\n",
    "    with open(historyPickle, 'rb') as f:\n",
    "        try:\n",
    "            scraped_urls = pickle.load(f)\n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "            print(\"Error loading history pickle file\")\n",
    "        print(len(scraped_urls))\n",
    "\n",
    "    to_scrape_urls = []\n",
    "    with open(toScrapePickle, 'rb') as f:\n",
    "        try:\n",
    "            to_scrape_urls = pickle.load(f)\n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "            print(\"Error loading to scrape url pickle file\")\n",
    "        print(len(to_scrape_urls))\n",
    "\n",
    "    return scraped_urls, to_scrape_urls\n",
    "\n",
    "def deduplicate(toScrapePickle, to_scrape_urls, already_scraped_urls):\n",
    "    to_scrape_urls = list(set(to_scrape_urls))\n",
    "    originalCount = len(to_scrape_urls)\n",
    "    to_scrape_urls = [url for url in to_scrape_urls if url not in already_scraped_urls]\n",
    "    print(f\"Removed {originalCount - len(to_scrape_urls)} duplicates\")\n",
    "\n",
    "    with open(toScrapePickle, 'wb') as to_scrape_file:\n",
    "        pickle.dump(to_scrape_urls, to_scrape_file)\n",
    "    \n",
    "    print(len(to_scrape_urls))\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Wait for an element to be present on the page and return it.\n",
    "\n",
    "Parameters:\n",
    "- driver: The WebDriver instance\n",
    "- by: The method to locate the element (default: By.CLASS_NAME)\n",
    "- name: The name or identifier of the element to wait for\n",
    "- base: The base element to search from (default: None, which uses the driver)\n",
    "- timeout: Maximum time to wait for the element (default: 180 seconds)\n",
    "\n",
    "Returns:\n",
    "- The WebElement if found\n",
    "- None if the element is not found within the timeout period\n",
    "\"\"\"\n",
    "def wait_for_element_to_load(driver, by=By.CLASS_NAME, name=\"pv-top-card\", base=None, timeout=100):\n",
    "    base = base or driver\n",
    "    try:\n",
    "        element = WebDriverWait(base, timeout).until(\n",
    "            EC.presence_of_element_located((by, name))\n",
    "        )\n",
    "        return element\n",
    "    except TimeoutException:\n",
    "        print(f\"Timed out waiting for element: {by}={name}\")\n",
    "        return None\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred while waiting for element {by}={name}: {str(e)}\")\n",
    "        return None\n",
    "\n",
    "# Scrolls the page to bring the specified profile element into view\n",
    "# Returns True if successful, False if an error occurs\n",
    "def scroll_to_profile(driver, profile):\n",
    "    try:\n",
    "        driver.execute_script(\"arguments[0].scrollIntoView();\", profile)\n",
    "        return True\n",
    "    except Exception as e:\n",
    "        print(f\"Error scrolling to profile: {e}\")\n",
    "        return False\n",
    "\n",
    "# Clicks on the profile element to open its details\n",
    "# Returns True if successful, False if an error occurs\n",
    "def click_profile(profile):\n",
    "    try:\n",
    "        salesNavOpenProfileButton = profile.find_element(By.CLASS_NAME, \"artdeco-entity-lockup__title\")\n",
    "        salesNavOpenProfileButton.click()\n",
    "        return True\n",
    "    except (NoSuchElementException, ElementClickInterceptedException) as e:\n",
    "        print(f\"Error clicking profile: {e}\")\n",
    "        return False\n",
    "\n",
    "# Clicks the three dots button to open the dropdown menu\n",
    "# Returns True if successful, False if an error occurs\n",
    "def click_three_dots_button(driver):\n",
    "    try:\n",
    "        actionContainer = driver.find_element(By.CLASS_NAME, \"_actions-container_1dg5u8\")\n",
    "        threeDotsButton = actionContainer.find_element(By.CLASS_NAME, \"_icon_ps32ck\")\n",
    "        threeDotsButton.click()\n",
    "        return True\n",
    "    except (NoSuchElementException, ElementClickInterceptedException) as e:\n",
    "        print(f\"Error clicking three dots button\")\n",
    "        return False\n",
    "\n",
    "# Retrieves the LinkedIn URL from the dropdown menu\n",
    "# Returns the URL if successful, None if an error occurs\n",
    "def get_linkedin_url(driver):\n",
    "    try:\n",
    "        dropdownContainer = driver.find_element(By.CLASS_NAME, \"_visible_x5gf48\")\n",
    "        normalLinkedInUrl = dropdownContainer.find_elements(By.TAG_NAME, \"a\")[1].get_attribute(\"href\")\n",
    "        return normalLinkedInUrl\n",
    "    except (NoSuchElementException, IndexError) as e:\n",
    "        print(f\"Error getting LinkedIn URL: {e}\")\n",
    "        return None\n",
    "\n",
    "# Closes the profile popout\n",
    "# Returns True if successful, False if an error occurs\n",
    "def close_popout(driver):\n",
    "    try:\n",
    "        header = driver.find_element(By.CLASS_NAME, \"_inline-sidesheet-header-actions_1cn7lg\")\n",
    "        button = header.find_elements(By.CLASS_NAME, \"_button_ps32ck\")[1]\n",
    "        button.click()\n",
    "        return True\n",
    "    except (NoSuchElementException, ElementClickInterceptedException, IndexError) as e:\n",
    "        print(f\"Error closing popout: {e}\")\n",
    "        return False\n",
    "\n",
    "# Navigates to the next page of search results\n",
    "# Returns True if successful, False if there are no more pages or an error occurs\n",
    "def navigate_to_next_page(driver):\n",
    "    try:\n",
    "        nextPageButton = driver.find_element(By.CLASS_NAME, \"artdeco-pagination__button--next\")\n",
    "        nextPageButton.click()\n",
    "        return True\n",
    "    except (NoSuchElementException, ElementClickInterceptedException) as e:\n",
    "        print(f\"No more pages or error navigating: {e}\")\n",
    "        return False\n",
    "\n",
    "# Main scraping function\n",
    "def scrape_profiles(driver, SALES_NAV_SEARCH_URL, already_scraped_urls, to_scrape_urls, toScrapePickle):\n",
    "    # reinstantiate_driver(driver)\n",
    "    driver.get(SALES_NAV_SEARCH_URL)\n",
    "    \n",
    "    while True:\n",
    "        wait_for_element_to_load(driver, By.ID, \"search-results-container\")\n",
    "        profiles = driver.find_elements(By.CLASS_NAME, \"artdeco-list__item\")\n",
    "        \n",
    "        for profile in profiles:\n",
    "            if not scroll_to_profile(driver, profile):\n",
    "                continue\n",
    "\n",
    "            # time.sleep(2 + random.random() * 6)\n",
    "\n",
    "\n",
    "            if not wait_for_element_to_load(driver, By.CLASS_NAME, \"artdeco-entity-lockup__title\"):\n",
    "                continue\n",
    "\n",
    "            if not click_profile(profile):\n",
    "                continue\n",
    "            \n",
    "            # time.sleep(2 + random.random() * 6)\n",
    "\n",
    "            if not wait_for_element_to_load(driver, By.CLASS_NAME, \"_actions-container_1dg5u8\"):\n",
    "                continue\n",
    "\n",
    "            if not click_three_dots_button(driver):\n",
    "                continue\n",
    "\n",
    "            # time.sleep(2 + random.random() * 6)\n",
    "\n",
    "\n",
    "            if not wait_for_element_to_load(driver, By.CLASS_NAME, \"_visible_x5gf48\"):\n",
    "                continue\n",
    "\n",
    "            normalLinkedInUrl = get_linkedin_url(driver)\n",
    "            if normalLinkedInUrl:\n",
    "                if normalLinkedInUrl in already_scraped_urls:\n",
    "                    print(\"Skipping (already scraped): \" + normalLinkedInUrl)\n",
    "                else:\n",
    "                    to_scrape_urls.append(normalLinkedInUrl)\n",
    "                    with open(toScrapePickle, 'wb') as f:\n",
    "                        pickle.dump(to_scrape_urls, f)\n",
    "                    print(\"Successfully scraped: \" + normalLinkedInUrl)\n",
    "\n",
    "            if not close_popout(driver):\n",
    "                continue\n",
    "\n",
    "            # time.sleep(2 + random.random() * 6)\n",
    "\n",
    "\n",
    "        next_button = wait_for_element_to_load(driver, By.CLASS_NAME, \"artdeco-pagination__button--next\")\n",
    "        if not next_button or not next_button.is_enabled():\n",
    "            break\n",
    "\n",
    "        next_button.click()\n",
    "\n",
    "        if not wait_for_element_to_load(driver, By.CLASS_NAME, \"artdeco-list__item\"):\n",
    "            break\n",
    "\n",
    "    return to_scrape_urls"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d9c83c0",
   "metadata": {},
   "source": [
    "# PROFILE SCRAPING HELPER FUNCTIONS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ce900eb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter():\n",
    "    pass     \n",
    "    # try:\n",
    "        #     WebDriverWait(driver, 240).until(lambda d: d.execute_script('return document.readyState') == 'complete')\n",
    "        # except:\n",
    "        #     driver = reinstantiate_driver()\n",
    "\n",
    "        # # FILTERING\n",
    "        \n",
    "        # likely_founder = True\n",
    "        # relevant_exp = True\n",
    "\n",
    "        # cur_exp = experiences[0]\n",
    "        # relevant_companies = [\"stealth\", \"new\"]\n",
    "        # if any(company in cur_exp.institution_name.split(\" Â·\")[0].lower() for company in relevant_companies) or \"present\" not in cur_exp.to_date.lower():\n",
    "        #     likely_founder = True\n",
    "\n",
    "        # relevant_titles = [\"product\", \"engineer\", \"sales\", \"business development\", \"founder\", \"head\", \"lead\", \"senior\", \"staff\", \"chief\", \"growth\"]\n",
    "        # for experience in experiences[1:5]:\n",
    "        #     if any(title in experience.position_title.lower() for title in relevant_titles):\n",
    "        #         relevant_exp = True\n",
    "        #         break\n",
    "        # relevant_exp = True\n",
    "\n",
    "        # if not (likely_founder and relevant_exp):\n",
    "        #     print(likely_founder, relevant_exp)\n",
    "        #     return None\n",
    "\n",
    "        # person_obj = Person(profile_link, driver = driver, scrape=False, experiences = [None])\n",
    "        # try:\n",
    "        #     WebDriverWait(driver, 240).until(lambda d: d.execute_script('return document.readyState') == 'complete')\n",
    "        # except:\n",
    "        #     driver = reinstantiate_driver(driver)\n",
    "        # time.sleep(2 + random.random() * 7)\n",
    "        \n",
    "# Helper function to scrape experiences\n",
    "# Returns scraped experiences if successful, otherwise returns empty list\n",
    "def get_experiences(driver):\n",
    "    scraped_experiences = []\n",
    "\n",
    "    try:\n",
    "        wait_for_element_to_load(driver, By.CSS_SELECTOR, \"li.pvs-list__paged-list-item.artdeco-list__item.pvs-list__item--line-separated.pvs-list__item--one-column\")\n",
    "        experience_items = driver.find_elements(By.CSS_SELECTOR, \"li.pvs-list__paged-list-item.artdeco-list__item.pvs-list__item--line-separated.pvs-list__item--one-column\")\n",
    "\n",
    "        if len(experience_items) > 0:\n",
    "            for item in experience_items:\n",
    "                hidden_spans = item.find_elements(By.CSS_SELECTOR, \"span.visually-hidden\")\n",
    "                experience_texts = [span.text for span in hidden_spans]\n",
    "                experience = {\n",
    "                    \"title: \": experience_texts[0],\n",
    "                    \"company: \": experience_texts[1],\n",
    "                    \"dates: \": experience_texts[2],\n",
    "                }\n",
    "                \n",
    "                if len(experience_texts) > 3:\n",
    "                    experience[\"location: \"] = experience_texts[3]\n",
    "                if len(experience_texts) > 4:\n",
    "                    experience[\"summary: \"] = experience_texts[4]\n",
    "                if len(experience_texts) > 5:\n",
    "                    experience[\"remaining: \"] = (\", \").join(experience_texts[5:])\n",
    "                scraped_experiences.append(experience)\n",
    "\n",
    "            print(\"Successfully scraped experiences\")\n",
    "        else:\n",
    "            print(\"No experiences found\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        print(\"ERROR: No experiences found\")\n",
    "\n",
    "    return scraped_experiences\n",
    "\n",
    "\n",
    "# Helper function to scrape education\n",
    "# Returns scraped education if successful, otherwise returns empty list\n",
    "def get_education(driver):\n",
    "    scraped_education = []\n",
    "\n",
    "    try:\n",
    "        wait_for_element_to_load(driver, By.CSS_SELECTOR, \"li.pvs-list__paged-list-item.artdeco-list__item.pvs-list__item--line-separated.pvs-list__item--one-column\")\n",
    "        education_items = driver.find_elements(By.CSS_SELECTOR, \"li.pvs-list__paged-list-item.artdeco-list__item.pvs-list__item--line-separated.pvs-list__item--one-column\")\n",
    "\n",
    "        if len(education_items) > 0:\n",
    "            for item in education_items:\n",
    "                hidden_spans = item.find_elements(By.CSS_SELECTOR, \"span.visually-hidden\")\n",
    "                education_texts = [span.text for span in hidden_spans]\n",
    "                education = {\n",
    "                    \"school: \": education_texts[0],\n",
    "                }\n",
    "                \n",
    "                if len(education_texts) > 1:\n",
    "                    education[\"degree: \"] = education_texts[1]\n",
    "                if len(education_texts) > 2:\n",
    "                    education[\"dates: \"] = education_texts[2]\n",
    "                if len(education_texts) > 3:\n",
    "                    education[\"remaining: \"] = (\", \").join(education_texts[5:])\n",
    "                scraped_education.append(education)\n",
    "\n",
    "            print(\"Successfully scraped education\")\n",
    "        else:\n",
    "            print(\"No education found\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        print(\"ERROR: No education found\")\n",
    "\n",
    "    return scraped_education\n",
    "\n",
    "\n",
    "# Helper function to scrape degree of connection and mutuals\n",
    "# Returns scraped degree of connection and mutuals if successful, otherwise returns N/A\n",
    "def get_degree_of_connection_and_mutuals(driver):\n",
    "    scraped_profile_dist = \"4+\"\n",
    "    scraped_mutuals = \"N/A\"\n",
    "\n",
    "    try:\n",
    "        scraped_profile_dist = wait_for_element_to_load(driver, By.CSS_SELECTOR, \"span.dist-value\").text\n",
    "        if scraped_profile_dist == \"1st\" or scraped_profile_dist == \"2nd\":\n",
    "            try:\n",
    "                span_element = wait_for_element_to_load(driver, By.CSS_SELECTOR, \"span.t-normal.t-black--light.t-14.hoverable-link-text\")\n",
    "                scraped_mutuals = span_element.text.split('\\n')[0]\n",
    "                print(\"Successfully found mutual connections: \" + scraped_mutuals)\n",
    "            except:\n",
    "                print(\"ERROR: mutuals not found\")\n",
    "        print(\"Successfully scraped degree of connection: \" + scraped_profile_dist)\n",
    "\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        print(\"No degree of connection found\")\n",
    "\n",
    "    return scraped_profile_dist, scraped_mutuals\n",
    "\n",
    "# Helper function to scrape description\n",
    "# Returns scraped description if successful, otherwise returns N/A\n",
    "def get_description(driver):\n",
    "    scraped_description = \"N/A\"\n",
    "    \n",
    "    try:\n",
    "        scraped_description = driver.find_element(By.CLASS_NAME, \"text-body-medium.break-words\").text\n",
    "        print(\"Successfully scraped description: \" + scraped_description)\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        print(\"ERROR: description not found\")\n",
    "        \n",
    "    return scraped_description\n",
    "\n",
    "# Main function to scrape degree of connection and mutuals\n",
    "# Returns scraped degree of connection and mutuals if successful, otherwise returns N/A\n",
    "def scrape_profile(driver, scraped_link):\n",
    "\n",
    "    # Scrape Name\n",
    "    driver.get(scraped_link)\n",
    "    scraped_profile_name = wait_for_element_to_load(driver, By.CSS_SELECTOR, \"h1.text-heading-xlarge.inline.t-24.v-align-middle.break-words\").text\n",
    "    time.sleep(2 + random.random() * 6)\n",
    "\n",
    "    # Scrape Experiences\n",
    "    experiences_url = os.path.join(scraped_link, \"details/experience\")\n",
    "    driver.get(experiences_url)\n",
    "    scraped_experiences = get_experiences(driver)\n",
    "    time.sleep(2 + random.random() * 6)\n",
    "\n",
    "\n",
    "    # Scrape Education\n",
    "    education_url = os.path.join(scraped_link, \"details/education\")\n",
    "    driver.get(education_url)\n",
    "    scraped_education = get_education(driver)\n",
    "    time.sleep(2 + random.random() * 6)\n",
    "\n",
    "\n",
    "    # Scrape degree of connection and mutuals if available\n",
    "    driver.get(scraped_link)\n",
    "    scraped_profile_dist, scraped_mutuals = get_degree_of_connection_and_mutuals(driver)\n",
    "    time.sleep(2 + random.random() * 6)\n",
    "\n",
    "    # Scrape description\n",
    "    scraped_description = get_description(driver)\n",
    "    time.sleep(2 + random.random() * 6)\n",
    "\n",
    "\n",
    "    # Scape profile link\n",
    "    scraped_link = driver.current_url\n",
    "    print(\"Successfully scraped profile link: \" + scraped_link)\n",
    "    time.sleep(2 + random.random() * 6)\n",
    "\n",
    "    \n",
    "    profile = ScrapedProfile(scraped_profile_name,\n",
    "                   scraped_experiences,\n",
    "                   scraped_education,\n",
    "                   scraped_profile_dist,\n",
    "                   scraped_mutuals,\n",
    "                   scraped_description,\n",
    "                   scraped_link)\n",
    "    print(\"\\nSuccess!\")\n",
    "    print(f\"Name: {profile.profile_name}\")\n",
    "    print(f\"Experiences: {profile.experiences}\")\n",
    "    print(f\"Education: {profile.education}\")\n",
    "    print(f\"Profile Distance: {profile.profile_dist}\")\n",
    "    print(f\"Mutuals: {profile.mutuals}\")\n",
    "    print(f\"Description: {profile.profile_description}\")\n",
    "    print(f\"Link: {profile.profile_link}\\n\")\n",
    "    return profile\n",
    "\n",
    "def scrape_all_profiles(driver, historyPickle, toScrapePickle, resultsPickle, failedURLsTextFile):\n",
    "    results = []\n",
    "    scraped_urls = []\n",
    "    history = []\n",
    "\n",
    "    with open(toScrapePickle, 'rb') as f:\n",
    "        try:\n",
    "            to_scrape_urls = pickle.load(f)\n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "            print(\"Error loading to scrape url pickle file\")\n",
    "        \n",
    "    totalCount = len(to_scrape_urls)\n",
    "\n",
    "    print(f'# of profiles to scrape: {totalCount}')\n",
    "    \n",
    "\n",
    "    for i in range(len(to_scrape_urls) - 1, -1, -1):\n",
    "        url = to_scrape_urls[i]\n",
    "        print(f'At index: {len(to_scrape_urls) - i} - url: {url}')\n",
    "\n",
    "        with open(historyPickle, 'rb') as f:\n",
    "            try:\n",
    "                history = pickle.load(f)\n",
    "            except Exception as e:\n",
    "                print(e)\n",
    "                print(\"Error loading history pickle file\")\n",
    "\n",
    "        try:\n",
    "            profile = scrape_profile(driver, url)\n",
    "            if profile != None:\n",
    "                print(\"saving profile info\")\n",
    "                results.append(profile)\n",
    "\n",
    "                with open(resultsPickle, 'wb') as f:\n",
    "                    pickle.dump(results, f)\n",
    "                \n",
    "                print(\"adding to history\")\n",
    "                history.append(url)\n",
    "                with open(historyPickle, 'wb') as f:\n",
    "                    pickle.dump(history, f)\n",
    "                \n",
    "                print(\"recording scraped url\")\n",
    "                scraped_urls.append(url)\n",
    "            else:\n",
    "                print(\"profile filtered out\")\n",
    "            \n",
    "            print(\"removing from to-scrape\")\n",
    "            to_scrape_urls.remove(url)\n",
    "            with open(toScrapePickle, 'wb') as f:\n",
    "                pickle.dump(to_scrape_urls, f)\n",
    "\n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "            print('Failed to scrape profile: ', url)\n",
    "\n",
    "            # TODO: CHANGED FAILED URLS\n",
    "            with open(failedURLsTextFile, 'a') as f:\n",
    "                f.write(url + '\\n')\n",
    "\n",
    "        print(((totalCount - i)/totalCount) * 100, '% Done - at index:', totalCount - i)\n",
    "        print('\\n----------------------------------------------------------------------------------------------------------\\n')\n",
    "    return results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdd18909",
   "metadata": {},
   "source": [
    "# EXPORTING HELPER FUNCTIONS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "80407bba",
   "metadata": {},
   "outputs": [],
   "source": [
    "def parseProfile(profile: ScrapedProfile):\n",
    "    res = {}\n",
    "    res['url'] = profile.profile_link\n",
    "    res['name'] = profile.profile_name\n",
    "    res['dist'] = profile.profile_dist\n",
    "    res['description'] = profile.profile_description\n",
    "\n",
    "    #Experiences first\n",
    "    print(profile.experiences)\n",
    "    for i, e in enumerate(profile.experiences):\n",
    "        res[f'exp{i} title'] = e[\"title: \"]\n",
    "        res[f'exp{i} company'] = e[\"company: \"]\n",
    "        res[f'exp{i} dates'] = e[\"dates: \"]\n",
    "\n",
    "    # School second\n",
    "    for i, e in enumerate(profile.profile_school):\n",
    "        res[f'edu{i} school'] = e[\"school: \"]\n",
    "        if \"degree :\" in e:\n",
    "            res[f'edu{i} degree'] = e[\"degree: \"]\n",
    "\n",
    "    \n",
    "    return res"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "095b1a61-bfdb-4ec3-b5f5-8bc81d74a68d",
   "metadata": {},
   "source": [
    "# QUERY 1: MAIN COMPANIES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "aca7a719",
   "metadata": {},
   "outputs": [],
   "source": [
    "historyPickle = 'db/main_history.pickle'\n",
    "toScrapePickle = 'db/main_to_scrape.pickle'\n",
    "resultsPickle = 'db/main_results.pickle'\n",
    "failedURLsTextFile = 'db/main_failed_urls.txt'\n",
    "\n",
    "SALES_NAV_SEARCH_URL = \"\"\"\n",
    "https://www.linkedin.com/sales/search/people?savedSearchId=1833971580&sessionId=jFoOWCqiTl2JVJ3QPZrL4g%3D%3D\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93127f7e",
   "metadata": {},
   "source": [
    "### Sales nav scraper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5e0ea7d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4\n",
      "302\n"
     ]
    }
   ],
   "source": [
    "already_scraped_urls, to_scrape_urls = check_pickles(historyPickle, toScrapePickle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e83345e3-d166-4e30-a7d6-2634b0db34a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully scraped: https://www.linkedin.com/in/andrewschen\n",
      "Successfully scraped: https://www.linkedin.com/in/arturomenacruz\n",
      "Successfully scraped: https://www.linkedin.com/in/zalkar\n",
      "Successfully scraped: https://www.linkedin.com/in/calebelston\n",
      "Successfully scraped: https://www.linkedin.com/in/bnhop\n",
      "Successfully scraped: https://www.linkedin.com/in/sonalisastry\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[9], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m----> 2\u001b[0m     updated_to_scrape_urls \u001b[38;5;241m=\u001b[39m \u001b[43mscrape_profiles\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdriver\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mSALES_NAV_SEARCH_URL\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43malready_scraped_urls\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mto_scrape_urls\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtoScrapePickle\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      3\u001b[0m     \u001b[38;5;28mprint\u001b[39m(updated_to_scrape_urls)\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "Cell \u001b[0;32mIn[4], line 166\u001b[0m, in \u001b[0;36mscrape_profiles\u001b[0;34m(driver, SALES_NAV_SEARCH_URL, already_scraped_urls, to_scrape_urls, toScrapePickle)\u001b[0m\n\u001b[1;32m    163\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m wait_for_element_to_load(driver, By\u001b[38;5;241m.\u001b[39mCLASS_NAME, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_actions-container_1dg5u8\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m    164\u001b[0m     \u001b[38;5;28;01mcontinue\u001b[39;00m\n\u001b[0;32m--> 166\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[43mclick_three_dots_button\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdriver\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[1;32m    167\u001b[0m     \u001b[38;5;28;01mcontinue\u001b[39;00m\n\u001b[1;32m    169\u001b[0m \u001b[38;5;66;03m# time.sleep(2 + random.random() * 6)\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[4], line 99\u001b[0m, in \u001b[0;36mclick_three_dots_button\u001b[0;34m(driver)\u001b[0m\n\u001b[1;32m     97\u001b[0m     actionContainer \u001b[38;5;241m=\u001b[39m driver\u001b[38;5;241m.\u001b[39mfind_element(By\u001b[38;5;241m.\u001b[39mCLASS_NAME, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_actions-container_1dg5u8\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     98\u001b[0m     threeDotsButton \u001b[38;5;241m=\u001b[39m actionContainer\u001b[38;5;241m.\u001b[39mfind_element(By\u001b[38;5;241m.\u001b[39mCLASS_NAME, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_icon_ps32ck\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 99\u001b[0m     \u001b[43mthreeDotsButton\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mclick\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    100\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m    101\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (NoSuchElementException, ElementClickInterceptedException) \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/selenium/webdriver/remote/webelement.py:94\u001b[0m, in \u001b[0;36mWebElement.click\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     92\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mclick\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m     93\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Clicks the element.\"\"\"\u001b[39;00m\n\u001b[0;32m---> 94\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execute\u001b[49m\u001b[43m(\u001b[49m\u001b[43mCommand\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mCLICK_ELEMENT\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/selenium/webdriver/remote/webelement.py:395\u001b[0m, in \u001b[0;36mWebElement._execute\u001b[0;34m(self, command, params)\u001b[0m\n\u001b[1;32m    393\u001b[0m     params \u001b[38;5;241m=\u001b[39m {}\n\u001b[1;32m    394\u001b[0m params[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mid\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_id\n\u001b[0;32m--> 395\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_parent\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexecute\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcommand\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparams\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/selenium/webdriver/remote/webdriver.py:345\u001b[0m, in \u001b[0;36mWebDriver.execute\u001b[0;34m(self, driver_command, params)\u001b[0m\n\u001b[1;32m    342\u001b[0m     \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msessionId\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m params:\n\u001b[1;32m    343\u001b[0m         params[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msessionId\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msession_id\n\u001b[0;32m--> 345\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcommand_executor\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexecute\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdriver_command\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparams\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    346\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m response:\n\u001b[1;32m    347\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39merror_handler\u001b[38;5;241m.\u001b[39mcheck_response(response)\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/selenium/webdriver/remote/remote_connection.py:302\u001b[0m, in \u001b[0;36mRemoteConnection.execute\u001b[0;34m(self, command, params)\u001b[0m\n\u001b[1;32m    300\u001b[0m trimmed \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_trim_large_entries(params)\n\u001b[1;32m    301\u001b[0m LOGGER\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m\"\u001b[39m, command_info[\u001b[38;5;241m0\u001b[39m], url, \u001b[38;5;28mstr\u001b[39m(trimmed))\n\u001b[0;32m--> 302\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_request\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcommand_info\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbody\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/selenium/webdriver/remote/remote_connection.py:322\u001b[0m, in \u001b[0;36mRemoteConnection._request\u001b[0;34m(self, method, url, body)\u001b[0m\n\u001b[1;32m    319\u001b[0m     body \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    321\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mkeep_alive:\n\u001b[0;32m--> 322\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_conn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbody\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbody\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mheaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mheaders\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    323\u001b[0m     statuscode \u001b[38;5;241m=\u001b[39m response\u001b[38;5;241m.\u001b[39mstatus\n\u001b[1;32m    324\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/urllib3/request.py:78\u001b[0m, in \u001b[0;36mRequestMethods.request\u001b[0;34m(self, method, url, fields, headers, **urlopen_kw)\u001b[0m\n\u001b[1;32m     74\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrequest_encode_url(\n\u001b[1;32m     75\u001b[0m         method, url, fields\u001b[38;5;241m=\u001b[39mfields, headers\u001b[38;5;241m=\u001b[39mheaders, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39murlopen_kw\n\u001b[1;32m     76\u001b[0m     )\n\u001b[1;32m     77\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m---> 78\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrequest_encode_body\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     79\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfields\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfields\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mheaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43murlopen_kw\u001b[49m\n\u001b[1;32m     80\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/urllib3/request.py:170\u001b[0m, in \u001b[0;36mRequestMethods.request_encode_body\u001b[0;34m(self, method, url, fields, headers, encode_multipart, multipart_boundary, **urlopen_kw)\u001b[0m\n\u001b[1;32m    167\u001b[0m extra_kw[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mheaders\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mupdate(headers)\n\u001b[1;32m    168\u001b[0m extra_kw\u001b[38;5;241m.\u001b[39mupdate(urlopen_kw)\n\u001b[0;32m--> 170\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43murlopen\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mextra_kw\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/urllib3/poolmanager.py:376\u001b[0m, in \u001b[0;36mPoolManager.urlopen\u001b[0;34m(self, method, url, redirect, **kw)\u001b[0m\n\u001b[1;32m    374\u001b[0m     response \u001b[38;5;241m=\u001b[39m conn\u001b[38;5;241m.\u001b[39murlopen(method, url, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkw)\n\u001b[1;32m    375\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 376\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[43mconn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43murlopen\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mu\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrequest_uri\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkw\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    378\u001b[0m redirect_location \u001b[38;5;241m=\u001b[39m redirect \u001b[38;5;129;01mand\u001b[39;00m response\u001b[38;5;241m.\u001b[39mget_redirect_location()\n\u001b[1;32m    379\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m redirect_location:\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/urllib3/connectionpool.py:703\u001b[0m, in \u001b[0;36mHTTPConnectionPool.urlopen\u001b[0;34m(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, **response_kw)\u001b[0m\n\u001b[1;32m    700\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_prepare_proxy(conn)\n\u001b[1;32m    702\u001b[0m \u001b[38;5;66;03m# Make the request on the httplib connection object.\u001b[39;00m\n\u001b[0;32m--> 703\u001b[0m httplib_response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_make_request\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    704\u001b[0m \u001b[43m    \u001b[49m\u001b[43mconn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    705\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    706\u001b[0m \u001b[43m    \u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    707\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout_obj\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    708\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbody\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbody\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    709\u001b[0m \u001b[43m    \u001b[49m\u001b[43mheaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    710\u001b[0m \u001b[43m    \u001b[49m\u001b[43mchunked\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mchunked\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    711\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    713\u001b[0m \u001b[38;5;66;03m# If we're going to release the connection in ``finally:``, then\u001b[39;00m\n\u001b[1;32m    714\u001b[0m \u001b[38;5;66;03m# the response doesn't need to know about the connection. Otherwise\u001b[39;00m\n\u001b[1;32m    715\u001b[0m \u001b[38;5;66;03m# it will also try to release it and we'll have a double-release\u001b[39;00m\n\u001b[1;32m    716\u001b[0m \u001b[38;5;66;03m# mess.\u001b[39;00m\n\u001b[1;32m    717\u001b[0m response_conn \u001b[38;5;241m=\u001b[39m conn \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m release_conn \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/urllib3/connectionpool.py:449\u001b[0m, in \u001b[0;36mHTTPConnectionPool._make_request\u001b[0;34m(self, conn, method, url, timeout, chunked, **httplib_request_kw)\u001b[0m\n\u001b[1;32m    444\u001b[0m             httplib_response \u001b[38;5;241m=\u001b[39m conn\u001b[38;5;241m.\u001b[39mgetresponse()\n\u001b[1;32m    445\u001b[0m         \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    446\u001b[0m             \u001b[38;5;66;03m# Remove the TypeError from the exception chain in\u001b[39;00m\n\u001b[1;32m    447\u001b[0m             \u001b[38;5;66;03m# Python 3 (including for exceptions like SystemExit).\u001b[39;00m\n\u001b[1;32m    448\u001b[0m             \u001b[38;5;66;03m# Otherwise it looks like a bug in the code.\u001b[39;00m\n\u001b[0;32m--> 449\u001b[0m             \u001b[43msix\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mraise_from\u001b[49m\u001b[43m(\u001b[49m\u001b[43me\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m    450\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (SocketTimeout, BaseSSLError, SocketError) \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    451\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_raise_timeout(err\u001b[38;5;241m=\u001b[39me, url\u001b[38;5;241m=\u001b[39murl, timeout_value\u001b[38;5;241m=\u001b[39mread_timeout)\n",
      "File \u001b[0;32m<string>:3\u001b[0m, in \u001b[0;36mraise_from\u001b[0;34m(value, from_value)\u001b[0m\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/urllib3/connectionpool.py:444\u001b[0m, in \u001b[0;36mHTTPConnectionPool._make_request\u001b[0;34m(self, conn, method, url, timeout, chunked, **httplib_request_kw)\u001b[0m\n\u001b[1;32m    441\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[1;32m    442\u001b[0m     \u001b[38;5;66;03m# Python 3\u001b[39;00m\n\u001b[1;32m    443\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 444\u001b[0m         httplib_response \u001b[38;5;241m=\u001b[39m \u001b[43mconn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgetresponse\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    445\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    446\u001b[0m         \u001b[38;5;66;03m# Remove the TypeError from the exception chain in\u001b[39;00m\n\u001b[1;32m    447\u001b[0m         \u001b[38;5;66;03m# Python 3 (including for exceptions like SystemExit).\u001b[39;00m\n\u001b[1;32m    448\u001b[0m         \u001b[38;5;66;03m# Otherwise it looks like a bug in the code.\u001b[39;00m\n\u001b[1;32m    449\u001b[0m         six\u001b[38;5;241m.\u001b[39mraise_from(e, \u001b[38;5;28;01mNone\u001b[39;00m)\n",
      "File \u001b[0;32m/Library/Developer/CommandLineTools/Library/Frameworks/Python3.framework/Versions/3.9/lib/python3.9/http/client.py:1349\u001b[0m, in \u001b[0;36mHTTPConnection.getresponse\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1347\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1348\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1349\u001b[0m         \u001b[43mresponse\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbegin\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1350\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mConnectionError\u001b[39;00m:\n\u001b[1;32m   1351\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclose()\n",
      "File \u001b[0;32m/Library/Developer/CommandLineTools/Library/Frameworks/Python3.framework/Versions/3.9/lib/python3.9/http/client.py:316\u001b[0m, in \u001b[0;36mHTTPResponse.begin\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    314\u001b[0m \u001b[38;5;66;03m# read until we get a non-100 response\u001b[39;00m\n\u001b[1;32m    315\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[0;32m--> 316\u001b[0m     version, status, reason \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_read_status\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    317\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m status \u001b[38;5;241m!=\u001b[39m CONTINUE:\n\u001b[1;32m    318\u001b[0m         \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "File \u001b[0;32m/Library/Developer/CommandLineTools/Library/Frameworks/Python3.framework/Versions/3.9/lib/python3.9/http/client.py:277\u001b[0m, in \u001b[0;36mHTTPResponse._read_status\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    276\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_read_status\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m--> 277\u001b[0m     line \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mstr\u001b[39m(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreadline\u001b[49m\u001b[43m(\u001b[49m\u001b[43m_MAXLINE\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124miso-8859-1\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    278\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(line) \u001b[38;5;241m>\u001b[39m _MAXLINE:\n\u001b[1;32m    279\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m LineTooLong(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstatus line\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m/Library/Developer/CommandLineTools/Library/Frameworks/Python3.framework/Versions/3.9/lib/python3.9/socket.py:704\u001b[0m, in \u001b[0;36mSocketIO.readinto\u001b[0;34m(self, b)\u001b[0m\n\u001b[1;32m    702\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[1;32m    703\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 704\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sock\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrecv_into\u001b[49m\u001b[43m(\u001b[49m\u001b[43mb\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    705\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m timeout:\n\u001b[1;32m    706\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_timeout_occurred \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "try:\n",
    "    updated_to_scrape_urls = scrape_profiles(driver, SALES_NAV_SEARCH_URL, already_scraped_urls, to_scrape_urls, toScrapePickle)\n",
    "    print(updated_to_scrape_urls)\n",
    "except Exception as e:\n",
    "    print(f\"An unexpected error occurred: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b1c8979-0304-4621-ae96-59108d44bada",
   "metadata": {},
   "source": [
    "### Deduplicate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0ea0fe49",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Removed 0 duplicates\n",
      "308\n"
     ]
    }
   ],
   "source": [
    "deduplicate(toScrapePickle, to_scrape_urls, already_scraped_urls)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6da7736-3078-4e03-8be6-967f48640cdc",
   "metadata": {},
   "source": [
    "### Profile scraper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7187f14e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# of profiles to scrape: 308\n",
      "At index: 1 - url: https://www.linkedin.com/in/oscar-garcia-0b49ab12a\n",
      "Successfully scraped experiences\n",
      "Successfully scraped education\n",
      "Successfully found mutual connections: Rishen Kapoor ã, Jon Gelsey, and 2 other mutual connections\n",
      "Successfully scraped degree of connection: 2nd\n",
      "Successfully scraped description: Commercial Account Executive at Descope\n",
      "Successfully scraped profile link: https://www.linkedin.com/in/oscar-garcia-0b49ab12a/\n",
      "\n",
      "Success!\n",
      "Name: Oscar Garcia\n",
      "Experiences: [{'title: ': 'Commercial Account Executive', 'company: ': 'Descope Â· Full-time', 'dates: ': 'Apr 2024 to Present Â· 5 mos', 'location: ': 'Seattle, Washington, United States Â· Remote', 'summary: ': 'LinkedIn helped me get this job', 'remaining: ': 'Sales'}, {'title: ': 'Account Executive', 'company: ': 'Stealth Startup Â· Full-time', 'dates: ': 'Jul 2023 to Apr 2024 Â· 10 mos', 'location: ': 'Seattle, Washington, United States Â· Hybrid'}, {'title: ': 'Emerging Business Account Executive', 'company: ': 'Okta Â· Full-time', 'dates: ': 'Feb 2022 to Jul 2023 Â· 1 yr 6 mos', 'location: ': 'Seattle, Washington, United States'}, {'title: ': 'Auth0', 'company: ': '4 yrs 9 mos', 'dates: ': 'Bellevue, WA', 'location: ': 'Small Medium Business Account Executive', 'summary: ': 'Full-time', 'remaining: ': 'Dec 2019 to Apr 2022 Â· 2 yrs 5 mos, Working with growing organizations to help them scale and future-proof their customer facing applications with secure, extensible, standards based identity software as a service. Auth0 makes life easier for application builders by providing an incredibly simple and flexible solution for practically any identity need. We enable our clients to focus on their core product and outsource the risky and tedious challenge of securing their applications., Sr. Account Development Representative, Aug 2017 to Dec 2019 Â· 2 yrs 5 mos, At Auth0, we got tired of the friction caused by complex identity environments. So we built a zero-friction, infinitely extensible, enterprise-class web-scale cloud solution that makes identity easy. Our Mission - Zero-friction authentication and authorization for developers., Bluetooth: Standards Based Organization Implements Standards Based Authentication - Auth0, , Safari: Auth0 SSO Drives B2B Expansion - Auth0, Safari needed a flexible SSO platform to simplify enterprise customer integrations and drive its B2B expansion.'}, {'title: ': 'Mid Market SDR', 'company: ': 'Pushpay', 'dates: ': 'Dec 2015 to Aug 2017 Â· 1 yr 9 mos', 'location: ': '18300 Redmond Way, Redmond, WA'}, {'title: ': 'Server', 'company: ': \"Emory's Lakehouse Bistro\", 'dates: ': 'Jan 2011 to Dec 2015 Â· 5 yrs', 'location: ': 'Everett, WA'}]\n",
      "Education: [{'school: ': 'Everett Community College', 'degree: ': '2014 - 2015'}]\n",
      "Profile Distance: 2nd\n",
      "Mutuals: Rishen Kapoor ã, Jon Gelsey, and 2 other mutual connections\n",
      "Description: Commercial Account Executive at Descope\n",
      "Link: https://www.linkedin.com/in/oscar-garcia-0b49ab12a/\n",
      "\n",
      "saving profile info\n",
      "adding to history\n",
      "recording scraped url\n",
      "removing from to-scrape\n",
      "0.3246753246753247 % Done - at index: 1\n",
      "\n",
      "----------------------------------------------------------------------------------------------------------\n",
      "\n",
      "At index: 1 - url: https://www.linkedin.com/in/vaniatominc\n",
      "Successfully scraped experiences\n",
      "Successfully scraped education\n",
      "Successfully scraped degree of connection: 3rd\n",
      "Successfully scraped description: Software Engineer | JavaScript js | Frontend | Women in Tech | Cybersecurity | Cryptocurrency\n",
      "Successfully scraped profile link: https://www.linkedin.com/in/vaniatominc/\n",
      "\n",
      "Success!\n",
      "Name: Vania Tominc\n",
      "Experiences: [{'title: ': 'CrowdStrike', 'company: ': 'Full-time Â· 2 yrs 7 mos', 'dates: ': 'London, England, United Kingdom', 'location: ': 'Software Engineer II', 'summary: ': 'Feb 2024 to Present Â· 7 mos', 'remaining: ': 'Software Engineer, Feb 2022 to Feb 2024 Â· 2 yrs 1 mo'}, {'title: ': 'FOH Assistant', 'company: ': 'Aldwych Theatre Â· Full-time', 'dates: ': 'Sep 2021 to Jan 2022 Â· 5 mos', 'location: ': 'London, England, United Kingdom'}, {'title: ': 'General Assembly - Software Engineering Student', 'company: ': 'General Assembly Â· Full-time', 'dates: ': 'May 2021 to Aug 2021 Â· 4 mos', 'location: ': 'London, England, United Kingdom', 'summary: ': 'A 12-week (full-time) coding bootcamp designed for a career transformation. During the course, we had daily stand-ups, online lessons, and labs. With projects at the end of each module, I learned how to build a full-stack app by either pair-coding or individual coding. For more information on all the projects I\\'ve built during the course, please check \"Projects\\' ReadMes\" under the Accomplishments section.', 'remaining: ': 'Project 04 - TruffÂ·lÂ·uxury, ReadMe: https://github.com/VaniaTominc/GA-Project-04 Deployed version: https://truffluxury.herokuapp.com/, Project 03 - Pistis Trust, ReadMe: https://github.com/VaniaTominc/GA-Project-03 Deployed Version: https://pistis-trust.herokuapp.com/, Project 02 - Rick and Morty, ReadMe: https://github.com/VaniaTominc/GA-Project-02 Deployed version: https://rickandmorty-ga.netlify.app/'}, {'title: ': 'Personal Assistant', 'company: ': 'Active Care Group Â· Full-time', 'dates: ': 'Feb 2019 to Aug 2021 Â· 2 yrs 7 mos', 'location: ': '- Supporting people with spinal cord injury to live an independent and active life. - Outstanding personal qualities applied: respectful, empathetic, reliable, patient, observant, good communication skills.'}]\n",
      "Education: [{'school: ': 'General Assembly', 'degree: ': 'Software Engineering Immersive', 'dates: ': 'May 2021 - Aug 2021', 'remaining: ': ''}, {'school: ': 'University of Ljubljana, Faculty of Arts', 'degree: ': \"Bachelor's and Master degree, Spanish Language and Literature, History\", 'dates: ': '2016', 'remaining: ': ''}]\n",
      "Profile Distance: 3rd\n",
      "Mutuals: N/A\n",
      "Description: Software Engineer | JavaScript js | Frontend | Women in Tech | Cybersecurity | Cryptocurrency\n",
      "Link: https://www.linkedin.com/in/vaniatominc/\n",
      "\n",
      "saving profile info\n",
      "adding to history\n",
      "recording scraped url\n",
      "removing from to-scrape\n",
      "0.6493506493506493 % Done - at index: 2\n",
      "\n",
      "----------------------------------------------------------------------------------------------------------\n",
      "\n",
      "At index: 1 - url: https://www.linkedin.com/in/alex-mentzelopoulos-904644178\n",
      "Successfully scraped experiences\n",
      "Successfully scraped education\n",
      "Successfully found mutual connections: Poni Bepo is a mutual connection\n",
      "Successfully scraped degree of connection: 2nd\n",
      "Successfully scraped description: Stealth Start-up, ex-Google\n",
      "Successfully scraped profile link: https://www.linkedin.com/in/alex-mentzelopoulos-904644178/\n",
      "\n",
      "Success!\n",
      "Name: Alex Mentzelopoulos\n",
      "Experiences: [{'title: ': 'Founder', 'company: ': 'Stealth Startup Â· Full-time', 'dates: ': 'Jan 2024 to Present Â· 8 mos', 'location: ': 'Exploring ideas', 'summary: ': 'Technical Project Leadership Â· Start-up Leadership Â· Software Product Management Â· User Experience (UX)'}, {'title: ': 'Strategic Partnerships', 'company: ': 'Google Â· Full-time', 'dates: ': 'Aug 2020 to May 2024 Â· 3 yrs 10 mos', 'location: ': 'London, England, United Kingdom', 'summary: ': 'Building Adtech/Martech integrations and new features together with Google partners in EMEA driving $XXX million in annual revenue I also undertake as part of 20% time, the testing of generative AI use-cases/features before deployment to help flag alignment/ethics concerns.', 'remaining: ': 'C-Level Â· Strategic Negotiations Â· Growth Strategies Â· Product Vision Â· Technology Consulting Â· Customer Success Â· Machine Learning Â· Product Management Â· Software as a Service (SaaS)'}, {'title: ': 'MBA Candidate', 'company: ': 'London Business School', 'dates: ': 'Sep 2018 to Jul 2020 Â· 1 yr 11 mos', 'location: ': 'London, England, United Kingdom'}, {'title: ': 'MBA Intern', 'company: ': 'Google', 'dates: ': 'Jun 2019 to Sep 2019 Â· 4 mos', 'location: ': 'London, United Kingdom'}, {'title: ': 'Product, Operations and Marketing', 'company: ': 'Luggage Forward', 'dates: ': 'Aug 2014 to Aug 2018 Â· 4 yrs 1 mo', 'location: ': 'Boston/London', 'summary: ': 'Start-up providing worldwide door-to-door luggage delivery so you can enjoy the journey without the baggage. Led a number of different areas including product, digital marketing, and operations. Highlights: -Generated, prioritised and evaluated features in the shaping of a new product, growth of 50% month on month, reaching 20K+ YAU -Managed 7-member operations team, implementing processes to improve operational KPIs -Proposed, coordinated, and launched European entity in London, generating new service offerings -Oversaw and grew digital marketing channels such as Google Adwords, Facebook Ads, email marketing, and content marketing -Initiated, developed, and proto-typed a laser-cut polymer luggage tag solution to achieve further automation and cost savings', 'remaining: ': 'Growth Strategies Â· Product Vision Â· Customer Success Â· Product Management Â· Software as a Service (SaaS)'}, {'title: ': 'Researcher', 'company: ': 'Massachusetts Institute of Technology', 'dates: ': '2012 to 2012 Â· Less than a year', 'location: ': 'Cambridge, MA', 'summary: ': 'MIT Quantitative Research in Linguistics Group Built stochastic models to predict stress patterns and quantitatively assessed weight distinctions in English. Implemented Noam Chomskyâs Main Stress Rule on English corpora in Python using regular expressions. Designed and ran artificial language experiments on Amazon Mechanical Turk', 'remaining: ': 'Machine Learning'}, {'title: ': 'Software Development Intern', 'company: ': 'MathWorks', 'dates: ': '2011 to 2011 Â· Less than a year', 'location: ': 'Natick, MA', 'summary: ': 'Improved computational accuracy of upper tail probabilities in over 25 probability distributions for MATLAB. Added software support for new probability distributions included in the 2012 release', 'remaining: ': 'Machine Learning'}]\n",
      "Education: [{'school: ': 'Massachusetts Institute of Technology', 'degree: ': \"Bachelor's - BSc\", 'dates: ': 'Coursework: Physics, Music, Linguistics'}, {'school: ': 'University of Bath', 'degree: ': 'Master of Science - MS, Artificial Intelligence'}, {'school: ': 'London Business School', 'degree: ': 'MBA, Business Administration and Management, General', 'dates: ': '2018 - 2020', 'remaining: ': ''}, {'school: ': 'New England Conservatory of Music', 'degree: ': 'Classical Guitar', 'dates: ': 'Private study - performance'}]\n",
      "Profile Distance: 2nd\n",
      "Mutuals: Poni Bepo is a mutual connection\n",
      "Description: Stealth Start-up, ex-Google\n",
      "Link: https://www.linkedin.com/in/alex-mentzelopoulos-904644178/\n",
      "\n",
      "saving profile info\n",
      "adding to history\n",
      "recording scraped url\n",
      "removing from to-scrape\n",
      "0.974025974025974 % Done - at index: 3\n",
      "\n",
      "----------------------------------------------------------------------------------------------------------\n",
      "\n",
      "At index: 1 - url: https://www.linkedin.com/in/kevindzkho\n",
      "Successfully scraped experiences\n",
      "Successfully scraped education\n",
      "Successfully found mutual connections: Hunter Reinhart and Kallie Legault are mutual connections\n",
      "Successfully scraped degree of connection: 2nd\n",
      "Successfully scraped description: Senior Financial Analyst at NVIDIA\n",
      "Successfully scraped profile link: https://www.linkedin.com/in/kevindzkho/\n",
      "\n",
      "Success!\n",
      "Name: Kevin Ho\n",
      "Experiences: [{'title: ': 'NVIDIA', 'company: ': '3 yrs 6 mos', 'dates: ': 'Senior Financial Analyst - Datacenter Engineering and Operations', 'location: ': 'Apr 2024 to Present Â· 5 mos', 'summary: ': 'Financial Analyst - Datacenter Engineering and Operations', 'remaining: ': 'Full-time, Aug 2022 to Apr 2024 Â· 1 yr 9 mos, Financial Analyst - Global Software Engineering, Full-time, Mar 2021 to Aug 2022 Â· 1 yr 6 mos'}, {'title: ': 'Project Management/Finance Intern', 'company: ': 'RStor - (Stealth) - Network is Storage', 'dates: ': 'May 2019 to Aug 2019 Â· 4 mos', 'location: ': 'Saratoga, California'}, {'title: ': 'Product Marketing Intern', 'company: ': 'RStor - (Stealth) - Network is Storage', 'dates: ': 'May 2018 to Aug 2018 Â· 4 mos', 'location: ': 'Saratoga, California'}, {'title: ': 'Marketing Intern', 'company: ': 'RStor - (Stealth) - Network is Storage', 'dates: ': 'May 2017 to Aug 2017 Â· 4 mos', 'location: ': 'Saratoga, California'}]\n",
      "Education: [{'school: ': 'University of Southern California', 'degree: ': 'Bachelors of Science , Economics/Mathematics', 'dates: ': '2016 - 2020', 'remaining: ': ''}, {'school: ': 'Bellarmine College Preparatory', 'degree: ': '2012 - 2016', 'dates: ': 'Grade: Overall GPA: 4.26', 'remaining: ': ''}]\n",
      "Profile Distance: 2nd\n",
      "Mutuals: Hunter Reinhart and Kallie Legault are mutual connections\n",
      "Description: Senior Financial Analyst at NVIDIA\n",
      "Link: https://www.linkedin.com/in/kevindzkho/\n",
      "\n",
      "saving profile info\n",
      "adding to history\n",
      "recording scraped url\n",
      "removing from to-scrape\n",
      "1.2987012987012987 % Done - at index: 4\n",
      "\n",
      "----------------------------------------------------------------------------------------------------------\n",
      "\n",
      "At index: 1 - url: https://www.linkedin.com/in/prasadpavuluri\n",
      "Successfully scraped experiences\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[11], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m results \u001b[38;5;241m=\u001b[39m \u001b[43mscrape_all_profiles\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdriver\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhistoryPickle\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtoScrapePickle\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mresultsPickle\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfailedURLsTextFile\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;28mlen\u001b[39m(results))\n",
      "Cell \u001b[0;32mIn[5], line 232\u001b[0m, in \u001b[0;36mscrape_all_profiles\u001b[0;34m(driver, historyPickle, toScrapePickle, resultsPickle, failedURLsTextFile)\u001b[0m\n\u001b[1;32m    229\u001b[0m         \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mError loading history pickle file\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    231\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 232\u001b[0m     profile \u001b[38;5;241m=\u001b[39m \u001b[43mscrape_profile\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdriver\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43murl\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    233\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m profile \u001b[38;5;241m!=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    234\u001b[0m         \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msaving profile info\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[0;32mIn[5], line 166\u001b[0m, in \u001b[0;36mscrape_profile\u001b[0;34m(driver, scraped_link)\u001b[0m\n\u001b[1;32m    164\u001b[0m education_url \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(scraped_link, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdetails/education\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    165\u001b[0m driver\u001b[38;5;241m.\u001b[39mget(education_url)\n\u001b[0;32m--> 166\u001b[0m scraped_education \u001b[38;5;241m=\u001b[39m \u001b[43mget_education\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdriver\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    167\u001b[0m time\u001b[38;5;241m.\u001b[39msleep(\u001b[38;5;241m2\u001b[39m \u001b[38;5;241m+\u001b[39m random\u001b[38;5;241m.\u001b[39mrandom() \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m6\u001b[39m)\n\u001b[1;32m    170\u001b[0m \u001b[38;5;66;03m# Scrape degree of connection and mutuals if available\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[5], line 80\u001b[0m, in \u001b[0;36mget_education\u001b[0;34m(driver)\u001b[0m\n\u001b[1;32m     77\u001b[0m scraped_education \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m     79\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 80\u001b[0m     \u001b[43mwait_for_element_to_load\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdriver\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mBy\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mCSS_SELECTOR\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mli.pvs-list__paged-list-item.artdeco-list__item.pvs-list__item--line-separated.pvs-list__item--one-column\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     81\u001b[0m     education_items \u001b[38;5;241m=\u001b[39m driver\u001b[38;5;241m.\u001b[39mfind_elements(By\u001b[38;5;241m.\u001b[39mCSS_SELECTOR, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mli.pvs-list__paged-list-item.artdeco-list__item.pvs-list__item--line-separated.pvs-list__item--one-column\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     83\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(education_items) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
      "Cell \u001b[0;32mIn[4], line 61\u001b[0m, in \u001b[0;36mwait_for_element_to_load\u001b[0;34m(driver, by, name, base, timeout)\u001b[0m\n\u001b[1;32m     59\u001b[0m base \u001b[38;5;241m=\u001b[39m base \u001b[38;5;129;01mor\u001b[39;00m driver\n\u001b[1;32m     60\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 61\u001b[0m     element \u001b[38;5;241m=\u001b[39m \u001b[43mWebDriverWait\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbase\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43muntil\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     62\u001b[0m \u001b[43m        \u001b[49m\u001b[43mEC\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpresence_of_element_located\u001b[49m\u001b[43m(\u001b[49m\u001b[43m(\u001b[49m\u001b[43mby\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     63\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     64\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m element\n\u001b[1;32m     65\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m TimeoutException:\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/selenium/webdriver/support/wait.py:102\u001b[0m, in \u001b[0;36mWebDriverWait.until\u001b[0;34m(self, method, message)\u001b[0m\n\u001b[1;32m    100\u001b[0m     screen \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mgetattr\u001b[39m(exc, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mscreen\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[1;32m    101\u001b[0m     stacktrace \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mgetattr\u001b[39m(exc, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstacktrace\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[0;32m--> 102\u001b[0m \u001b[43mtime\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msleep\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_poll\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    103\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m time\u001b[38;5;241m.\u001b[39mmonotonic() \u001b[38;5;241m>\u001b[39m end_time:\n\u001b[1;32m    104\u001b[0m     \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "results = scrape_all_profiles(driver, historyPickle, toScrapePickle, resultsPickle, failedURLsTextFile)\n",
    "print(len(results))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "801b0bdc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[<__main__.ScrapedProfile object at 0x1411f7e20>, <__main__.ScrapedProfile object at 0x1411f0820>, <__main__.ScrapedProfile object at 0x1411f0730>, <__main__.ScrapedProfile object at 0x1505326a0>]\n",
      "4\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'path/to/your/credentials.json'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[15], line 63\u001b[0m\n\u001b[1;32m     61\u001b[0m list_of_results \u001b[38;5;241m=\u001b[39m [main_results]  \u001b[38;5;66;03m# Your lists of ScrapedProfile objects\u001b[39;00m\n\u001b[1;32m     62\u001b[0m sheet_names \u001b[38;5;241m=\u001b[39m [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mResults1\u001b[39m\u001b[38;5;124m\"\u001b[39m]  \u001b[38;5;66;03m# Names for each sheet\u001b[39;00m\n\u001b[0;32m---> 63\u001b[0m sheet_url \u001b[38;5;241m=\u001b[39m \u001b[43mcreate_google_sheet\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlist_of_results\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msheet_names\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[15], line 6\u001b[0m, in \u001b[0;36mcreate_google_sheet\u001b[0;34m(list_of_results, sheet_names)\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcreate_google_sheet\u001b[39m(list_of_results, sheet_names):\n\u001b[1;32m      4\u001b[0m     \u001b[38;5;66;03m# Set up the credentials\u001b[39;00m\n\u001b[1;32m      5\u001b[0m     scope \u001b[38;5;241m=\u001b[39m [\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhttps://spreadsheets.google.com/feeds\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhttps://www.googleapis.com/auth/drive\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[0;32m----> 6\u001b[0m     creds \u001b[38;5;241m=\u001b[39m \u001b[43mServiceAccountCredentials\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_json_keyfile_name\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mpath/to/your/credentials.json\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mscope\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      7\u001b[0m     client \u001b[38;5;241m=\u001b[39m gspread\u001b[38;5;241m.\u001b[39mauthorize(creds)\n\u001b[1;32m      9\u001b[0m     \u001b[38;5;66;03m# Create a new Google Sheet\u001b[39;00m\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/oauth2client/service_account.py:219\u001b[0m, in \u001b[0;36mServiceAccountCredentials.from_json_keyfile_name\u001b[0;34m(cls, filename, scopes, token_uri, revoke_uri)\u001b[0m\n\u001b[1;32m    193\u001b[0m \u001b[38;5;129m@classmethod\u001b[39m\n\u001b[1;32m    194\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfrom_json_keyfile_name\u001b[39m(\u001b[38;5;28mcls\u001b[39m, filename, scopes\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[1;32m    195\u001b[0m                            token_uri\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, revoke_uri\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[1;32m    197\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Factory constructor from JSON keyfile by name.\u001b[39;00m\n\u001b[1;32m    198\u001b[0m \n\u001b[1;32m    199\u001b[0m \u001b[38;5;124;03m    Args:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    217\u001b[0m \u001b[38;5;124;03m            the keyfile.\u001b[39;00m\n\u001b[1;32m    218\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 219\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mfilename\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mr\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m file_obj:\n\u001b[1;32m    220\u001b[0m         client_credentials \u001b[38;5;241m=\u001b[39m json\u001b[38;5;241m.\u001b[39mload(file_obj)\n\u001b[1;32m    221\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m_from_parsed_json_keyfile(client_credentials, scopes,\n\u001b[1;32m    222\u001b[0m                                          token_uri\u001b[38;5;241m=\u001b[39mtoken_uri,\n\u001b[1;32m    223\u001b[0m                                          revoke_uri\u001b[38;5;241m=\u001b[39mrevoke_uri)\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'path/to/your/credentials.json'"
     ]
    }
   ],
   "source": [
    "# NEW GOOGLE SHEETS INTEGRATION\n",
    "\n",
    "def create_google_sheet(list_of_results, sheet_names):\n",
    "    # Set up the credentials\n",
    "    scope = ['https://spreadsheets.google.com/feeds', 'https://www.googleapis.com/auth/drive']\n",
    "    creds = ServiceAccountCredentials.from_json_keyfile_name('path/to/your/credentials.json', scope)\n",
    "    client = gspread.authorize(creds)\n",
    "\n",
    "    # Create a new Google Sheet\n",
    "    sheet = client.create('LinkedIn Scraped Profiles')\n",
    "    \n",
    "    for results, sheet_name in zip(list_of_results, sheet_names):\n",
    "        # Create a new worksheet\n",
    "        worksheet = sheet.add_worksheet(title=sheet_name, rows=\"1000\", cols=\"20\")\n",
    "\n",
    "        # Parse candidates and prepare rows\n",
    "        rows = []\n",
    "        for candidate in results:\n",
    "            row = parseProfile(candidate)\n",
    "            rows.append(row)\n",
    "\n",
    "        if not rows:\n",
    "            print(f\"No data for sheet: {sheet_name}\")\n",
    "            continue\n",
    "\n",
    "        # Get column titles\n",
    "        col_titles = list(rows[0].keys())\n",
    "\n",
    "        # Prepare the data for batch update\n",
    "        cells = [col_titles]  # Start with the header row\n",
    "        for row in rows:\n",
    "            cells.append([row.get(col, '') for col in col_titles])\n",
    "\n",
    "        # Update the sheet in batch\n",
    "        cell_range = f'A1:{gspread.utils.rowcol_to_a1(len(cells), len(col_titles))}'\n",
    "        worksheet.update(cell_range, cells)\n",
    "\n",
    "        print(f\"Successfully added data to sheet: {sheet_name}\")\n",
    "\n",
    "    # Delete the default \"Sheet1\" if it exists\n",
    "    try:\n",
    "        sheet.del_worksheet(sheet.worksheet(\"Sheet1\"))\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "    print(f\"Google Sheet created successfully. URL: {sheet.url}\")\n",
    "    return sheet.url\n",
    "\n",
    "def openResults(resultsPickle):\n",
    "    with open(resultsPickle, 'rb') as f:\n",
    "        try:\n",
    "            main_results = pickle.load(f)\n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "            print(\"Error loading to scrape url pickle file\")\n",
    "        print(main_results)\n",
    "        print(len(main_results))\n",
    "\n",
    "# Usage\n",
    "main_results = openResults(resultsPickle)\n",
    "list_of_results = [main_results]  # Your lists of ScrapedProfile objects\n",
    "sheet_names = [\"Results1\"]  # Names for each sheet\n",
    "sheet_url = create_google_sheet(list_of_results, sheet_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0df469c1",
   "metadata": {},
   "source": [
    "# QUERY 2: UNICORN COMPANIES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df63090d",
   "metadata": {},
   "outputs": [],
   "source": [
    "historyPickle = 'db/unicorn_history.pickle'\n",
    "toScrapePickle = 'db/unicorn_to_scrape.pickle'\n",
    "resultsPickle = 'db/unicorn_results.pickle'\n",
    "failedURLsTextFile = 'db/unicorn_failed_urls.txt'\n",
    "\n",
    "SALES_NAV_SEARCH_URL = \"\"\"\n",
    "https://www.linkedin.com/sales/search/people?savedSearchId=1833971588&sessionId=zVKTgf2GQk2owfFVzL9aFQ%3D%3D\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1a8a32b",
   "metadata": {},
   "source": [
    "### Sales nav scraper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc90b784",
   "metadata": {},
   "outputs": [],
   "source": [
    "already_scraped_urls, to_scrape_urls = check_pickles(historyPickle, toScrapePickle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82fb8467",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    updated_to_scrape_urls = scrape_profiles(driver, SALES_NAV_SEARCH_URL, already_scraped_urls, to_scrape_urls, toScrapePickle)\n",
    "    print(updated_to_scrape_urls)\n",
    "except Exception as e:\n",
    "    print(f\"An unexpected error occurred: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98ff5ea6",
   "metadata": {},
   "source": [
    "### Deduplicate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96c9a4c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "deduplicate(toScrapePickle, to_scrape_urls, already_scraped_urls)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29a8309e",
   "metadata": {},
   "source": [
    "### Profile scraper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77b3e90f",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = scrape_all_profiles(driver, historyPickle, toScrapePickle, resultsPickle, failedURLsTextFile)\n",
    "print(len(results))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d71f2e26",
   "metadata": {},
   "source": [
    "# QUERY 3: ACQUIRED COMPANIES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22414fd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "historyPickle = 'db/acquired_history.pickle'\n",
    "toScrapePickle = 'db/acquired_to_scrape.pickle'\n",
    "resultsPickle = 'db/acquired_results.pickle'\n",
    "failedURLsTextFile = 'db/acquired_failed_urls.txt'\n",
    "\n",
    "SALES_NAV_SEARCH_URL = \"\"\"\n",
    "https://www.linkedin.com/sales/search/people?savedSearchId=1833971612&sessionId=R7vv8AiYRXiqMseY1m3CwQ%3D%3D\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c88db61",
   "metadata": {},
   "source": [
    "### Sales nav scraper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "643faf85",
   "metadata": {},
   "outputs": [],
   "source": [
    "already_scraped_urls, to_scrape_urls = check_pickles(historyPickle, toScrapePickle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2ee22e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    updated_to_scrape_urls = scrape_profiles(driver, SALES_NAV_SEARCH_URL, already_scraped_urls, to_scrape_urls, toScrapePickle)\n",
    "    print(updated_to_scrape_urls)\n",
    "except Exception as e:\n",
    "    print(f\"An unexpected error occurred: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8eb29f7",
   "metadata": {},
   "source": [
    "### Deduplicate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d324c97",
   "metadata": {},
   "outputs": [],
   "source": [
    "deduplicate(toScrapePickle, to_scrape_urls, already_scraped_urls)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e12ff51",
   "metadata": {},
   "source": [
    "### Profile scraper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c357cf25",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = scrape_all_profiles(driver, historyPickle, toScrapePickle, resultsPickle, failedURLsTextFile)\n",
    "print(len(results))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b637904",
   "metadata": {},
   "source": [
    "# QUERY 4: VC PORTFOLIO COMPANIES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4efcd7ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "historyPickle = 'db/portfolio_history.pickle'\n",
    "toScrapePickle = 'db/portfolio_to_scrape.pickle'\n",
    "resultsPickle = 'db/portfolio_results.pickle'\n",
    "failedURLsTextFile = 'db/portfolio_failed_urls.txt'\n",
    "\n",
    "SALES_NAV_SEARCH_URL = \"\"\"\n",
    "https://www.linkedin.com/sales/search/people?savedSearchId=1833971604&sessionId=zVKTgf2GQk2owfFVzL9aFQ%3D%3D\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "457c988d",
   "metadata": {},
   "source": [
    "### Sales nav scraper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e3747a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "already_scraped_urls, to_scrape_urls = check_pickles(historyPickle, toScrapePickle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a411299f",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    updated_to_scrape_urls = scrape_profiles(driver, SALES_NAV_SEARCH_URL, already_scraped_urls, to_scrape_urls, toScrapePickle)\n",
    "    print(updated_to_scrape_urls)\n",
    "except Exception as e:\n",
    "    print(f\"An unexpected error occurred: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c4905aa",
   "metadata": {},
   "source": [
    "### Deduplicate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "baf6a7b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "deduplicate(toScrapePickle, to_scrape_urls, already_scraped_urls)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5281ff8",
   "metadata": {},
   "source": [
    "### Profile scraper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5ec701f",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = scrape_all_profiles(driver, historyPickle, toScrapePickle, resultsPickle, failedURLsTextFile)\n",
    "print(len(results))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47d73a6f",
   "metadata": {},
   "source": [
    "# QUERY 5: INFRA COMPANIES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa93ce00",
   "metadata": {},
   "outputs": [],
   "source": [
    "historyPickle = 'db/infra_history.pickle'\n",
    "toScrapePickle = 'db/infra_to_scrape.pickle'\n",
    "resultsPickle = 'db/infra_results.pickle'\n",
    "failedURLsTextFile = 'db/infra_failed_urls.txt'\n",
    "\n",
    "SALES_NAV_SEARCH_URL = \"\"\"\n",
    "https://www.linkedin.com/sales/search/people?savedSearchId=1833971620&sessionId=R7vv8AiYRXiqMseY1m3CwQ%3D%3D\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "924aae38",
   "metadata": {},
   "source": [
    "### Sales nav scraper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7961266",
   "metadata": {},
   "outputs": [],
   "source": [
    "already_scraped_urls, to_scrape_urls = check_pickles(historyPickle, toScrapePickle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2f9a6f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    updated_to_scrape_urls = scrape_profiles(driver, SALES_NAV_SEARCH_URL, already_scraped_urls, to_scrape_urls, toScrapePickle)\n",
    "    print(updated_to_scrape_urls)\n",
    "except Exception as e:\n",
    "    print(f\"An unexpected error occurred: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eec8b4b3",
   "metadata": {},
   "source": [
    "### Deduplicate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2df023a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "deduplicate(toScrapePickle, to_scrape_urls, already_scraped_urls)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f55d6e65",
   "metadata": {},
   "source": [
    "### Profile scraper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dffc900a",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = scrape_all_profiles(driver, historyPickle, toScrapePickle, resultsPickle, failedURLsTextFile)\n",
    "print(len(results))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47185763-6f2b-4f89-9acc-1651887008f6",
   "metadata": {},
   "source": [
    "# EXPORTING RESULTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a13c33b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# NEW GOOGLE SHEETS INTEGRATION\n",
    "\n",
    "def create_google_sheet(list_of_results, sheet_names):\n",
    "    # Set up the credentials\n",
    "    scope = ['https://spreadsheets.google.com/feeds', 'https://www.googleapis.com/auth/drive']\n",
    "    creds = ServiceAccountCredentials.from_json_keyfile_name('path/to/your/credentials.json', scope)\n",
    "    client = gspread.authorize(creds)\n",
    "\n",
    "    # Create a new Google Sheet\n",
    "    sheet = client.create('LinkedIn Scraped Profiles')\n",
    "    \n",
    "    for results, sheet_name in zip(list_of_results, sheet_names):\n",
    "        # Create a new worksheet\n",
    "        worksheet = sheet.add_worksheet(title=sheet_name, rows=\"1000\", cols=\"20\")\n",
    "\n",
    "        # Parse candidates and prepare rows\n",
    "        rows = []\n",
    "        for candidate in results:\n",
    "            row = parseCandidate(candidate)\n",
    "            rows.append(row)\n",
    "\n",
    "        if not rows:\n",
    "            print(f\"No data for sheet: {sheet_name}\")\n",
    "            continue\n",
    "\n",
    "        # Get column titles\n",
    "        col_titles = list(rows[0].keys())\n",
    "\n",
    "        # Prepare the data for batch update\n",
    "        cells = [col_titles]  # Start with the header row\n",
    "        for row in rows:\n",
    "            cells.append([row.get(col, '') for col in col_titles])\n",
    "\n",
    "        # Update the sheet in batch\n",
    "        cell_range = f'A1:{gspread.utils.rowcol_to_a1(len(cells), len(col_titles))}'\n",
    "        worksheet.update(cell_range, cells)\n",
    "\n",
    "        print(f\"Successfully added data to sheet: {sheet_name}\")\n",
    "\n",
    "    # Delete the default \"Sheet1\" if it exists\n",
    "    try:\n",
    "        sheet.del_worksheet(sheet.worksheet(\"Sheet1\"))\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "    print(f\"Google Sheet created successfully. URL: {sheet.url}\")\n",
    "    return sheet.url\n",
    "\n",
    "# Usage\n",
    "list_of_results = [results]  # Your lists of ScrapedProfile objects\n",
    "sheet_names = [\"Results1\"]  # Names for each sheet\n",
    "sheet_url = create_google_sheet(list_of_results, sheet_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0906d83",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create dataframe\n",
    "rows = []\n",
    "for candidate in results:\n",
    "    row = parseCandidate(candidate)\n",
    "    rows.append(row)\n",
    "\n",
    "# Write to CSV\n",
    "col_titles = rows[0].keys()\n",
    "\n",
    "try:\n",
    "    with open('candidates.csv', 'w', newline='') as output_file:\n",
    "        dict_writer = csv.DictWriter(output_file, col_titles)\n",
    "        dict_writer.writeheader()\n",
    "        dict_writer.writerows(rows)\n",
    "        print(\"Successfully exported to csv\")\n",
    "except Exception as e:\n",
    "    print(f\"Failed to export to csv. Error: {e}\")\n",
    "\n",
    "#Export to Excel\n",
    "try:\n",
    "    df.to_excel('candidates.xlsx', index=False)\n",
    "    print(\"Exported to Excel\")\n",
    "except:\n",
    "    print(\"Failed to export to Excel\")\n",
    "\n",
    "    \n",
    "# update db/already_scraped.pickle\n",
    "with open('db/already_scraped.pickle', 'rb') as f:\n",
    "    already_scraped = pickle.load(f)\n",
    "    print(f\"Previously scraped: {len(already_scraped)}\")\n",
    "    already_scraped = already_scraped + already_scraped_urls\n",
    "    already_scraped = list(set(already_scraped))\n",
    "    print(f\"Newly scraped: {len(already_scraped)}\")\n",
    "with open('db/already_scraped.pickle', 'wb') as f:\n",
    "    pickle.dump(already_scraped, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97efd509",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from google.auth.transport.requests import Request\n",
    "# from google.oauth2.credentials import Credentials\n",
    "# from google_auth_oauthlib.flow import InstalledAppFlow\n",
    "# from googleapiclient.discovery import build\n",
    "# from googleapiclient.http import MediaFileUpload\n",
    "\n",
    "# # If modifying these SCOPES, delete the file token.pickle.\n",
    "# SCOPES = ['https://www.googleapis.com/auth/drive.file']\n",
    "\n",
    "# def authenticate():\n",
    "#     creds = None\n",
    "#     # The file token.pickle stores the user's access and refresh tokens, and is created automatically when the authorization flow completes for the first time.\n",
    "#     if os.path.exists('token.pickle'):\n",
    "#         with open('token.pickle', 'rb') as token:\n",
    "#             creds = pickle.load(token)\n",
    "#     # If there are no (valid) credentials available, let the user log in.\n",
    "#     if not creds or not creds.valid:\n",
    "#         if creds and creds.expired and creds.refresh_token:\n",
    "#             creds.refresh(Request())\n",
    "#         else:\n",
    "#             flow = InstalledAppFlow.from_client_secrets_file(\n",
    "#                 'client_secrets.json', SCOPES)\n",
    "#             creds = flow.run_local_server(port=0)\n",
    "#         # Save the credentials for the next run\n",
    "#         with open('token.pickle', 'wb') as token:\n",
    "#             pickle.dump(creds, token)\n",
    "#     return creds\n",
    "\n",
    "# def upload_file_to_drive(file_path, file_name, mime_type):\n",
    "#     creds = authenticate()\n",
    "#     service = build('drive', 'v3', credentials=creds)\n",
    "\n",
    "#     file_metadata = {'name': file_name}\n",
    "#     media = MediaFileUpload(file_path, mimetype=mime_type)\n",
    "\n",
    "#     file = service.files().create(body=file_metadata, media_body=media, fields='id').execute()\n",
    "#     print('File ID: %s' % file.get('id'))\n",
    "\n",
    "# if __name__ == '__main__':\n",
    "#     file_path = 'path_to_your_local_excel_file.xlsx'  # Replace with the path to your local file\n",
    "#     file_name = 'your_excel_file.xlsx'  # Replace with the desired name for the file in Google Drive\n",
    "#     mime_type = 'application/vnd.openxmlformats-officedocument.spreadsheetml.sheet'\n",
    "\n",
    "#     upload_file_to_drive(file_path, file_name, mime_type)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ee20a9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# categories = COMPANY_CATEGORIES\n",
    "\n",
    "# # Example color mapping for categories\n",
    "# category_colors = {\n",
    "#     \"SECURITY\": 'red',\n",
    "#     \"OTHER\": 'blue',\n",
    "#     \"PUBLIC\": 'green',\n",
    "#     \"INFRA\": 'yellow',\n",
    "#     \"FINTECH\": 'orange',\n",
    "#     \"CRYPTO\": 'purple',\n",
    "#     \"FRONTIER\": 'cyan',\n",
    "#     \"AI\": 'magenta'\n",
    "# }\n",
    "\n",
    "# # Create a reverse dictionary for easier lookup: {company: category}\n",
    "# company_category = {}\n",
    "# for category, companies in categories.items():\n",
    "#     for company in companies:\n",
    "#         company_category[company] = category\n",
    "\n",
    "# # Modify the style function\n",
    "# def highlight_by_category(val):\n",
    "#     category = company_category.get(val)\n",
    "#     if category:\n",
    "#         color = category_colors.get(category, 'none')  # default to 'none' if no color is specified\n",
    "#     else:\n",
    "#         color = 'none'\n",
    "#     return f'background-color: {color}'\n",
    "\n",
    "# import re\n",
    "# ILLEGAL_CHARACTERS_RE = re.compile(r'[\\000-\\010]|[\\013-\\014]|[\\016-\\037]')\n",
    "# def find_illegal_characters(df):\n",
    "#     for column in df.columns:\n",
    "#         for idx, item in enumerate(df[column]):\n",
    "#             if isinstance(item, str) and ILLEGAL_CHARACTERS_RE.search(item):\n",
    "#                 # replace illegal characters with an empty string\n",
    "#                 df[column][idx] = ILLEGAL_CHARACTERS_RE.sub('', item)\n",
    "#     return df\n",
    "\n",
    "# styled_df = df\n",
    "# styled_df = find_illegal_characters(styled_df)\n",
    "# styled_df = df.style.applymap(highlight_by_category)\n",
    "\n",
    "# # Save the styled DataFrame to an Excel file\n",
    "# # get today's date in MM-DD-YYYY format\n",
    "# import datetime\n",
    "# now = datetime.datetime.now()\n",
    "# date = now.strftime(\"%m-%d-%Y\")\n",
    "# styled_df.to_excel(f'results/{date}_{start}-{end}.xlsx', engine='openpyxl', index=False)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
